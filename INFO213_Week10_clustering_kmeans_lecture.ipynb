{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anyuanay/INFO213/blob/main/INFO213_Week10_clustering_kmeans_lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymbZqfpDLHBQ"
      },
      "source": [
        "# INFO 213: Data Science Programming 2\n",
        "___\n",
        "\n",
        "### Week 10: Clustering and K-Means\n",
        "\n",
        "\n",
        "**Agenda:**\n",
        "- Definition and significance of clustering in unsupervised learning.\n",
        "- Differentiating between supervised and unsupervised learning.\n",
        "- Detailed explanation of the k-means algorithm.\n",
        "- How the algorithm initializes centroids and iteratively assigns data points to clusters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWwDdmA2Bv6X"
      },
      "source": [
        "# What is Clustering? How is clustering different from classification?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Zg2y0uoBv6X"
      },
      "source": [
        "Here we will examine a class of unsupervised machine learning models: clustering algorithms.\n",
        "\n",
        "- Clustering algorithms seek to learn, from the properties of the data, an optimal division or discrete labeling of groups of points.\n",
        "\n",
        "- Many clustering algorithms are available in Scikit-Learn and elsewhere, but perhaps the simplest to understand is an algorithm known as *k-means clustering*, which is implemented in ``sklearn.cluster.KMeans``.\n",
        "- Unlike classification, clusturing doesn't need to learn predefined labels for data.\n",
        "\n",
        "We begin with the standard imports:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCr-h2N7Bv6Z"
      },
      "source": [
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()  # for plot styling\n",
        "import numpy as np\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqIEuFX_Bv6a"
      },
      "source": [
        "## What is the K-Means clustering algorithm? How does the K-Means algorithm work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iJBBxKbBv6a"
      },
      "source": [
        "The *k*-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset.\n",
        "It accomplishes this using a simple conception of what the optimal clustering looks like:\n",
        "\n",
        "- The \"cluster center\" is the arithmetic mean of all the points belonging to the cluster.\n",
        "- Each point is closer to its own cluster center than to other cluster centers.\n",
        "\n",
        "Those two assumptions are the basis of the *k*-means model.\n",
        "We will soon dive into exactly *how* the algorithm reaches this solution, but for now let's take a look at a simple dataset and see the *k*-means result.\n",
        "\n",
        "First, let's generate a two-dimensional dataset containing four distinct blobs.\n",
        "To emphasize that this is an unsupervised algorithm, we will leave the labels out of the visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fAsF2O4Bv6c",
        "outputId": "22b88c1f-aafb-4480-c517-eb00e98ffd7a"
      },
      "source": [
        "```python\n",
        "from sklearn.datasets import make_blobs\n",
        "X, y_true = make_blobs(n_samples=300, centers=4,\n",
        "                       cluster_std=0.60, random_state=0)\n",
        "plt.scatter(X[:, 0], X[:, 1], s=50);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2YHvnwCBv6f"
      },
      "source": [
        "By eye, it is relatively easy to pick out the four clusters.\n",
        "The *k*-means algorithm does this automatically, and in Scikit-Learn uses the typical estimator API:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ZMHHiAjiBv6g"
      },
      "source": [
        "```python\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=4, n_init=10)\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZr5vHIoBv6h"
      },
      "source": [
        "Let's visualize the results by plotting the data colored by these labels.\n",
        "We will also plot the cluster centers as determined by the *k*-means estimator:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WnFLO1tBv6h",
        "outputId": "84f5ff2b-28a3-4cf3-c9c0-7f801bc96431"
      },
      "source": [
        "```python\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
        "\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk_dSIoZBv6i"
      },
      "source": [
        "- The good news is that the *k*-means algorithm (at least in this simple case) assigns the points to clusters very similarly to how we might assign them by eye.\n",
        "- But you might wonder how this algorithm finds these clusters so quickly! After all, the number of possible combinations of cluster assignments is exponential in the number of data points—an exhaustive search would be very, very costly.\n",
        "- Fortunately for us, such an exhaustive search is not necessary: instead, the typical approach to *k*-means involves an intuitive iterative approach known as *expectation–maximization*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7L84xRvdBv6i"
      },
      "source": [
        "## What is expectation-maximazation? How is expectaion-maximazation used in the k-means algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh7gcqjGBv6j"
      },
      "source": [
        "- Expectation–maximization (E–M) is a powerful algorithm that comes up in a variety of contexts within data science.\n",
        "- *k*-means is a particularly simple and easy-to-understand application of the algorithm, and we will walk through it briefly here.\n",
        "- In short, the expectation–maximization approach here consists of the following procedure:\n",
        "\n",
        "1. Guess some cluster centers\n",
        "2. Repeat until converged\n",
        "   1. *E-Step*: assign points to the nearest cluster center\n",
        "   2. *M-Step*: set the cluster centers to the mean\n",
        "\n",
        "- Here the \"E-step\" or \"Expectation step\" is so-named because it involves updating our expectation of which cluster each point belongs to.\n",
        "- The \"M-step\" or \"Maximization step\" is so-named because it involves maximizing some fitness function that defines the location of the cluster centers—in this case, that maximization is accomplished by taking a simple mean of the data in each cluster.\n",
        "\n",
        "- The literature about this algorithm is vast, but can be summarized as follows:\n",
        "    - under typical circumstances, each repetition of the E-step and M-step will always result in a better estimate of the cluster characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How is the k-means algorithm implemented?\n",
        "In practice, the k-means algorithm is summarized by the following four steps:\n",
        "1. Randomly pick $k$ centroids from the examples as initial cluster centers.\n",
        "2. Assign each example to the nearest centroid, $\\mu^{(j)}, j\\in\\{1..k\\}$.\n",
        "3. Move the centroids to the center of the examples that were assigned to it.\n",
        "4. Repeat steps 2 and 3 until the cluster assignments do not change or a user-defined tolerance or maximum number of iterations is reached.\n",
        "\n",
        "- The similarity between objects is defined by the squared Euclidean distance between two points $x$ and $y$ in $m$-dimensional space:\n",
        "\\begin{equation}\n",
        " d(x, y)^2 = \\Sigma_j^m(x_j-y_j)^2\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "Yf5UNgikoHbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual Example\n",
        "1. Data points: $\\{a=(1, 2), b=(2, 1), c=(3, 4), d=(4, 6), e=(5, 3)\\}$ and $K = 2$\n",
        "2. Randomly pick 2 centroids: $a=(1, 2)$ and $b=(2, 1)$\n",
        "3. Compute the distances from other points to the centroids:\n",
        " - $d(c,a)^2 = (3-1)^2+(4-2)^2=8$\n",
        " - $d(c,b)^2 = (3-2)^2+(4-1)^2=10$\n",
        " - $d(d,a)^2 = (4-1)^2+(6-2)^2=25$\n",
        " - $d(d,b)^2 = (4-2)^2+(6-1)^2=29$\n",
        " - $d(e,a)^2 = (5-1)^2+(3-2)^2=17$\n",
        " - $d(e,b)^2 = (5-2)^2+(3-1)^2=13$  \n",
        "4. We have two clusters: $k_1=\\{a, c, d\\}$ and $k_2=\\{b, e\\}$.\n",
        "5. Calculate the centroids of the clusters $k_1$ and $k_2$:\n",
        " - $C_1 = (\\frac{1+3+4}{3}, \\frac{2+4+6}{3})=(\\frac{8}{3}, 4)$.\n",
        " - $C_2 = (\\frac{2+5}{2}, \\frac{1+3}{2})=(\\frac{7}{2}, 2)$.\n",
        "6. Compute the distances from all points to the new centroids:\n",
        " - $d(a, C_1)^2 = (1-\\frac{8}{3})^2 + (2-4)^2 = 6.78$\n",
        " - $d(a, C_2)^2 = (1-\\frac{7}{2})^2 + (2-2)^2 = 6.25$\n",
        " - $d(b, C_1)^2 = (2-\\frac{8}{3})^2 + (1-4)^2 = 9.43$\n",
        " - $d(b, C_2)^2 = (2-\\frac{7}{2})^2 + (1-2)^2 = 3.25$\n",
        " - $d(c, C_1)^2 = (3-\\frac{8}{3})^2 + (4-4)^2 = 0.11$\n",
        " - $d(c, C_2)^2 = (3-\\frac{7}{2})^2 + (4-2)^2 = 4.25$\n",
        " - $d(d, C_1)^2 = (4-\\frac{8}{3})^2 + (6-4)^2 = 5.78$\n",
        " - $d(d, C_2)^2 = (4-\\frac{7}{2})^2 + (6-2)^2 = 16.25$\n",
        " - $d(e, C_1)^2 = (5-\\frac{8}{3})^2 + (3-4)^2 = 6.44$\n",
        " - $d(e, C_2)^2 = (5-\\frac{7}{2})^2 + (3-2)^2 = 3.25$\n",
        "7. We have two new clusters: $k_1=\\{c, d\\}$ and $k_2=\\{a, b, e\\}$.\n",
        "8. Recalculate the centroids of the clusters $k_1$ and $k_2$:\n",
        " - $C_1 = (\\frac{3+4}{2}, \\frac{4+6}{2})=(\\frac{7}{2}, 5)$.\n",
        " - $C_2 = (\\frac{1+2+5}{3}, \\frac{2+1+3}{3})=(\\frac{8}{3}, 2)$.\n",
        "9. Compute the distances from all points to the new centroids:\n",
        " - $d(a, C_1)^2 = (1-\\frac{7}{2})^2 + (2-5)^2 = 15.25$\n",
        " - $d(a, C_2)^2 = (1-\\frac{8}{3})^2 + (2-2)^2 = 3.78$\n",
        " - $d(b, C_1)^2 = (2-\\frac{7}{2})^2 + (1-5)^2 = 18.25$\n",
        " - $d(b, C_2)^2 = (2-\\frac{8}{3})^2 + (1-2)^2 = 1.44$\n",
        " - $d(c, C_1)^2 = (3-\\frac{7}{2})^2 + (4-5)^2 = 1.25$\n",
        " - $d(c, C_2)^2 = (3-\\frac{8}{3})^2 + (4-2)^2 = 4.11$\n",
        " - $d(d, C_1)^2 = (4-\\frac{7}{2})^2 + (6-5)^2 = 1.25$\n",
        " - $d(d, C_2)^2 = (4-\\frac{8}{3})^2 + (6-2)^2 = 17.78$\n",
        " - $d(e, C_1)^2 = (5-\\frac{7}{2})^2 + (3-5)^2 = 6.25$\n",
        " - $d(e, C_2)^2 = (5-\\frac{8}{3})^2 + (3-2)^2 = 6.44$\n",
        "10. We have two new clusters: $k_1=\\{a, b\\}$ and $k_2=\\{c, d, e\\}$.\n",
        "11. Recalculate the centroids of the clusters $k_1$ and $k_2$:\n",
        " - $C_1 = (\\frac{1+2}{2}, \\frac{2+1}{2})=(\\frac{3}{2}, \\frac{3}{2})$.\n",
        " - $C_2 = (\\frac{3+4+5}{3}, \\frac{4+6+3}{3})=(4, \\frac{13}{3})$.\n",
        "12. Compute the distances from all points to the new centroids:\n",
        " - $d(a, C_1)^2 = (1-\\frac{3}{2})^2 + (2-\\frac{3}{2})^2 = 0.5$\n",
        " - $d(a, C_2)^2 = (1-4)^2 + (2-\\frac{13}{3})^2 = 14.44$\n",
        " - $d(b, C_1)^2 = (2-\\frac{3}{2})^2 + (1-\\frac{3}{2})^2 = 0.5$\n",
        " - $d(b, C_2)^2 = (2-4)^2 + (1-\\frac{13}{3})^2 = 15.11$\n",
        " - $d(c, C_1)^2 = (3-\\frac{3}{2})^2 + (4-\\frac{3}{2})^2 = 8.5$\n",
        " - $d(c, C_2)^2 = (3-4)^2 + (4-\\frac{13}{3})^2 = 1.11$\n",
        " - $d(d, C_1)^2 = (4-\\frac{3}{2})^2 + (6-\\frac{3}{2})^2 = 26.5 $\n",
        " - $d(d, C_2)^2 = (4-4)^2 + (6-\\frac{13}{3})^2 = 2.78$\n",
        " - $d(e, C_1)^2 = (5-\\frac{3}{2})^2 + (3-\\frac{3}{2})^2 = 14.5 $\n",
        " - $d(e, C_2)^2 = (5-4)^2 + (3-\\frac{13}{3})^2 = 2.78$\n",
        "13. The two clusters remain: $k_1=\\{a, b\\}$ and $k_2=\\{c, d, e\\}$.\n",
        "14. The cluster assignments do not change. We are done.\n"
      ],
      "metadata": {
        "id": "TRy1mN6ltM4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "(5-3/2)**2+(3-3/2)**2\n",
        "```"
      ],
      "metadata": {
        "id": "MON8eZr_GbE6",
        "outputId": "c6af38b4-711e-4291-b3de-9e1596a564e8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "(5-4)**2+(3-13/3)**2\n",
        "```"
      ],
      "metadata": {
        "id": "6V0BoSIwGIex",
        "outputId": "d25811d5-8ecc-4731-e732-1c220d653f40"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSv1DzJeBv6m"
      },
      "source": [
        "## Should the number of clusters be selected beforehand for the k-means algorithm?\n",
        "- A common challenge with *k*-means is that you must tell it how many clusters you expect: it cannot learn the number of clusters from the data.\n",
        "- For example, if we ask the algorithm to identify six clusters, it will happily proceed and find the best six clusters:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4voO3HHBv6n",
        "outputId": "cc439491-8281-4b43-b220-69e764c778f2"
      },
      "source": [
        "```python\n",
        "labels = KMeans(6, random_state=0, n_init=10).fit_predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
        "            s=50, cmap='viridis');\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYCV_fmBBv6n"
      },
      "source": [
        "- Whether the result is meaningful is a question that is difficult to answer definitively; one approach that is rather intuitive, but that we won't discuss further here, is called [silhouette analysis](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html).\n",
        "\n",
        "- Alternatively, you might use a more complicated clustering algorithm which has a better quantitative measure of the fitness per number of clusters (e.g., Gaussian mixture models); or which *can* choose a suitable number of clusters (e.g., DBSCAN, mean-shift, or affinity propagation, all available in the ``sklearn.cluster`` submodule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVnEXWIhBv6n"
      },
      "source": [
        "## Is the k-means algorithm limited to linear cluster boundaries?\n",
        "- The fundamental model assumptions of *k*-means (points will be closer to their own cluster center than to others) means that the algorithm will often be ineffective if the clusters have complicated geometries.\n",
        "\n",
        "- In particular, the boundaries between *k*-means clusters will always be linear, which means that it will fail for more complicated boundaries.\n",
        "- Consider the following data, along with the cluster labels found by the typical *k*-means approach:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Vga57zQHBv6o"
      },
      "source": [
        "```python\n",
        "from sklearn.datasets import make_moons\n",
        "X, y = make_moons(200, noise=.05, random_state=0)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlJ9ua8pBv6o",
        "outputId": "6af40a49-a887-4f7f-fb17-49e57ec4613c"
      },
      "source": [
        "```python\n",
        "labels = KMeans(2, random_state=0, n_init=10).fit_predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
        "            s=50, cmap='viridis');\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rubwEVHuBv6o"
      },
      "source": [
        "- We can use a kernel transformation to project the data into a higher dimension where a linear separation is possible.\n",
        "\n",
        "- One version of this kernelized *k*-means is implemented in Scikit-Learn within the ``SpectralClustering`` estimator.\n",
        "- It uses the graph of nearest neighbors to compute a higher-dimensional representation of the data, and then assigns labels using a *k*-means algorithm:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck4ja8egBv6p",
        "outputId": "54284533-6c3f-4f77-ee09-459392e8ab12"
      },
      "source": [
        "```python\n",
        "from sklearn.cluster import SpectralClustering\n",
        "model = SpectralClustering(n_clusters=2, affinity='nearest_neighbors',\n",
        "                           assign_labels='kmeans')\n",
        "labels = model.fit_predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
        "            s=50, cmap='viridis');\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E88VDnAZBv6p"
      },
      "source": [
        "- We see that with this kernel transform approach, the kernelized *k*-means is able to find the more complicated nonlinear boundaries between clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMgu4hxYBv6p"
      },
      "source": [
        "## Is the k-means algorithm slow for large numbers of samples?\n",
        "- Because each iteration of *k*-means must access every point in the dataset, the algorithm can be relatively slow as the number of samples grows.\n",
        "- You might wonder if this requirement to use all data at each iteration can be relaxed; for example, you might just use a subset of the data to update the cluster centers at each step.\n",
        "- This is the idea behind batch-based *k*-means algorithms, one form of which is implemented in ``sklearn.cluster.MiniBatchKMeans``.\n",
        "- The interface for this is the same as for standard ``KMeans``; we will see an example of its use as we continue our discussion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSCIgP2hBv6q"
      },
      "source": [
        "## Examples\n",
        "\n",
        "- Being careful about these limitations of the algorithm, we can use *k*-means to our advantage in a wide variety of situations.\n",
        "We'll now take a look at a couple examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGbyFNPYBv6q"
      },
      "source": [
        "### Example 1: k-means on digits\n",
        "\n",
        "- Here we will attempt to use *k*-means to try to identify similar digits *without using the original label information*; this might be similar to a first step in extracting meaning from a new dataset about which you don't have any *a priori* label information.\n",
        "\n",
        "- We will start by loading the digits and then finding the ``KMeans`` clusters. The digits consist of 1,797 samples with 64 features, where each of the 64 features is the brightness of one pixel in an 8×8 image:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R21m4SzhBv6q",
        "outputId": "05766c06-fc64-4b75-cf43-a2b444add754"
      },
      "source": [
        "```python\n",
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "digits.data.shape\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4rXoaJOBv6q"
      },
      "source": [
        "- The clustering can be performed as we did before:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twUNK681Bv6r",
        "outputId": "fed78c98-b994-49c9-b792-c9f300823917"
      },
      "source": [
        "```python\n",
        "kmeans = KMeans(n_clusters=10, random_state=0, n_init=10)\n",
        "clusters = kmeans.fit_predict(digits.data)\n",
        "kmeans.cluster_centers_.shape\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwhLmvdQBv6r"
      },
      "source": [
        "- The result is 10 clusters in 64 dimensions.\n",
        "- Notice that the cluster centers themselves are 64-dimensional points, and can themselves be interpreted as the \"typical\" digit within the cluster.\n",
        "- Let's see what these cluster centers look like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4jYS4MoBv6s",
        "outputId": "2709cdc3-5f5b-4cd8-bfd9-7a01cfe67a55"
      },
      "source": [
        "```python\n",
        "fig, ax = plt.subplots(2, 5, figsize=(8, 3))\n",
        "centers = kmeans.cluster_centers_.reshape(10, 8, 8)\n",
        "for axi, center in zip(ax.flat, centers):\n",
        "    axi.set(xticks=[], yticks=[])\n",
        "    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkm80pheBv6s"
      },
      "source": [
        "- We see that *even without the labels*, ``KMeans`` is able to find clusters whose centers are recognizable digits, with perhaps the exception of some digits.\n",
        "\n",
        "- Because *k*-means knows nothing about the identity of the cluster, the 0–9 labels may be permuted.\n",
        "- We can fix this by matching each learned cluster label with the true labels found in them:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc0jWBCgBv6s"
      },
      "source": [
        "```python\n",
        "from scipy.stats import mode\n",
        "\n",
        "labels = np.zeros_like(clusters)\n",
        "for i in range(10):\n",
        "    mask = (clusters == i)\n",
        "    labels[mask] = mode(digits.target[mask])[0]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "fig, ax = plt.subplots(2, 5, figsize=(12, 9))\n",
        "display_digits = digits.data[:10].reshape(10, 8, 8)\n",
        "idx = 0\n",
        "for axi, digi in zip(ax.flat, display_digits):\n",
        "    axi.set(xticks=[], yticks=[])\n",
        "    axi.imshow(digi, interpolation='nearest', cmap=plt.cm.binary)\n",
        "    axi.set_xlabel(\"True Label:\" + str(digits.target[idx]) + \"\\nPredicted Label:\" + str(labels[idx]))\n",
        "    idx += 1\n",
        "\n",
        "plt.tight_layout()\n",
        "```"
      ],
      "metadata": {
        "id": "7wuqwTJKD3k7",
        "outputId": "6dbef60b-1f4c-4f25-8b3b-35f76883d364"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrWj7-_zBv6s"
      },
      "source": [
        "- Now we can check how accurate our unsupervised clustering was in finding similar digits within the data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d6YN4elBv6t",
        "outputId": "e5392eaa-5f99-46a0-a16a-50374898d8c0"
      },
      "source": [
        "```python\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(digits.target, labels)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVEYiEpGBv6t"
      },
      "source": [
        "With just a simple *k*-means algorithm, we discovered the correct grouping for 80% of the input digits!\n",
        "Let's check the confusion matrix for this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv0QfvHcBv6t",
        "outputId": "513cfa9c-7885-4ff7-f09d-d501a8429006"
      },
      "source": [
        "```python\n",
        "from sklearn.metrics import confusion_matrix\n",
        "mat = confusion_matrix(digits.target, labels)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
        "            xticklabels=digits.target_names,\n",
        "            yticklabels=digits.target_names)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFXPB2cxBv6t"
      },
      "source": [
        "- As we might expect from the cluster centers we visualized before, the main point of confusion is between the eights and ones.\n",
        "- But this still shows that using *k*-means, we can essentially build a digit classifier *without reference to any known labels*!\n",
        "\n",
        "- Just for fun, let's try to push this even farther.\n",
        "We can use the t-distributed stochastic neighbor embedding (t-SNE) algorithm to pre-process the data before performing *k*-means.\n",
        "- t-SNE is a nonlinear embedding algorithm that is particularly adept at preserving points within clusters.\n",
        "Let's see how it does:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri8LW-t6Bv6t",
        "outputId": "3d142ac9-c7eb-43fc-f087-e5629a04195a"
      },
      "source": [
        "```python\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Project the data: this step will take several seconds\n",
        "tsne = TSNE(n_components=2, init='random', random_state=0)\n",
        "digits_proj = tsne.fit_transform(digits.data)\n",
        "\n",
        "# Compute the clusters\n",
        "kmeans = KMeans(n_clusters=10, random_state=0)\n",
        "clusters = kmeans.fit_predict(digits_proj)\n",
        "\n",
        "# Permute the labels\n",
        "labels = np.zeros_like(clusters)\n",
        "for i in range(10):\n",
        "    mask = (clusters == i)\n",
        "    labels[mask] = mode(digits.target[mask])[0]\n",
        "\n",
        "# Compute the accuracy\n",
        "accuracy_score(digits.target, labels)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ7Pm1QGBv6u"
      },
      "source": [
        "### That's nearly 94% classification accuracy *without using the labels*. This is the power of unsupervised learning when used carefully: it can extract information from the dataset that it might be difficult to do by hand or by eye."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6VcjRiOBv6u"
      },
      "source": [
        "### Example 2: *k*-means for color compression\n",
        "\n",
        "- One interesting application of clustering is in color compression within images.\n",
        "- For example, imagine you have an image with millions of colors.\n",
        "- In most images, a large number of the colors will be unused, and many of the pixels in the image will have similar or even identical colors.\n",
        "\n",
        "- For example, consider the image shown in the following figure, which is from the Scikit-Learn ``datasets`` module (for this to work, you'll have to have the ``pillow`` Python package installed)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBDuvZIwBv6u",
        "outputId": "579cd6a6-ea40-4e5b-ef63-580148a7d0b1"
      },
      "source": [
        "```python\n",
        "# Note: this requires the ``pillow`` package to be installed\n",
        "from sklearn.datasets import load_sample_image\n",
        "china = load_sample_image(\"china.jpg\")\n",
        "ax = plt.axes(xticks=[], yticks=[])\n",
        "ax.imshow(china);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNJdzRIvBv6u"
      },
      "source": [
        "- The image itself is stored in a three-dimensional array of size ``(height, width, RGB)``, containing red/blue/green contributions as integers from 0 to 255:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bt8wAydBv6u",
        "outputId": "e4b02494-b0f6-446e-e95a-3a7741d3b294"
      },
      "source": [
        "```python\n",
        "china.shape\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucbJGU0xBv6u"
      },
      "source": [
        "- One way we can view this set of pixels is as a cloud of points in a three-dimensional color space.\n",
        "- We will reshape the data to ``[n_samples x n_features]``, and rescale the colors so that they lie between 0 and 1:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucJk5pRWBv6v",
        "outputId": "84aecea6-755a-41e2-9ecf-7a623cb44d3b"
      },
      "source": [
        "```python\n",
        "data = china / 255.0 # use 0...1 scale\n",
        "data = data.reshape(427 * 640, 3)\n",
        "data.shape\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVBBwQ9ABv6v"
      },
      "source": [
        "- We can visualize these pixels in this color space, using a subset of 10,000 pixels for efficiency:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "colors = data\n",
        "rng = np.random.RandomState(0)\n",
        "i = rng.permutation(data.shape[0])[:10000]\n",
        "i.shape, i\n",
        "```"
      ],
      "metadata": {
        "id": "UJ2hz9qBtIv6",
        "outputId": "31926a58-7d8b-47fc-8641-fc799b0aea83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "colors = colors[i]\n",
        "colors\n",
        "```"
      ],
      "metadata": {
        "id": "8D_tG8XytYR9",
        "outputId": "045b3c02-9242-4965-b5b2-261d1871dde5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "R, G, B = data[i].T\n",
        "R\n",
        "```"
      ],
      "metadata": {
        "id": "Us3zIX-utd2R",
        "outputId": "f88b55e5-b3c5-4183-89bf-1783b9c6c206"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "plt.scatter(R, G, color=colors, marker=\".\")\n",
        "plt.xlabel(\"Red\")\n",
        "plt.ylabel(\"Green\")\n",
        "plt.title(\"16 Million Possible Colors\")\n",
        "```"
      ],
      "metadata": {
        "id": "HWMFE7jOtpEg",
        "outputId": "5f50152d-3b08-491b-8d9c-337401741cb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "plt.scatter(R, B, color=colors, marker=\".\")\n",
        "plt.xlabel(\"Red\")\n",
        "plt.ylabel(\"Blue\")\n",
        "plt.title(\"16 Million Possible Colors\")\n",
        "```"
      ],
      "metadata": {
        "id": "qINcGdVdtkAN",
        "outputId": "c311643b-0b4d-4485-d727-ed0641e34551"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2FUWjxDBv6v"
      },
      "source": [
        "- Now let's reduce these 16 million colors to just 16 colors, using a *k*-means clustering across the pixel space.\n",
        "- Because we are dealing with a very large dataset, we will use the mini batch *k*-means, which operates on subsets of the data to compute the result much more quickly than the standard *k*-means algorithm:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d-EAvzBBv6v"
      },
      "source": [
        "```python\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "kmeans = MiniBatchKMeans(16)\n",
        "kmeans.fit(data)\n",
        "new_colors = kmeans.cluster_centers_[kmeans.predict(data)]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "new_colors.shape\n",
        "```"
      ],
      "metadata": {
        "id": "2YB2rQ4zvhHa",
        "outputId": "ecb23450-1e4d-4d23-a209-83930f9a24e2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "rng = np.random.RandomState(0)\n",
        "i = rng.permutation(data.shape[0])[:10000]\n",
        "colors = new_colors[i]\n",
        "R, G, B = data[i].T\n",
        "```"
      ],
      "metadata": {
        "id": "M9tZveRPvxGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "plt.scatter(R, G, color=colors, marker=\".\")\n",
        "plt.xlabel(\"Red\")\n",
        "plt.ylabel(\"Green\")\n",
        "plt.title(\"16 Possible Colors\")\n",
        "```"
      ],
      "metadata": {
        "id": "nwE-T934wA2L",
        "outputId": "25e31754-d730-4c6f-d84a-ca25c78971af"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "plt.scatter(R, G, color=colors, marker=\".\")\n",
        "plt.xlabel(\"Red\")\n",
        "plt.ylabel(\"Blue\")\n",
        "plt.title(\"16 Possible Colors\")\n",
        "```"
      ],
      "metadata": {
        "id": "fFHrygL4wERM",
        "outputId": "9169a23f-4f7a-4e46-df81-cb431b8bb668"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dylf0i5vBv6w"
      },
      "source": [
        "- The result is a re-coloring of the original pixels, where each pixel is assigned the color of its closest cluster center.\n",
        "- Plotting these new colors in the image space rather than the pixel space shows us the effect of this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gps-ovDSBv6w",
        "outputId": "0f862401-037d-4767-a4c4-016ad903c829"
      },
      "source": [
        "```python\n",
        "china_recolored = new_colors.reshape(china.shape)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6),\n",
        "                       subplot_kw=dict(xticks=[], yticks=[]))\n",
        "fig.subplots_adjust(wspace=0.05)\n",
        "ax[0].imshow(china)\n",
        "ax[0].set_title('Original Image', size=16)\n",
        "ax[1].imshow(china_recolored)\n",
        "ax[1].set_title('16-color Image', size=16);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "a8LaJW3LHobc"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}