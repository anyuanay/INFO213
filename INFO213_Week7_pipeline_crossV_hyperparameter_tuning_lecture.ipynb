{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anyuanay/INFO213/blob/main/INFO213_Week7_pipeline_crossV_hyperparameter_tuning_lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymbZqfpDLHBQ"
      },
      "source": [
        "# INFO 213: Data Science Programming 2\n",
        "___\n",
        "\n",
        "### Week 7: Pipeline, Cross Validation, and Hyperparameter Tuning\n",
        "\n",
        "\n",
        "**Agenda:**\n",
        "- Pipeline for wrapping up preprocessing and model fitting steps.\n",
        "- Needs for cross-validation to assess model performance effectively.\n",
        "- K-Fold Cross-Validation.\n",
        "- Understanding hyperparameters vs. parameters.\n",
        "- Grid Search: Exhaustive search over specified hyperparameter values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Czrrdz5fiXq"
      },
      "source": [
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why do we need model evaluation and selection?\n",
        "- Machine learning involves choosing from a variety of algorithms and models.\n",
        "- We need to compare different models and algorithms to determine which one performs best.\n",
        "-  Model selection and evaluation help ensure that a chosen model is not overfitting or underfitting, and can be generalized to unseen data."
      ],
      "metadata": {
        "id": "jYUYArVNGZg1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "8zE_5piRHACq"
      },
      "source": [
        "## Underfitting and Overfitting\n",
        "\n",
        "A common danger in machine learning is overfitting—producing a model that performs well on the data you train it on but that generalizes poorly to any new data.\n",
        "\n",
        "The other side of this is underfitting, producing a model that doesn’t perform well\n",
        "even on the training data, although typically when this happens you decide your\n",
        "model isn’t good enough and keep looking for a better one.\n",
        "\n",
        "![](https://i.imgur.com/oDDNc07.png)\n",
        "\n",
        "\n",
        "The most fundamental approach to dealing with underfitting and overfitting is to use different data to train the\n",
        "model and to test the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61qGKH8qHACq"
      },
      "source": [
        "## The Bias and Variance Trade-Off\n",
        "- Underfitting models will make a lot of mistakes for pretty much any training set (drawn from the same population),\n",
        "which means that it has a high bias. Any two randomly chosen\n",
        "training sets should give pretty similar models (since any two randomly chosen training\n",
        "sets should have pretty similar average values). So we say that it has a low variance.\n",
        "- Overfitting models have very low bias but very high variance (since any two training sets would likely give rise to very different models).\n",
        "\n",
        "Thinking about model problems this way can help you figure out what to do when your\n",
        "model doesn’t work so well.\n",
        "\n",
        "- If your model has high bias (which means it performs poorly even on your training data) then one thing to try is adding more features.\n",
        "    - Adopt more powerful model\n",
        "    - Better feature selection\n",
        "    - Reduce regularization\n",
        "\n",
        "- If your model has high variance, then you can similarly remove features. But another solution is to obtain more data (if you can).\n",
        "    - Decrease model complexity\n",
        "    - Increase training data sample size\n",
        "    - Regularization (parameter tuning, may increase bias)\n",
        "\n",
        "Holding model complexity constant, the more data you have, the harder it is to overfit.\n",
        "On the other hand, more data won’t help with bias. If your model doesn’t use enough\n",
        "features to capture regularities in the data, throwing more data at it won’t help.\n",
        "\n",
        "<img src=\"https://github.com/anyuanay/INFO213/blob/main/underfitting-overfitting.png?raw=true\" width=\"600px\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcpQMlMWfiXq"
      },
      "source": [
        "# Streamlining Workflows with Pipeline\n",
        "\n",
        "- ## How to create and use a pileline?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsAVnoCSfiXq"
      },
      "source": [
        "- Prior to training a model, we may apply a variety of preprocessing techniques on the data, such as standardization for feature scaling.\n",
        "- We have to reuse the parameters that were obtained during the fitting of the training data to scale and compress any new data.\n",
        "- Scikit-learn provides an extremely handy tool, the `Pipeline`\n",
        "class that allows us to fit a model including an arbitrary number of transformation steps and apply it to make predictionsabout new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVtREdLyfiXq"
      },
      "source": [
        "## We will use the Breast Cancer Wisconsin dataset for illustration:\n",
        "### Load the data from the file:\n",
        "- We will be working with the Breast Cancer Wisconsin dataset, which contains 569 examples of malignant and benign tumor cells.\n",
        "- The first two columns in the dataset store the unique ID\n",
        "numbers of the examples and the corresponding diagnoses (M = malignant, B = benign), respectively.\n",
        "- Columns 3-32 contain 30 real-valued features that have been computed from digitized images of the cell nuclei, which can be used to build a model to predict whether a tumor is benign or malignant."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "```"
      ],
      "metadata": {
        "id": "EErHv-BhNXpq",
        "outputId": "19ecb682-abd0-4cdc-bcd9-a60c255c2be8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nigzIs9cfiXq",
        "outputId": "6874529d-cc93-4646-a959-04d4bea83e94"
      },
      "source": [
        "```python\n",
        "df = pd.read_csv(\"wdbc.data\", header=None)\n",
        "\n",
        "df.head()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHha1V5lfiXq",
        "outputId": "59487d06-1fbe-42a7-a360-7bd4752bc101"
      },
      "source": [
        "```python\n",
        "df.shape\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhUcpnWifiXq"
      },
      "source": [
        "We will assign the 30 features to a NumPy array, X. Using a\n",
        "LabelEncoder object, we will transform the class labels from their original string representation ('M' and 'B') into integers:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txWT8KXofiXq"
      },
      "source": [
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X = df.loc[:, 2:].values\n",
        "y = df.loc[:, 1].values\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "X.shape, y.shape\n",
        "```"
      ],
      "metadata": {
        "id": "YJ2zhKEGN5yG",
        "outputId": "78f56bcb-ad47-4e5c-c8f6-03a8167a994b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "le.classes_\n",
        "```"
      ],
      "metadata": {
        "id": "2VRGFOu4ueEO",
        "outputId": "a25ba701-ef22-4f35-b5b9-5d54b9c29d4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- After encoding the class labels (diagnosis) in an array, y, the malignant tumors are now represented as class 1, and the benign tumorsare represented as class 0, respectively.\n",
        "- We can double-check this mapping by calling the transform method of the fitted LabelEncoder on two dummy class labels:"
      ],
      "metadata": {
        "id": "oJV3CRh2Lfia"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLsJZWFPfiXr",
        "outputId": "6f1f4989-105c-4336-ec9a-4f01e40b9026"
      },
      "source": [
        "```python\n",
        "le.transform(['M', 'B'])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide the dataset into a separate training dataset (80 percentof the data) and a separate test dataset (20 percent of the data):"
      ],
      "metadata": {
        "id": "XG2TbBjILx-g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbq-G4cNfiXr"
      },
      "source": [
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(X, y,\n",
        "                     test_size=0.20,\n",
        "                     stratify=y,\n",
        "                     random_state=1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADuA8MErfiXr"
      },
      "source": [
        "## How to combine transformers and estimators in a pipeline?\n",
        "- We will standardize the columns in the Breast Cancer\n",
        "Wisconsin dataset before we feed them to a linear classifier, such as logistic regression.\n",
        "- We can also compress our data from the initial 30 dimensions onto a lower two-dimensional subspace via principal component analysis (PCA), a feature extraction technique for dimensionality reduction.\n",
        "- We can chain the StandardScaler and LogisticRegression\n",
        "objects in a pipeline (with or without PCA):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOmllOWEfiXr"
      },
      "source": [
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipe_lr = make_pipeline(StandardScaler(),\n",
        "                        PCA(n_components=2),\n",
        "                        LogisticRegression(random_state=1, solver='lbfgs'))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "pipe_lr.fit(X_train, y_train)\n",
        "y_pred = pipe_lr.predict(X_test)\n",
        "print('Test Accuracy: %.3f' % pipe_lr.score(X_test, y_test))\n",
        "```"
      ],
      "metadata": {
        "id": "67t_iU9Vu37_",
        "outputId": "4802639f-1c78-421a-f12e-c8c75cf9712c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipelines of the scikit-learn library are immensely useful wrapper tools. To make sure that you've got a good grasp of how the Pipeline object works, please take a close look at the following illustration:\n",
        "<img src=\"https://i.imgur.com/1vxItHg.png\" width = 800>"
      ],
      "metadata": {
        "id": "jtKHDgJ0MwQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explain the pipeline:\n",
        "- The make_pipeline function takes an arbitrary number of scikit-learn transformers (objects that support\n",
        "the fit and transform methods as input), followed by a scikit-learn estimator that implements the\n",
        "fit and predict methods.\n",
        "    - In our example, we provided two scikit-learn transformers,\n",
        "StandardScaler and PCA, and a LogisticRegression estimator as inputs to the make_pipeline function.\n",
        "- We can think of a scikit-learn Pipeline as a meta-estimator or wrapper around those individual transformers and estimators.\n",
        "- If we call the fit method of Pipeline, the data will be passed down a series of transformers via fit and transform calls on these intermediate steps until it arrives at the\n",
        "estimator object.\n",
        "- The estimator will then be fitted to the transformed\n",
        "training data.\n",
        "- We should note that there is no limit to the number of\n",
        "intermediate steps in a pipeline; however, if we want to use the pipeline for prediction tasks, the last\n",
        "pipeline element has to be an estimator.\n",
        "- Similar to calling fit on a pipeline, pipelines also implement a predict method if the last step in the\n",
        "pipeline is an estimator.\n",
        "    - If we feed a dataset to the predict call of a Pipeline object instance, the data will pass through the intermediate steps via transform calls. In the final step, the estimator object will then return a prediction on the transformed data.\n"
      ],
      "metadata": {
        "id": "eX9s5MGniL2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Practice on Previous Weeks"
      ],
      "metadata": {
        "id": "IMI_GK5klL0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the data points\n"
      ],
      "metadata": {
        "id": "8ErsgzMPQhnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_train)\n",
        "for label in [0, 1]:\n",
        "    plt.scatter(\n",
        "        X_pca[y_train == label, 0],\n",
        "        X_pca[y_train == label, 1],\n",
        "        label=f'Class {label}',\n",
        "        alpha=0.7\n",
        "    )\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA of Training Data')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "HdbUjLYLQ6Km",
        "outputId": "da23b96b-958a-4d71-ce2d-a76c9690dd2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "X_test_pca = pca.fit_transform(X_test)\n",
        "for label in [0, 1]:\n",
        "    plt.scatter(\n",
        "        X_test_pca[y_pred == label, 0],\n",
        "        X_test_pca[y_pred == label, 1],\n",
        "        label=f'Class {label}',\n",
        "        alpha=0.7\n",
        "    )\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA of Classified Test Data')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "nLh2NBVPSiNA",
        "outputId": "390c3234-1ad7-4006-88f4-dd0ac6de9fa1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline without PCA"
      ],
      "metadata": {
        "id": "bMSpXMb9OyZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "pipe_lr = make_pipeline(StandardScaler(),\n",
        "                        LogisticRegression(random_state=1, solver='lbfgs'))\n",
        "```"
      ],
      "metadata": {
        "id": "hBenIcT3O2IA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "pipe_lr.fit(X_train, y_train)\n",
        "```"
      ],
      "metadata": {
        "id": "rDbIN81TO7Tz",
        "outputId": "bd053b0b-73e8-4819-e809-569d5fd91013"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "y_pred = pipe_lr.predict(X_test)\n",
        "```"
      ],
      "metadata": {
        "id": "CA6XshenO_nT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from sklearn.metrics import confusion_matrix\n",
        "```"
      ],
      "metadata": {
        "id": "iGh8TH2cPvAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "confusion_matrix(y_test, y_pred)\n",
        "```"
      ],
      "metadata": {
        "id": "r_kZQs1DQBl6",
        "outputId": "35b30a9a-5614-4fec-9b27-6d79bcbd548d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "```"
      ],
      "metadata": {
        "id": "PARofvH1PJ9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "precision_recall_fscore_support(y_test, y_pred)\n",
        "```"
      ],
      "metadata": {
        "id": "OdHKIpFxPWbx",
        "outputId": "de5d77c0-c674-4435-814b-92c78fd1c208"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzRwBRPqfiXr"
      },
      "source": [
        "# K-Fold Cross Validation for Assessing Model Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6mohwPYfiXr"
      },
      "source": [
        "## What is the heldout method and what is the purpose?\n",
        "- In typical machine learning applications, we are interested in tuning and comparing different parameter settings to further improve the performance for making predictions on unseen data. - This process is called **model selection**, with the name referring to a given classification problem for which we want to select the optimal values of tuning parameters (also called hyperparameters).\n",
        "- In machine learning, a \"held-out\" set is a portion of the dataset that is intentionally set aside and not used during the training of a model.\n",
        "- The held-out set is reserved for evaluation purposes.\n",
        "- The main idea is to have a separate dataset that the model has not seen during training, allowing us to assess how well the model generalizes to new, unseen data.\n",
        "\n",
        "<img src=\"https://i.imgur.com/t6Nyb0t.png\" width=800>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why do we need multiple held-out sets?\n",
        "- The evaluation may be sensitive to how to choose a single held-out set.\n",
        "- We can obtain a more reliable estimate of a model's performance by using multiple subsets of the dataset for training and testing.  \n",
        "- Cross validation allows us to split the training data into folds for more robust evaluation."
      ],
      "metadata": {
        "id": "577wEfW8H00d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYCqK_2LfiXr"
      },
      "source": [
        "## What is K-fold cross-validation?\n",
        "- K-fold cross-validation avoids overlapping test sets\n",
        "    - First step: split data into k subsets of equal size\n",
        "    - Second step: use each subset in turn for testing, the remainder for training\n",
        "- This means the learning algorithm is applied to k different training sets\n",
        "- The error estimates are averaged to yield an overall error estimate; also, standard deviation is often computed\n",
        "\n",
        "<img src=\"https://i.imgur.com/DfOfyn8.png\" width=800>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Stratified K-Fold cross validation?\n",
        "- A slight improvement over the standard k-fold cross-validation approach is stratified k-fold cross-validation.\n",
        "- In stratified cross-validation, the class label proportions\n",
        "are preserved in each fold to ensure that each fold is representative of the class proportions in the training dataset."
      ],
      "metadata": {
        "id": "NoIappCeQO4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "pd.value_counts(y_train) / y_train.shape[0]\n",
        "```"
      ],
      "metadata": {
        "id": "0m_ZTlpPTQ5l",
        "outputId": "0d692ac7-fd58-4b52-bb71-97fd12fb6ec9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "X_train.shape, y_train.shape\n",
        "```"
      ],
      "metadata": {
        "id": "NUe4lnvvUTJy",
        "outputId": "b60ce011-59a2-4284-a171-16b74d96e510"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omMaiBl9fiXs"
      },
      "source": [
        "```python\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=10).split(X_train, y_train)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "for k, (train, test) in enumerate(kfold):\n",
        "    train_counts = np.bincount(y_train[train])\n",
        "    test_counts = np.bincount(y_train[test])\n",
        "\n",
        "    train_percent = 100 * train_counts / train_counts.sum()\n",
        "    test_percent = 100 * test_counts / test_counts.sum()\n",
        "\n",
        "    print('Fold: %2d, Train class dist.: %s, Test class dist: %s' % (\n",
        "        k + 1,\n",
        "        np.array2string(train_percent, precision=1, separator=', '),\n",
        "        np.array2string(test_percent, precision=1, separator=', ')\n",
        "    ))\n",
        "```"
      ],
      "metadata": {
        "id": "Bzt43UIKUCh2",
        "outputId": "8d2633aa-2010-4140-f746-c92505517af7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "scores = []\n",
        "for k, (train, test) in enumerate(kfold):\n",
        "    pipe_lr.fit(X_train[train], y_train[train])\n",
        "    score = pipe_lr.score(X_train[test], y_train[test])\n",
        "    scores.append(score)\n",
        "    print('Fold: %2d, Class dist.: %s, Acc: %.3f' % (k+1,\n",
        "          np.bincount(y_train[train]), score))\n",
        "```"
      ],
      "metadata": {
        "id": "sH-he1_LwLhz",
        "outputId": "ffa9ff13-c411-4636-9ed7-cd1cd7768b2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n",
        "```"
      ],
      "metadata": {
        "id": "bM9HsTu-wRcB",
        "outputId": "a43eada7-a9fa-4c45-c717-065389145307"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to choose the numbe of folds in cross-validation?\n",
        "- Standard method for evaluation: stratified ten-fold cross-validation\n",
        "- Why ten?\n",
        "    - Extensive experiments have shown that this is the best choice to get an accurate estimate\n",
        "    - There is also some theoretical evidence for this\n",
        "Stratification reduces the estimate’s variance\n",
        "- Even better: repeated stratified cross-validation\n",
        "E.g., ten-fold cross-validation is repeated ten times and results are averaged (reduces the variance)"
      ],
      "metadata": {
        "id": "w65WDlBbJBjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scikit-Learn K-Fold Cross-Validation Scorer\n",
        "- For convenience, Scikit-Learn also implements a k-fold cross-validation scorer, which allows us to evaluate our model using stratified k-fold cross-validation less verbosely.\n",
        "- An extremely useful feature of the cross_val_score approach is that we can distribute the evaluation of the different folds across multiple central processing units (CPUs) on our machine. - If we set the `n_jobs=2`, we could distribute the 10 rounds\n",
        "of cross-validation to two CPUs (if available on our machine), and by setting `n_jobs=-1`, we can use all available CPUs on our machine to do the computation in parallel."
      ],
      "metadata": {
        "id": "hab0vMTXQEPb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD5AsXsUfiXs",
        "outputId": "7b19110a-8705-4264-8c04-0ce67d00b837"
      },
      "source": [
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(estimator=pipe_lr,\n",
        "                         X=X_train,\n",
        "                         y=y_train,\n",
        "                         cv=10,\n",
        "                         n_jobs=1)\n",
        "print('CV accuracy scores: %s' % scores)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n",
        "```"
      ],
      "metadata": {
        "id": "8mt8KtJmwXcw",
        "outputId": "c696da32-9898-45af-fb1b-a384412e86fc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkRDfnS3fiXs"
      },
      "source": [
        "# Debugging Algorithms with Learning and Validation Curves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAeDeSQFfiXs"
      },
      "source": [
        "- Two very simple yet powerful diagnostic tools that can help us to improve the performance of a learning algorithm: learning curves and validation curves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxnBTD7NfiXs"
      },
      "source": [
        "## Diagnosing bias and variance problems with learning curves\n",
        "\n",
        "- If a model is too complex for a given training dataset,\n",
        "it can help to collect more training examples to reduce the degree of overfitting.\n",
        "- However, in practice, it can often be very expensive or simply not feasible to collect more data.\n",
        "- By plotting the model training and validation accuracies as functions of the training dataset size, we can\n",
        "easily detect whether the model suffers from high variance or high bias, and whether the collection\n",
        "of more data could help to address this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use the learning_curve in scikit-learn?\n",
        "\n",
        "<img src=\"https://i.imgur.com/zWLPRXO.png\" width=800>\n",
        "\n",
        "- The graph in the upper left shows a model with a high bias. This model has both low training and cross-validation accuracy.\n",
        "- The graph in the upper-right shows a model that suffers from high variance, which is indicated by the\n",
        "large gap between the training and cross-validation accuracy.\n"
      ],
      "metadata": {
        "id": "BXp_WGeJxeUn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LNOcCKifiXx"
      },
      "source": [
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "\n",
        "pipe_lr = make_pipeline(StandardScaler(),\n",
        "                        LogisticRegression(penalty='l2', random_state=1,\n",
        "                                           solver='lbfgs', max_iter=10000))\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "train_sizes, train_scores, test_scores =\\\n",
        "                learning_curve(estimator=pipe_lr,\n",
        "                               X=X_train,\n",
        "                               y=y_train,\n",
        "                               train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "                               cv=10,\n",
        "                               n_jobs=1)\n",
        "```"
      ],
      "metadata": {
        "id": "ZgD5iTYGw61b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "test_mean = np.mean(test_scores, axis=1)\n",
        "test_std = np.std(test_scores, axis=1)\n",
        "```"
      ],
      "metadata": {
        "id": "8oYJeGlpxClA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "plt.plot(train_sizes, train_mean,\n",
        "         color='blue', marker='o',\n",
        "         markersize=5, label='Training accuracy')\n",
        "\n",
        "plt.fill_between(train_sizes,\n",
        "                 train_mean + train_std,\n",
        "                 train_mean - train_std,\n",
        "                 alpha=0.15, color='blue')\n",
        "\n",
        "plt.plot(train_sizes, test_mean,\n",
        "         color='green', linestyle='--',\n",
        "         marker='s', markersize=5,\n",
        "         label='Validation accuracy')\n",
        "\n",
        "plt.fill_between(train_sizes,\n",
        "                 test_mean + test_std,\n",
        "                 test_mean - test_std,\n",
        "                 alpha=0.15, color='green')\n",
        "\n",
        "plt.grid()\n",
        "plt.xlabel('Number of training examples')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim([0.8, 1.03])\n",
        "plt.tight_layout()\n",
        "# plt.savefig('images/06_05.png', dpi=300)\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "72d752BVxHta",
        "outputId": "1dca5ac4-d54e-4dac-d6c8-2389a9f8b1b3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As we can see in the preceding learning curve plot, our model performs quite well on both the training and validation datasets if it has seen more than 250 examples during training.\n",
        "- We can also see that the\n",
        "training accuracy increases for training datasets with fewer than 250 examples, and the gap between\n",
        "validation and training accuracy widens—an indicator of an increasing degree of overfitting."
      ],
      "metadata": {
        "id": "XZ_ikmLxmaNW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNLzVpv4fiXx"
      },
      "source": [
        "## How to address over- and underfitting with validation curves in scikit-learn?\n",
        "\n",
        "- Validation curves are related to learning curves.\n",
        "- But instead of plotting the training and test accuracies as functions of the sample size, we vary the values of the model parameters, for example, the inverse regularization parameter, C, in logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOgxzAqRfiXx"
      },
      "source": [
        "```python\n",
        "from sklearn.model_selection import validation_curve\n",
        "\n",
        "\n",
        "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "train_scores, test_scores = validation_curve(\n",
        "                estimator=pipe_lr,\n",
        "                X=X_train,\n",
        "                y=y_train,\n",
        "                param_name='logisticregression__C',\n",
        "                param_range=param_range,\n",
        "                cv=10)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "test_mean = np.mean(test_scores, axis=1)\n",
        "test_std = np.std(test_scores, axis=1)\n",
        "```"
      ],
      "metadata": {
        "id": "aXY_nLbFxW4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "plt.plot(param_range, train_mean,\n",
        "         color='blue', marker='o',\n",
        "         markersize=5, label='Training accuracy')\n",
        "\n",
        "plt.fill_between(param_range, train_mean + train_std,\n",
        "                 train_mean - train_std, alpha=0.15,\n",
        "                 color='blue')\n",
        "\n",
        "plt.plot(param_range, test_mean,\n",
        "         color='green', linestyle='--',\n",
        "         marker='s', markersize=5,\n",
        "         label='Validation accuracy')\n",
        "\n",
        "plt.fill_between(param_range,\n",
        "                 test_mean + test_std,\n",
        "                 test_mean - test_std,\n",
        "                 alpha=0.15, color='green')\n",
        "\n",
        "plt.grid()\n",
        "plt.xscale('log')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel('Parameter C')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.8, 1.0])\n",
        "plt.tight_layout()\n",
        "# plt.savefig('images/06_06.png', dpi=300)\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "cjdVRliIxaHr",
        "outputId": "930732a9-1cc3-42cb-db4d-b9e2f6d5433d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Similar to the learning_curve function, the validation_curve function uses stratified k-fold cross-validation\n",
        "by default to estimate the performance of the classifier.\n",
        "- Although the differences in the accuracy for varying values of C are subtle, we can see that the model slightly underfits the data when we increase the regularization strength (small values of C).\n",
        "- However, for large values of C, it means lowering the strength of regularization, so the model tends to slightly\n",
        "overfit the data. In this case, the sweet spot appears to be between 0.01 and 0.1 of the C value."
      ],
      "metadata": {
        "id": "lWbMa71un428"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Selection\n",
        "- Cross validation evaluates the robust performance on a single model.\n",
        "- How to systematically evaluate multiple models and selection the right one?\n",
        "- Grid search is a hyperparameter tuning technique used in machine learning to systematically search for the optimal combination of hyperparameter values for a given model."
      ],
      "metadata": {
        "id": "xBl__eyTJwSf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p1hv17XfiXx"
      },
      "source": [
        "# How to fine-tuning machine learning models via grid search?\n",
        "- There two types of parameters: those that are learned from the training data, for\n",
        "example, the weights in logistic regression, and the parameters of a learning algorithm that are optimized separately.\n",
        "- The latter are the tuning parameters (or hyperparameters) of a model, for example, the regularization parameter in logistic regression or the depth parameter of a decision tree.\n",
        "- Grid search is a popular hyperparameter optimization\n",
        "technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IZho6bKfiXx"
      },
      "source": [
        "## What is grid-search and how to use it in scikit-learn to tune hyperparameters?\n",
        "- It's a brute-force exhaustive search paradigm.\n",
        "- We specify a list of values for different\n",
        "hyperparameters.\n",
        "- The computer evaluates the model performance for each combination to obtain the optimal combination of values from this set.\n",
        "\n",
        "<img src=\"https://github.com/anyuanay/INFO213/blob/main/grid-search-2d.png?raw=true\" width=\"600px\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wcOeXp5fiXx"
      },
      "source": [
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pipe_lr = make_pipeline(StandardScaler(),\n",
        "                        PCA(n_components=2),\n",
        "                        LogisticRegression(random_state=1))\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "param_range = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "\n",
        "param_grid = {'logisticregression__C': param_range, 'logisticregression__solver': ('lbfgs', 'liblinear')}\n",
        "\n",
        "gs = GridSearchCV(estimator=pipe_lr,\n",
        "                  param_grid=param_grid,\n",
        "                  scoring='accuracy',\n",
        "                  refit=True,\n",
        "                  cv=10,\n",
        "                  n_jobs=-1)\n",
        "```"
      ],
      "metadata": {
        "id": "Ud7xmkoTyOoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "gs = gs.fit(X_train, y_train)\n",
        "```"
      ],
      "metadata": {
        "id": "o6lYf-NJyR6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "print(gs.best_score_)\n",
        "print(gs.best_params_)\n",
        "```"
      ],
      "metadata": {
        "id": "be867znEyUou",
        "outputId": "8a36912a-d8d6-44ce-e77d-ba784d7f4b7e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE8qTpe3fiXy",
        "outputId": "448fff6e-29a2-4a99-978d-9f9b4fec507d"
      },
      "source": [
        "```python\n",
        "clf = gs.best_estimator_\n",
        "\n",
        "# clf.fit(X_train, y_train)\n",
        "# note that we do not need to refit the classifier\n",
        "# because this is done automatically via refit=True.\n",
        "\n",
        "print('Test accuracy: %.3f' % clf.score(X_test, y_test))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring hyperparameter configurations more widely with randomized search\n",
        "\n",
        "- Specifying large hyperparameter\n",
        "grids makes grid search very expensive in practice.\n",
        "- An alternative approach for sampling different\n",
        "parameter combinations is randomized search.\n",
        "- We draw hyperparameter configurations randomly from distributions (or discrete sets).\n",
        "- Randomized search does not do an exhaustive search over the hyperparameter space.\n",
        "- Still, it allows us to explore a wider range of hyperparameter value settings in a more cost- and time-effective manner."
      ],
      "metadata": {
        "id": "_eHaa8P9psJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "import scipy\n",
        "param_range = scipy.stats.loguniform(0.0001, 1000.0)\n",
        "```"
      ],
      "metadata": {
        "id": "WA7xDwG5q57H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Using a loguniform distribution instead of a regular uniform distribution will ensure that in a sufficiently large number of trials, the same number of samples will be drawn from the\n",
        "[0.0001, 0.001] range as, for example, the [10.0, 100.0] range. - To check its behavior, we can draw 10\n",
        "random samples from this distribution via the rvs(10) method, as shown here:"
      ],
      "metadata": {
        "id": "Aw5JlAPDrshs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "np.random.seed(1)\n",
        "param_range.rvs(10)\n",
        "```"
      ],
      "metadata": {
        "id": "9DmqQ2MSr0qs",
        "outputId": "a3386d51-e1b6-47e4-cd68-fc4c3de3792b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now see the RandomizedSearchCV in action and tune a LogisticRegression as we did with GridSearchCV:"
      ],
      "metadata": {
        "id": "QTVRzQ5cr904"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "```"
      ],
      "metadata": {
        "id": "eCmFxUR2sGhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "param_grid = {'logisticregression__C': param_range, 'logisticregression__solver': ('lbfgs', 'liblinear')}\n",
        "\n",
        "rs = RandomizedSearchCV(estimator=pipe_lr,\n",
        "                  param_distributions=param_grid,\n",
        "                  scoring='accuracy',\n",
        "                  refit=True,\n",
        "                  cv=10,\n",
        "                  n_jobs=-1)\n",
        "```"
      ],
      "metadata": {
        "id": "H-KwM1xmsSNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "rs.fit(X_train, y_train)\n",
        "```"
      ],
      "metadata": {
        "id": "ng4nWN6tsplQ",
        "outputId": "ee9a2b6f-179f-406e-f3f4-c7b9c6df6fbf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "print(rs.best_score_)\n",
        "print(rs.best_params_)\n",
        "```"
      ],
      "metadata": {
        "id": "nYCkT5jds0No",
        "outputId": "64e6b082-2962-440a-db74-4e3aa70d4bf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "clf = rs.best_estimator_\n",
        "\n",
        "print('Test accuracy: %.3f' % clf.score(X_test, y_test))\n",
        "```"
      ],
      "metadata": {
        "id": "dYLDQc5Ss6ZO",
        "outputId": "eef1203f-d341-4e31-bc85-66d5ac961046"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KECYM11pfiXy"
      },
      "source": [
        "## What is the nested cross-validation? How to use it?\n",
        "\n",
        "- Using k-fold cross-validation in combination with grid search or randomized search is a useful approach\n",
        "for fine-tuning the performance of a machine learning model by varying its hyperparameter values.\n",
        "- If we want to select among different machine\n",
        "learning algorithms, though, another recommended approach is nested cross-validation.\n",
        "- In nested cross-validation, we have an outer k-fold cross-validation loop to split the data into training and test folds.\n",
        "- An inner loop is used to select the model using k-fold cross-validation on the training fold.\n",
        "- After model selection, the test fold is then used to evaluate the model performance.\n",
        "<img src=\"https://i.imgur.com/6ny8VBz.png\" width=800>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_6qnxAgfiXy",
        "outputId": "c249f9b5-faf6-499f-ac16-c55bd225e7e1"
      },
      "source": [
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "param_range = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "\n",
        "param_grid = {'logisticregression__C': param_range, 'logisticregression__solver': ('lbfgs', 'liblinear')}\n",
        "\n",
        "\n",
        "gs = GridSearchCV(estimator=pipe_lr,\n",
        "                  param_grid=param_grid,\n",
        "                  scoring='accuracy',\n",
        "                  cv=2)\n",
        "\n",
        "scores = cross_val_score(gs, X_train, y_train,\n",
        "                         scoring='accuracy', cv=5)\n",
        "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores),\n",
        "                                      np.std(scores)))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the nested cross-validation approach to compare a logistic regression model to a simple decision tree classifier:"
      ],
      "metadata": {
        "id": "d8CXBePddKBa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE0Hqqy8fiXy",
        "outputId": "5bda3da1-c26c-43e3-dba1-fe272023b48d"
      },
      "source": [
        "```python\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "gs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0),\n",
        "                  param_grid=[{'max_depth': [1, 2, 3, 4, 5, 6, 7, None]}],\n",
        "                  scoring='accuracy',\n",
        "                  cv=2)\n",
        "\n",
        "scores = cross_val_score(gs, X_train, y_train,\n",
        "                         scoring='accuracy', cv=5)\n",
        "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores),\n",
        "                                      np.std(scores)))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the nested cross-validation performance of the Logist Regression model (94.1 percent) is better than the performance of the decision tree (93.4 percent), and thus, we'd expect that it might be the better choice to classify new data that comes from the same population as this particular dataset."
      ],
      "metadata": {
        "id": "ZaHn3V2NdYKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Looking at Different Performance Evaluation Metrics\n",
        "## Precision, Recall, and F-Score\n",
        "\n",
        "Given a set of labeled data and a predictive model, every data point lies in one of four categories:\n",
        "* True positive (tp): “This message is spam, and we correctly predicted spam.”\n",
        "* False positive (fp) (Type 1 Error): “This message is not spam, but we predicted spam.”\n",
        "* False negative (fn) (Type 2 Error): “This message is spam, but we predicted not\n",
        "spam.”\n",
        "* True negative (tn): “This message is not spam, and we correctly predicted not spam.”\n",
        "\n",
        "We often represent these as counts in a confusion matrix:\n",
        "\n",
        "|      |positive | negative |\n",
        "|------|------|------|\n",
        "|positive|tp |fp|\n",
        "|negative|fn | tn|\n",
        "\n",
        "$accuracy = \\frac{tp + tn}{tp + fp+tn+fn}$\n",
        "\n",
        "$precision = \\frac{tp}{tp+fp}$\n",
        "\n",
        "$recall = \\frac{tp}{tp+fn}$\n",
        "\n",
        "$F1\\_Score = \\frac{2}{\\frac{1}{recall} + \\frac{1}{precision}}$"
      ],
      "metadata": {
        "id": "ygcvnYPsgb6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "pipe_lr = make_pipeline(StandardScaler(),\n",
        "                        LogisticRegression(random_state=1, solver='lbfgs'))\n",
        "```"
      ],
      "metadata": {
        "id": "uhrW2hbowOx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "pipe_lr.fit(X_train, y_train)\n",
        "```"
      ],
      "metadata": {
        "id": "3cv0wdl-wRAO",
        "outputId": "b6ab3542-20a2-4d6c-c31d-9d1e6ce268e3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "y_pred = pipe_lr.predict(X_test)\n",
        "```"
      ],
      "metadata": {
        "id": "eaeamYzKwUSX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from sklearn.metrics import confusion_matrix\n",
        "```"
      ],
      "metadata": {
        "id": "Spv_rs_5wYGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "confmat = confusion_matrix(y_test, y_pred)\n",
        "confmat\n",
        "```"
      ],
      "metadata": {
        "id": "lFos-NbvwbeJ",
        "outputId": "b188c30d-0bcb-45fe-ad83-1814ac9bc946"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "fig, ax = plt.subplots(figsize=(2.5, 2.5))\n",
        "ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
        "for i in range(confmat.shape[0]):\n",
        "    for j in range(confmat.shape[1]):\n",
        "        ax.text(x=j, y=i, s=confmat[i, j],\n",
        "                va='center', ha='center')\n",
        "ax.xaxis.set_ticks_position('bottom')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.show()\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "PQQ7BFtZdUdH",
        "outputId": "52eaee9d-15ba-409d-ceed-c21c3552c67e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting a receiver operating characteristic\n",
        "- Receiver operating characteristic (ROC) graphs are useful tools to select models for classification\n",
        "based on their performance with respect to the false positive rate (FPR) and true positive rate (TPR).\n",
        "- The diagonal of a ROC graph can be interpreted as random guessing, and classification models that fall below the diagonal are considered as worse than random guessing.\n",
        "- A perfect classifier would fall into the top-left corner of the graph with a TPR of 1 and an FPR of 0.\n",
        "- Based on the ROC curve, we can then compute the so-called ROC area under the curve (ROC AUC) to characterize the performance of a classification model.\n",
        "- Similar to ROC curves, we can compute precision-recall curves for different probability thresholds of a classifier.\n",
        "- A function for plotting those precision-recall curves is also implemented in scikit-learn.\n"
      ],
      "metadata": {
        "id": "3pXH2eslxoIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from numpy import interp\n",
        "\n",
        "pipe_lr = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    PCA(n_components=2),\n",
        "    LogisticRegression(penalty='l2', random_state=1,\n",
        "        solver='lbfgs', C=100.0)\n",
        "    )\n",
        "```"
      ],
      "metadata": {
        "id": "WxCns0Day8pk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "# we are only using two features this time. This is to make the classification task more challenging for the\n",
        "# classifier, by withholding useful information contained in the other features, so that the resulting\n",
        "# ROC curve becomes visually more interesting.\n",
        "X_train2 = X_train[:, [4, 14]]\n",
        "```"
      ],
      "metadata": {
        "id": "5Ll3K-BkzFBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "# For similar reasons, we are also reducing the number\n",
        "# of folds in the StratifiedKFold validator to three.\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "cv = list(StratifiedKFold(n_splits=3).split(X_train, y_train))\n",
        "```"
      ],
      "metadata": {
        "id": "4bs-XSDIzban"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "fig = plt.figure(figsize=(7, 5))\n",
        "\n",
        "mean_tpr = 0.0\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "all_tpr = []\n",
        "\n",
        "for i, (train, test) in enumerate(cv):\n",
        "    probas = pipe_lr.fit(X_train2[train],\n",
        "                         y_train[train]).predict_proba(X_train2[test])\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_train[test],\n",
        "                                     probas[:, 1],\n",
        "                                     pos_label=1)\n",
        "    mean_tpr += interp(mean_fpr, fpr, tpr)\n",
        "    mean_tpr[0] = 0.0\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr,\n",
        "             tpr,\n",
        "             label=f'ROC fold {i+1} (area = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1],\n",
        "         [0, 1],\n",
        "         linestyle='--',\n",
        "         color=(0.6, 0.6, 0.6),\n",
        "         label='Random guessing (area = 0.5)')\n",
        "\n",
        "mean_tpr /= len(cv)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = auc(mean_fpr, mean_tpr)\n",
        "plt.plot(mean_fpr, mean_tpr, 'k--',\n",
        "         label=f'Mean ROC (area = {mean_auc:.2f})', lw=2)\n",
        "plt.plot([0, 0, 1],\n",
        "         [0, 1, 1],\n",
        "         linestyle=':',\n",
        "         color='black',\n",
        "         label='Perfect performance (area = 1.0)')\n",
        "\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.tight_layout()\n",
        "# plt.savefig('figures/06_10.png', dpi=300)\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "8elMl4t_z0wh",
        "outputId": "1a300043-525b-41f8-bc8f-071bed063ff5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We interpolated the average ROC curve from the three folds via the interp function that we imported from SciPy and calculated the area under the curve via the auc function.\n",
        "- The resulting ROC curve indicates that there is a certain degree of variance between the different folds, and the average\n",
        "ROC AUC (0.76) falls between a perfect score (1.0) and random guessing (0.5).\n",
        "- Note that if we are just interested in the ROC AUC score, we could also directly import the roc_auc_score function from the sklearn.metrics.\n",
        "- Reporting the performance of a classifier as the ROC AUC can yield further insights into a classifier's performance with respect to imbalanced samples."
      ],
      "metadata": {
        "id": "kigYZm9e0OnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dealing with class imbalance\n",
        "- Class imbalance is a quite common problem when working with real-world data.\n",
        "- Imagine that the Breast Cancer Wisconsin dataset that we've been working with in this chapter consisted of 90 percent healthy patients.\n",
        "- Training a model on such a dataset that achieves approximately\n",
        "90 percent test accuracy would mean our model hasn't learned anything useful from the features provided in this dataset.\n",
        "- Let us create an imbalanced dataset from our dataset, which originally consisted of 357 benign tumors (class 0) and 212 malignant tumors (class 1):\n"
      ],
      "metadata": {
        "id": "1quNpQ8Z0163"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "X_imb = np.vstack((X[y == 0], X[y == 1][:40]))\n",
        "y_imb = np.hstack((y[y == 0], y[y == 1][:40]))\n",
        "```"
      ],
      "metadata": {
        "id": "F82f1CsZ3F1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "X_imb.shape, y_imb.shape\n",
        "```"
      ],
      "metadata": {
        "id": "vREn_LCr3e6C",
        "outputId": "0bbebc32-ee50-4fca-cc51-7b7052ec50f4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "np.mean(y_imb)\n",
        "```"
      ],
      "metadata": {
        "id": "q-FcM_0m3hLI",
        "outputId": "c936f4dd-acce-41e9-e489-211ae0df44c8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- One way to deal with imbalanced class proportions during model fitting is to assign a larger penalty to wrong predictions on the minority class.\n",
        "- Via scikit-learn, adjusting such a penalty is as convenient\n",
        "as setting the class_weight parameter to `class_weight='balanced`, which is implemented for most\n",
        "classifiers.\n",
        "- Other popular strategies for dealing with class imbalance include upsampling the minority class, downsampling the majority class, and the generation of synthetic training examples.\n",
        "- There's no universally best solution or technique that works best across different problem domains.\n",
        "- It is recommended to try out different strategies on a given problem, evaluate tht results, and choose the technique that seems most appropriate.\n",
        "- The scikit-learn library implements a simple resample function that can help with the upsampling of the minority class by drawing new samples from the dataset with replacement."
      ],
      "metadata": {
        "id": "H5BoYn5y4HyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from sklearn.utils import resample\n",
        "\n",
        "print('Number of class 1 examples before:', X_imb[y_imb == 1].shape[0])\n",
        "```"
      ],
      "metadata": {
        "id": "90_ML8aC3y2r",
        "outputId": "c84ce6ca-9610-40cb-dd70-c91133180a42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "X_upsampled, y_upsampled = resample(X_imb[y_imb == 1],\n",
        "                                    y_imb[y_imb == 1],\n",
        "                                    replace=True,\n",
        "                                    n_samples=X_imb[y_imb == 0].shape[0],\n",
        "                                    random_state=123)\n",
        "\n",
        "print('Number of class 1 examples after:', X_upsampled.shape[0])\n",
        "```"
      ],
      "metadata": {
        "id": "LQ3FH-Z84ouS",
        "outputId": "30acd192-dedd-41cc-bac9-fae723fb3e81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "X_bal = np.vstack((X[y == 0], X_upsampled))\n",
        "y_bal = np.hstack((y[y == 0], y_upsampled))\n",
        "```"
      ],
      "metadata": {
        "id": "zLeosb7h4su5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "np.mean(y_bal)\n",
        "```"
      ],
      "metadata": {
        "id": "sntMV9xq4z2I",
        "outputId": "e48881bc-caad-41a1-b5f5-b83418499173"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "gHpXF0mm41e0"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}