{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anyuanay/INFO213/blob/main/INFO213_Week10_more_clustering_lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "9K-U6PnoWYFM"
      },
      "source": [
        "# INFO 213: Data Science Programming 2\n",
        "___\n",
        "\n",
        "### Week 10: More on Clustering Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwuGeVMBWYFX"
      },
      "source": [
        "### Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZUTG-RuWYFX"
      },
      "source": [
        "- [Grouping objects by similarity using k-means](#Grouping-objects-by-similarity-using-k-means)\n",
        "  - [Using the elbow method to find the optimal number of clusters](#Using-the-elbow-method-to-find-the-optimal-number-of-clusters)\n",
        "  - [Quantifying the quality of clustering via silhouette plots](#Quantifying-the-quality-of-clustering-via-silhouette-plots)\n",
        "- [Organizing clusters as a hierarchical tree](#Organizing-clusters-as-a-hierarchical-tree)\n",
        "  - [Grouping clusters in bottom-up fashion](#Grouping-clusters-in-bottom-up-fashion)\n",
        "  - [Performing hierarchical clustering on a distance matrix](#Performing-hierarchical-clustering-on-a-distance-matrix)\n",
        "  - [Attaching dendrograms to a heat map](#Attaching-dendrograms-to-a-heat-map)\n",
        "  - [Applying agglomerative clustering via scikit-learn](#Applying-agglomerative-clustering-via-scikit-learn)\n",
        "- [Locating regions of high density via DBSCAN](#Locating-regions-of-high-density-via-DBSCAN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CyXSLsyWYFX"
      },
      "source": [
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Simple 2-D Dataset"
      ],
      "metadata": {
        "id": "iTdaiVzAJnw8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUDq9rayWYFX"
      },
      "source": [
        "```python\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "\n",
        "X, y = make_blobs(n_samples=150,\n",
        "                  n_features=2,\n",
        "                  centers=3,\n",
        "                  cluster_std=0.5,\n",
        "                  shuffle=True,\n",
        "                  random_state=0)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the data set"
      ],
      "metadata": {
        "id": "3WQgKQkwJ2XU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3FJ_wvNWYFX",
        "outputId": "4673c2e0-43d6-4618-ef92-94cf30f6986c"
      },
      "source": [
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1],\n",
        "            c='white', marker='o', edgecolor='black', s=50)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "#plt.savefig('figures/10_01.png', dpi=300)\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yalMdPRbWYFY"
      },
      "source": [
        "## Using the elbow method to find the optimal number of clusters\n",
        "\n",
        "- One of the main challenges in unsupervised learning is that we do not know the definitive answer.\n",
        "- We don’t have the ground-truth class labels in our dataset that allow us to apply the techniques model evaluation and selection.\n",
        "- To quantify the quality of clustering, we need to use\n",
        "intrinsic metrics—such as the within-cluster SSE (distortion).\n",
        "- Conveniently, we don’t need to compute the within-cluster SSE explicitly when we are using scikit-learn, as it is already accessible via the inertia_ attribute after fitting a KMeans model:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "km = KMeans(n_clusters=3,\n",
        "            init='random',\n",
        "            n_init=10,\n",
        "            max_iter=300,\n",
        "            tol=1e-04,\n",
        "            random_state=0)\n",
        "\n",
        "y_km = km.fit_predict(X)\n",
        "```"
      ],
      "metadata": {
        "id": "kqJCvX_MKf-q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYh8bokxWYFY",
        "outputId": "c9a9441c-4f61-4988-8b2b-cb46e01c88e9"
      },
      "source": [
        "```python\n",
        "print(f'Distortion: {km.inertia_:.2f}')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Based on the within-cluster SSE, we can use a graphical tool, the so-called elbow method, to estimate\n",
        "the optimal number of clusters, k, for a given task.\n",
        "- We can say that if k increases, the distortion will\n",
        "decrease.\n",
        "- This is because the examples will be closer to the centroids they are assigned to.\n",
        "- The idea behind the elbow method is to identify the value of k where the distortion begins to increase most\n",
        "rapidly, which will become clearer if we plot the distortion for different values of k:"
      ],
      "metadata": {
        "id": "Ay2LlR3gLGKB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldsq5i9VWYFY",
        "outputId": "25a67330-2db3-47a1-db28-7ecb9c8dc82d"
      },
      "source": [
        "```python\n",
        "distortions = []\n",
        "for i in range(1, 11):\n",
        "    km = KMeans(n_clusters=i,\n",
        "                init='k-means++',\n",
        "                n_init=10,\n",
        "                max_iter=300,\n",
        "                random_state=0)\n",
        "    km.fit(X)\n",
        "    distortions.append(km.inertia_)\n",
        "plt.plot(range(1, 11), distortions, marker='o')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Distortion')\n",
        "plt.tight_layout()\n",
        "#plt.savefig('figures/10_03.png', dpi=300)\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVfqHk7kWYFY"
      },
      "source": [
        "- As you can see, the elbow is located at k = 3, so this is supporting evidence that k = 3 is indeed a good choice for this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhM_mXnlWYFY"
      },
      "source": [
        "## Quantifying the quality of clustering  via silhouette plots\n",
        "\n",
        "- Another intrinsic metric to evaluate the quality of a clustering is silhouette analysis.\n",
        "- Silhouette analysis can be used as a graphical tool to plot a measure of how tightly grouped the examples\n",
        "in the clusters are.\n",
        "- To calculate the silhouette coefficient of a single example in our dataset, we can apply the following three steps:\n",
        "\n",
        "1. Calculate the cluster cohesion, $a^{(i)}$, as the average distance between an example, $x^{(i)}$, and all\n",
        "other points in the same cluster.\n",
        "2. Calculate the cluster separation, $b^{(i)}$, from the next closest cluster as the average distance between the example, $x^{(i)}$, and all examples in the nearest cluster.\n",
        "3. Calculate the silhouette, $s^{(i)}$, as the difference between cluster cohesion and separation divided\n",
        "by the greater of the two, as shown here:\n",
        "$$\n",
        "s^{(i)} = \\frac{b^{(i)} - a^{(i)}}{max\\{b^{(i)}, a^{(i)}\\}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The silhouette coefficient is bounded in the range –1 to 1.\n",
        "- The silhouette coefficient is 0 if the cluster separation and cohesion are equal (b(i) = a(i)).\n",
        "- The coefficient is 1 if b(i) >> a(i).\n",
        "- The silhouette coefficient is available as silhouette_samples from scikit-learn’s metric module, and\n",
        "optionally, the silhouette_scores function can be imported for convenience.\n",
        "- The silhouette_scores function calculates the average silhouette coefficient across all examples, which is equivalent to numpy.mean(silhouette_samples(...)).\n",
        "- By executing the following code, we will now create a plot of the silhouette coefficients for a k-means clustering with k = 3:"
      ],
      "metadata": {
        "id": "4EDRqfm6Mr5K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxt4RZFmWYFZ",
        "outputId": "ed2192d8-a1f6-4664-c716-698fd01d8a0c"
      },
      "source": [
        "```python\n",
        "import numpy as np\n",
        "from matplotlib import cm\n",
        "from sklearn.metrics import silhouette_samples\n",
        "\n",
        "\n",
        "km = KMeans(n_clusters=3,\n",
        "            init='k-means++',\n",
        "            n_init=10,\n",
        "            max_iter=300,\n",
        "            tol=1e-04,\n",
        "            random_state=0)\n",
        "y_km = km.fit_predict(X)\n",
        "\n",
        "cluster_labels = np.unique(y_km)\n",
        "n_clusters = cluster_labels.shape[0]\n",
        "\n",
        "silhouette_vals = silhouette_samples(X, y_km, metric='euclidean')\n",
        "y_ax_lower, y_ax_upper = 0, 0\n",
        "yticks = []\n",
        "for i, c in enumerate(cluster_labels):\n",
        "    c_silhouette_vals = silhouette_vals[y_km == c]\n",
        "    c_silhouette_vals.sort()\n",
        "    y_ax_upper += len(c_silhouette_vals)\n",
        "    color = cm.jet(float(i) / n_clusters)\n",
        "    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0,\n",
        "             edgecolor='none', color=color)\n",
        "\n",
        "    yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
        "    y_ax_lower += len(c_silhouette_vals)\n",
        "\n",
        "silhouette_avg = np.mean(silhouette_vals)\n",
        "plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "plt.yticks(yticks, cluster_labels + 1)\n",
        "plt.ylabel('Cluster')\n",
        "plt.xlabel('Silhouette coefficient')\n",
        "\n",
        "plt.tight_layout()\n",
        "#plt.savefig('figures/10_04.png', dpi=300)\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Through a visual inspection of the silhouette plot, we can quickly scrutinize the sizes of the different\n",
        "clusters and identify clusters that contain outliers.\n",
        "- However, as you can see in the preceding silhouette plot, the silhouette coefficients are not close to 0\n",
        "and are approximately equally far away from the average silhouette score, which is, in this case, an\n",
        "indicator of good clustering.\n",
        "- Furthermore, to summarize the goodness of our clustering, we added the average silhouette coefficient to the plot (dotted line)."
      ],
      "metadata": {
        "id": "y6xWaX_uN4uD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hfzpsmFWYFZ"
      },
      "source": [
        "- To see what a silhouette plot looks like for a relatively bad clustering, let’s seed the k-means algorithm\n",
        "with only two centroids:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEpmr6o_WYFZ",
        "outputId": "f407f67a-b814-4427-80f1-9668e932b0bc"
      },
      "source": [
        "```python\n",
        "km = KMeans(n_clusters=2,\n",
        "            init='k-means++',\n",
        "            n_init=10,\n",
        "            max_iter=300,\n",
        "            tol=1e-04,\n",
        "            random_state=0)\n",
        "y_km = km.fit_predict(X)\n",
        "\n",
        "plt.scatter(X[y_km == 0, 0],\n",
        "            X[y_km == 0, 1],\n",
        "            s=50,\n",
        "            c='lightgreen',\n",
        "            edgecolor='black',\n",
        "            marker='s',\n",
        "            label='Cluster 1')\n",
        "plt.scatter(X[y_km == 1, 0],\n",
        "            X[y_km == 1, 1],\n",
        "            s=50,\n",
        "            c='orange',\n",
        "            edgecolor='black',\n",
        "            marker='o',\n",
        "            label='Cluster 2')\n",
        "\n",
        "plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n",
        "            s=250, marker='*', c='red', label='Centroids')\n",
        "\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "#plt.savefig('figures/10_05.png', dpi=300)\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O73F5SbMWYFZ",
        "outputId": "7da1b959-1700-4f99-ec6a-056495cd6552"
      },
      "source": [
        "```python\n",
        "cluster_labels = np.unique(y_km)\n",
        "n_clusters = cluster_labels.shape[0]\n",
        "silhouette_vals = silhouette_samples(X, y_km, metric='euclidean')\n",
        "y_ax_lower, y_ax_upper = 0, 0\n",
        "yticks = []\n",
        "for i, c in enumerate(cluster_labels):\n",
        "    c_silhouette_vals = silhouette_vals[y_km == c]\n",
        "    c_silhouette_vals.sort()\n",
        "    y_ax_upper += len(c_silhouette_vals)\n",
        "    color = cm.jet(float(i) / n_clusters)\n",
        "    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0,\n",
        "             edgecolor='none', color=color)\n",
        "\n",
        "    yticks.append((y_ax_lower + y_ax_upper) / 2.)\n",
        "    y_ax_lower += len(c_silhouette_vals)\n",
        "\n",
        "silhouette_avg = np.mean(silhouette_vals)\n",
        "plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "plt.yticks(yticks, cluster_labels + 1)\n",
        "plt.ylabel('Cluster')\n",
        "plt.xlabel('Silhouette coefficient')\n",
        "\n",
        "plt.tight_layout()\n",
        "#plt.savefig('figures/10_06.png', dpi=300)\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i50GfUbfWYFZ"
      },
      "source": [
        "- As you can see in Figure, the silhouettes now have visibly different lengths and widths, which is evidence of a relatively bad or at least suboptimal clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sql9jJ2FWYFZ"
      },
      "source": [
        "# Organizing clusters as a hierarchical tree\n",
        "\n",
        "- One advantage of the hierarchical clustering algorithm is that it allows us to plot dendrograms\n",
        "(visualizations of a binary hierarchical clustering), which can help with the interpretation of\n",
        "the results by creating meaningful taxonomies.\n",
        "- Another advantage of this hierarchical approach is\n",
        "that we do not need to specify the number of clusters upfront."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJAl52J8WYFZ"
      },
      "source": [
        "## Grouping clusters in bottom-up fashion\n",
        "\n",
        "- The two standard algorithms for agglomerative hierarchical clustering are single linkage and complete\n",
        "linkage.\n",
        "- Using single linkage, we compute the distances between the most similar members for each pair of clusters and merge the two clusters for which the distance between the most similar members is the smallest.\n",
        "- The complete linkage approach is similar to single linkage but, instead of comparing the most similar members in each pair of clusters, we compare the most dissimilar members to perform\n",
        "the merge.\n",
        "\n",
        "<img src=\"https://github.com/rasbt/machine-learning-book/blob/main/ch10/figures/10_07.png?raw=true\" width=\"600px\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agglomerative Clustering\n",
        "- Hierarchical complete linkage clustering is an iterative procedure that can be summarized by the following steps:\n",
        "1. Compute a pair-wise distance matrix of all examples.\n",
        "2. Represent each data point as a singleton cluster.\n",
        "3. Merge the two closest clusters based on the distance between the most dissimilar (distant)\n",
        "members.\n",
        "4. Update the cluster linkage matrix.\n",
        "5. Repeat steps 2-4 until one single cluster remains.\n"
      ],
      "metadata": {
        "id": "9VzVk1zUPlAi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl8dDwxIWYFe",
        "outputId": "15429077-fb5e-4e37-ba34-7dc8d8e3e5eb"
      },
      "source": [
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "variables = ['X', 'Y', 'Z']\n",
        "labels = ['ID_0', 'ID_1', 'ID_2', 'ID_3', 'ID_4']\n",
        "\n",
        "X = np.random.random_sample([5, 3])*10\n",
        "df = pd.DataFrame(X, columns=variables, index=labels)\n",
        "df\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCEXotuhWYFe"
      },
      "source": [
        "## Performing hierarchical clustering on a distance matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cpv6aLn1WYFe",
        "outputId": "325750aa-b87c-4979-ae8b-0aa4a9752515"
      },
      "source": [
        "```python\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "\n",
        "row_dist = pd.DataFrame(squareform(pdist(df, metric='euclidean')),\n",
        "                        columns=labels,\n",
        "                        index=labels)\n",
        "row_dist\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nDtd_bjWYFe"
      },
      "source": [
        "- We can either pass a condensed distance matrix (upper triangular) from the `pdist` function, or we can pass the \"original\" data array and define the `metric='euclidean'` argument in `linkage`.\n",
        "- However, we should not pass the squareform distance matrix, which would yield different distance values although the overall clustering could be the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCNlojXkWYFf",
        "outputId": "0cdad593-e35f-4cb0-e720-cfc94b3a0bb0"
      },
      "source": [
        "```python\n",
        "# 1. incorrect approach: Squareform distance matrix\n",
        "\n",
        "from scipy.cluster.hierarchy import linkage\n",
        "\n",
        "\n",
        "row_clusters = linkage(row_dist, method='complete', metric='euclidean')\n",
        "pd.DataFrame(row_clusters,\n",
        "             columns=['row label 1', 'row label 2',\n",
        "                      'distance', 'no. of items in clust.'],\n",
        "             index=[f'cluster {(i + 1)}'\n",
        "                    for i in range(row_clusters.shape[0])])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "scrolled": true,
        "id": "MJj1PnC0WYFf",
        "outputId": "6afd077b-95f7-44ff-c686-8f2dfd4f1d0e"
      },
      "source": [
        "```python\n",
        "# 2. correct approach: Condensed distance matrix\n",
        "\n",
        "row_clusters = linkage(pdist(df, metric='euclidean'), method='complete')\n",
        "pd.DataFrame(row_clusters,\n",
        "             columns=['row label 1', 'row label 2',\n",
        "                      'distance', 'no. of items in clust.'],\n",
        "            index=[f'cluster {(i + 1)}'\n",
        "                    for i in range(row_clusters.shape[0])])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4TMadjTWYFf",
        "outputId": "12d50f84-5e5f-46b7-9787-7fc4f4b06a53"
      },
      "source": [
        "```python\n",
        "# 3. correct approach: Input matrix\n",
        "\n",
        "row_clusters = linkage(df.values, method='complete', metric='euclidean')\n",
        "pd.DataFrame(row_clusters,\n",
        "             columns=['row label 1', 'row label 2',\n",
        "                      'distance', 'no. of items in clust.'],\n",
        "             index=[f'cluster {(i + 1)}'\n",
        "                    for i in range(row_clusters.shape[0])])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OjhudwtWYFf",
        "outputId": "cfc6b1aa-7434-42de-f949-e194e02204f5"
      },
      "source": [
        "```python\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "\n",
        "\n",
        "# make dendrogram black (part 1/2)\n",
        "# from scipy.cluster.hierarchy import set_link_color_palette\n",
        "# set_link_color_palette(['black'])\n",
        "\n",
        "row_dendr = dendrogram(row_clusters,\n",
        "                       labels=labels,\n",
        "                       # make dendrogram black (part 2/2)\n",
        "                       # color_threshold=np.inf\n",
        "                       )\n",
        "plt.tight_layout()\n",
        "plt.ylabel('Euclidean distance')\n",
        "#plt.savefig('figures/10_11.png', dpi=300,\n",
        "#            bbox_inches='tight')\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KQbLxhZWYFf"
      },
      "source": [
        "## Applying agglomerative clustering via scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x36u40tWYFf",
        "outputId": "d79ff237-9db5-4b72-bdb7-397572f57613"
      },
      "source": [
        "```python\n",
        "from packaging import version\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "\n",
        "ac = AgglomerativeClustering(n_clusters=3,\n",
        "                             metric=\"euclidean\",\n",
        "                             linkage=\"complete\"\n",
        "                            )\n",
        "\n",
        "labels = ac.fit_predict(X)\n",
        "print(f'Cluster labels: {labels}')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REUqdePEWYFf",
        "outputId": "46283e36-9ee5-452e-873d-4622d3a6abd0"
      },
      "source": [
        "```python\n",
        "# Two clusters\n",
        "ac = AgglomerativeClustering(n_clusters=2,\n",
        "                             metric=\"euclidean\",\n",
        "                             linkage=\"complete\"\n",
        "                            )\n",
        "\n",
        "labels = ac.fit_predict(X)\n",
        "\n",
        "print(f'Cluster labels: {labels}')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9lamr7yWYFg"
      },
      "source": [
        "# Locating regions of high density via DBSCAN\n",
        "\n",
        "- Density-Based Spatial Clustering of Applications with\n",
        "Noise (DBSCAN) does not make assumptions about spherical clusters like k-means, nor does\n",
        "it partition the dataset into hierarchies that require a manual cut-off point.\n",
        "- As its name implies, density-\n",
        "based clustering assigns cluster labels based on dense regions of points.\n",
        "- In DBSCAN, the notion of density is defined as the number of points within a specified radius, $\\epsilon$ .\n",
        "- According to the DBSCAN algorithm, a special label is assigned to each example (data point) using\n",
        "the following criteria:\n",
        "    - A point is considered a core point if at least a specified number (MinPts) of neighboring points fall within the specified radius, $\\epsilon$.\n",
        "    - A border point is a point that has fewer neighbors than MinPts within $\\epsilon$, but lies within the $\\epsilon$ radius of a core point.\n",
        "    - All other points that are neither core nor border points are considered noise points.\n",
        "\n",
        "- After labeling the points as core, border, or noise, the DBSCAN algorithm can be summarized in two simple steps:\n",
        "    1. Form a separate cluster for each core point or connected group of core points. (Core points are connected if they are no farther away than $\\epsilon$.)\n",
        "    2. Assign each border point to the cluster of its corresponding core point.\n",
        "\n",
        "<img src=\"https://github.com/rasbt/machine-learning-book/blob/main/ch10/figures/10_13.png?raw=true\" width=\"600px\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrF5q-OwWYFg",
        "outputId": "1fa56f61-3e43-4852-8147-be41fd00734b"
      },
      "source": [
        "```python\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "\n",
        "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
        "plt.scatter(X[:, 0], X[:, 1])\n",
        "\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "#plt.savefig('figures/10_14.png', dpi=300)\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6FIvi7lWYFg"
      },
      "source": [
        "- K-means and hierarchical clustering:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlszA7utWYFg",
        "outputId": "1cea42b1-e554-4272-de28-3aa137612cb2"
      },
      "source": [
        "```python\n",
        "\n",
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
        "\n",
        "km = KMeans(n_clusters=2, random_state=0)\n",
        "y_km = km.fit_predict(X)\n",
        "ax1.scatter(X[y_km == 0, 0], X[y_km == 0, 1],\n",
        "            edgecolor='black',\n",
        "            c='lightblue', marker='o', s=40, label='cluster 1')\n",
        "ax1.scatter(X[y_km == 1, 0], X[y_km == 1, 1],\n",
        "            edgecolor='black',\n",
        "            c='red', marker='s', s=40, label='cluster 2')\n",
        "ax1.set_title('K-means clustering')\n",
        "\n",
        "ax1.set_xlabel('Feature 1')\n",
        "ax1.set_ylabel('Feature 2')\n",
        "\n",
        "ac = AgglomerativeClustering(n_clusters=2,\n",
        "                             metric='euclidean',\n",
        "                             linkage='complete')\n",
        "y_ac = ac.fit_predict(X)\n",
        "ax2.scatter(X[y_ac == 0, 0], X[y_ac == 0, 1], c='lightblue',\n",
        "            edgecolor='black',\n",
        "            marker='o', s=40, label='Cluster 1')\n",
        "ax2.scatter(X[y_ac == 1, 0], X[y_ac == 1, 1], c='red',\n",
        "            edgecolor='black',\n",
        "            marker='s', s=40, label='Cluster 2')\n",
        "ax2.set_title('Agglomerative clustering')\n",
        "\n",
        "ax2.set_xlabel('Feature 1')\n",
        "ax2.set_ylabel('Feature 2')\n",
        "\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "#plt.savefig('figures/10_15.png', dpi=300)\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hVidSWGWYFg"
      },
      "source": [
        "- Density-based clustering:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GagWyvL9WYFg",
        "outputId": "9d38e927-1abe-4560-b2ad-ea88d71e9678"
      },
      "source": [
        "```python\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "\n",
        "db = DBSCAN(eps=0.2, min_samples=5, metric='euclidean')\n",
        "y_db = db.fit_predict(X)\n",
        "plt.scatter(X[y_db == 0, 0], X[y_db == 0, 1],\n",
        "            c='lightblue', marker='o', s=40,\n",
        "            edgecolor='black',\n",
        "            label='Cluster 1')\n",
        "plt.scatter(X[y_db == 1, 0], X[y_db == 1, 1],\n",
        "            c='red', marker='s', s=40,\n",
        "            edgecolor='black',\n",
        "            label='Cluster 2')\n",
        "\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "#plt.savefig('figures/10_16.png', dpi=300)\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8qfQqVWWYFg"
      },
      "source": [
        "- The DBSCAN algorithm can successfully detect the half-moon shapes, which highlights one of the strengths of DBSCAN—clustering data of arbitrary shapes.\n",
        "- However, we should also note some of the disadvantages of DBSCAN. With an increasing number of\n",
        "features in our dataset, the negative effect of the\n",
        "curse of dimensionality increases.\n",
        "- This is especially a problem if we are using the Euclidean distance metric.\n",
        "- However, the problem of the curse of dimensionality is not unique to DBSCAN: it also affects\n",
        "other clustering algorithms that use the Euclidean distance metric, for example, k-means and hierarchical\n",
        "clustering algorithms.\n",
        "- In addition, we have two hyperparameters in DBSCAN (MinPts and $\\epsilon$) that need to be optimized to yield good clustering results.\n",
        "- Finding a good combination of MinPts and $\\epsilon$ can be problematic if the density differences in the dataset are relatively large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm-JDbOqWYFg"
      },
      "source": [
        "# Summary\n",
        "\n",
        "- In practice, it is not always obvious which clustering algorithm will perform best on a given\n",
        "dataset, especially if the data comes in multiple dimensions that make it hard or impossible to visualize.\n",
        "- Furthermore, it is important to emphasize that a successful clustering does not only depend on\n",
        "the algorithm and its hyperparameters; rather, the choice of an appropriate distance metric and the\n",
        "use of domain knowledge that can help to guide the experimental setup can be even more important.\n",
        "- In the context of the curse of dimensionality, it is thus common practice to apply dimensionality\n",
        "reduction techniques prior to performing clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "RUiJESZQTY8N"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}