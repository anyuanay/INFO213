{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anyuanay/INFO213/blob/main/INFO213_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Km0XQRaaAvP"
      },
      "source": [
        "## Drexel University\n",
        "## College of Computing and Informatics\n",
        "## INFO 213: Data Science Programming II\n",
        "## Assignment 2\n",
        "## Due Date: Sunday, April 27, 2024\n",
        "\n",
        "\n",
        "\n",
        "### A. Assignment Overview\n",
        "This assignment provides the opportunity for you to practice with the data science in Python.\n",
        "\n",
        "### B. What to Hand In\n",
        "\n",
        "Sumbit a completed this Jupyter notebook.\n",
        "\n",
        "### C. How to Hand In\n",
        "\n",
        "Submit your Jupyter notebook file through the course website in the Blackboard Learn system.\n",
        "\n",
        "### D. When to Hand In\n",
        "\n",
        "1. Submit your assignment no later than 11:59pm in the due date.\n",
        "2. There will be a 10% (absolute value) deduction for each day of lateness, to a maximum of 3 days; assignments will not be accepted beyond that point. Missing work will earn a zero grade.\n",
        "\n",
        "### E. Written Presentation Requirements (if applicable)\n",
        "Assignments will be judged on the basis of visual appearance, grammatical correctness, and quality of writing, as well as their contents. Please make sure that the text of your assignments is well-structured, using paragraphs, full sentences, and other features of well-written presentation.\n",
        "\n",
        "### F. Marking Schemes:\n",
        "\n",
        "Marking assignments will be based on several aspects: presentation, correctness and coding styles.\n",
        "\n",
        "1. Write a good comment for each variable, each method, each control branch, and each loop.\n",
        "2. Your method comments must mention the purpose of each parameter, and must be grammatically correct.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRxdEw_NF1Db"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JonBzpD19Dsp"
      },
      "source": [
        "# Question 1: Implement Logistic Regression\n",
        "In this question, you will build a logistic regression model to\n",
        "predict whether a student gets admitted into a university based on historical data. For each training example, you have the applicant's scores on two exams and the admissions decision (1 for positive and 0 for negative). Your task is to build a classication model that estimates an applicant's probability of admission based the scores from those two exams.\n",
        "\n",
        "The algorithm for training a logistic regression model can be summarized as follows:\n",
        "- Assume we have $n$ training examples. Each example has $m$ features.\n",
        "- Given input $\\mathbf{x}=\\{x_j^{(i)}\\}$, $i=1...n, j=1...m$, and labels $\\mathbf{y}=\\{y^{(i)}\\}$, where $y^{(i)}\\in[0, 1]$, $i=1...n$.\n",
        "- Initialize weights $\\mathbf{w}=\\{w_j\\}$, $j=0...m$\n",
        "- Compute $z^{(i)}=\\mathbf{w}^{T}\\mathbf{x}^{(i)}=w_0x_0^{(i)}+w_1x_1^{(i)}+...+w_mx_m^{(i)}$\n",
        "- Compute $\\bar{y}^{(i)}=\\phi(z^{(i)})=\\frac{1}{1+e^{-z^{(i)}}}$ (**Notice here $\\bar{y}$ is the real value of the sigmoid function, not the predicted classification label in \\{0,1\\}.**)\n",
        "- Calculate the loss: \\begin{equation}\n",
        "J(\\mathbf{w}) = -\\Sigma_i(y^{(i)} log(\\bar{y}^{(i)})+(1-y^{(i)}) log(1-\\bar{y}^{(i)}))\n",
        "\\end{equation}\n",
        "- Update the weights by taking a step in the opposite direction of the gradient, $\\nabla J(\\mathbf{w})$ of our cost function $J(\\mathbf{w})$:\n",
        " - $\\mathbf{w} := \\mathbf{w} + \\Delta \\mathbf{w}$, where $\\Delta \\mathbf{w}= - \\eta \\nabla J(\\mathbf{w})$.\n",
        " - Update the weight $w_j$ as:\n",
        "\\begin{equation}\n",
        "\\Delta w_j = - \\eta \\frac{\\partial J}{\\partial w_j} = \\eta \\Sigma_i(y^{(i)}-\\phi(z^{(i)}))x_j^{(i)}\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76bZ69OZG5F6"
      },
      "source": [
        "Step 1: The training data set is `exams.csv`. Load the training data as a Pandas DataFrame. Visualize the exam1_score and exam2_score in a 2-dimensional plot. Label the data with their admission decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmP9VvauurKm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up72LYxOeyt5"
      },
      "source": [
        "Step 2: Implement the `sigmoid` function: $sigmoid(z)=\\frac{1}{1+e^{-z}}$. Test the function using a few values. For large positive values of x, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0. Evaluating sigmoid(0) should give you exactly 0.5. Your code should also work with vectors and matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4-44nN3e0sM"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    compute the sigmoid activation of the given value\n",
        "    input: z: given value\n",
        "    output: sigmoid of the input value\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIrO57rNhVIy"
      },
      "source": [
        "Step 3: Implement the function `computeZ(w, X)`: $\\mathbf{z}=\\mathbf{X}\\mathbf{w}[1:] + {w}_0$, where $\\mathbf{X}$ is the input $n\\times m$ matrix with $n$ examples and $m$ features, $\\mathbf{w}$ is $(m+1)\\times 1$ feature vector, and $w_0$ is the bias. The function returns a vector of values. Test the function on the first two rows of the training data `exams.csv` with the weights $\\mathbf{w}=[0.5, 1, -1]$. Verify the results are correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM1wjfrpinrj"
      },
      "outputs": [],
      "source": [
        "def computeZ(w, X):\n",
        "    \"\"\"\n",
        "    compute the output z given the input and the weights\n",
        "    input: X: the nxm input values\n",
        "           w: the (m+1)x1 weights\n",
        "    output: nx1 output values\n",
        "    \"\"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNlRXO92mPzD"
      },
      "source": [
        "Step 4: Implement the function `computeLoss(w, X, y)` to compute the total loss, given the weights $\\mathbf{w}$, input $\\mathbf{X}$, and true label $\\mathbf{y}$:\n",
        "\\begin{equation}\n",
        "J(\\mathbf{w}) = -\\Sigma_i(y^{(i)} log(\\bar{y}^{(i)})+(1-y^{(i)}) log(1-\\bar{y}^{(i)}))\n",
        "\\end{equation}\n",
        "where $\\mathbf{X}$ is the input $n\\times m$ matrix with $n$ examples and $m$ features, $\\mathbf{w}$ is $(m+1)\\times 1$ feature vector, and $w_0$ is the bias, $\\mathbf{y}$ is a $n\\times 1$ vector of true labels. The function returns a single value. Test the function on the first two rows of the training data `exams.csv` with the weights $\\mathbf{w}=[0.5, 1, -1]$ and true labels $y=[1, 1]$. Verify the result is correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqixrdPkn09Z"
      },
      "outputs": [],
      "source": [
        "def computeLoss(w, X, y):\n",
        "    \"\"\"\n",
        "    compute the loss of predicting X given the weights and true labels\n",
        "    input: X: the nxm input values\n",
        "           w: the (m+1)x1 weights\n",
        "           y: the nx1 true labels\n",
        "    output: a single loss value\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2wyJU3hrWcA"
      },
      "source": [
        "Step 5: Implement the function `computeErrors(w, X, y)` to compute the errors of predicting the input, given the weights  ð° , input  ð— , and true label  ð² : $\\mathbf{errors} = \\mathbf{y} - \\bar{\\mathbf{y}}$,\n",
        "where  ð—  is the input  ð‘›Ã—ð‘š  matrix with  ð‘›  examples and  ð‘š  features,  ð°  is  (ð‘š+1)Ã—1  feature vector, and  $w_0$  is the bias,  ð²  is a  ð‘›Ã—1  vector of true labels, $\\bar{\\mathbf{y}}$ is a $n\\times 1$ vector of predicted probabilities. The function returns a $n\\times 1$ vector of errors. Test the function on the first two rows of the training data with the weights  ð°=[0.5,1,âˆ’1]  and true labels  ð‘¦=[1,1]. Verify the results are correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVnu5BD8mOnu"
      },
      "outputs": [],
      "source": [
        "def computeErrors(w, X, y):\n",
        "    \"\"\"\n",
        "    compute the errors of predicting X given the weights and true labels\n",
        "    input: X: the nxm input values\n",
        "           w: the (m+1)x1 weights\n",
        "           y: the nx1 true labels\n",
        "    output: a nx1 vector of errors\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    return errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qQ7YXGlu6VM"
      },
      "source": [
        "Step 6: Implement the training function `fit(eta, epochs, X, y, random_state=123, verbose=False)` to train a logistic regression model, given a learning rate `eta` and the number of `epochs`. The `fit()` function initializes a weight vector by small random values. The parameter ð—  is the input  ð‘›Ã—ð‘š  matrix with  ð‘›  examples and  ð‘š  features,  ð²  is a  ð‘›Ã—1  vector of true labels, `random_state` sets a random state value, and `verbose` is a boolean value indicating whether to print out intermediate weights and losses. The function returns a $(m+1)\\times 1$ vector of weights and a $epochs\\times 1$ vector of losses. Test the function on the first two rows of the training data with the true labels  ð‘¦=[1,1], eta=0.01, epochs=5. Verify the results are correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XDUlepBu60l"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def fit(eta, epochs, X, y, random_state=1234, verbose=False):\n",
        "    \"\"\"\n",
        "    fit a logistic regression model by updating the given weights\n",
        "    input: eta: learning rate\n",
        "           epochs: the number of learning iterations\n",
        "           X: the nxm input values\n",
        "           y: the nx1 true labels\n",
        "           random_state: set up a random state for reproducible results\n",
        "           verbose: a boolean value; if True, print out intermediate values\n",
        "    output: w: the (m+1)x1 weights after logistic regression learning\n",
        "            losses: the loss in each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    rgen = np.random.RandomState(random_state)\n",
        "    w = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
        "\n",
        "    loss = computeLoss(w, X, y)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"The initial weights are: {}\\nThe initial loss is: {}\".format(w, loss))\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    ### YOUR CODE BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return w, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUVCe9B_-qsT"
      },
      "source": [
        "Step 7: Implement the function `predict_proba(w, X)` to predit the probability of classifying the input as label 1. The parameter $\\mathbf{w}$ is a $(m+1)\\times 1$ vector of weights, $w_0$ is the bias, ð— is the input ð‘›Ã—ð‘š matrix with ð‘› examples and ð‘š features. The function returns a $n\\times 1$ vector of probabilities. Test the function on the first two rows of the training data and $w=[0.5, 1, -1]$. **What do the results tell you?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZMBynHV-zGl"
      },
      "outputs": [],
      "source": [
        "def predict_proba(w, X):\n",
        "    \"\"\"\n",
        "    compute the probabilities of predicting X as class 1\n",
        "    input: X: the nxm input values\n",
        "           w: the (m+1)x1 weights\n",
        "    output: a nx1 vector of probabilities\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPITIKRdAoto"
      },
      "source": [
        "Step 8: Implement the function `predict(w, X)` to predit the input as class 1 or 0. The parameter $\\mathbf{w}$ is a $(m+1)\\times 1$ vector of weights, $w_0$ is the bias, ð— is the input ð‘›Ã—ð‘š matrix with ð‘› examples and ð‘š features. The function returns a $n\\times 1$ vector of 1 or 0. Test the function on the first two rows of the training data and $w=[0.5, 1, -1]$. **What do the results tell you?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpmziKLR-y7m"
      },
      "outputs": [],
      "source": [
        "def predict(w, X):\n",
        "    \"\"\"\n",
        "    compute the classes of the input X\n",
        "    input: X: the nxm input values\n",
        "           w: the (m+1)x1 weights\n",
        "    output: a nx1 vector of 0 or 1\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return np.where(y_hat >= 0.5, 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-PFCNWIBMxz"
      },
      "source": [
        "Step 9: Train the logistic regression model on the training data using appropriate learning rate and number of epochs so that the algorithm coverges. My suggesetion is `eta=0.05` and `epochs=200`. Assign the final weights to `weights` and losses to `losses`. Prior to training the model, it's essential to standardize the input X. Consider exploring the potential outcomes when X isn't standardized. Using the given function `plot_decision_regions(X, y, weights, resolution)` to plot the decision regions of the logistic regression. **Explain the results.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpUTbj0wvXrZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6B53x3DvXoT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHasIvlDvXjs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XVOehr8BFhx"
      },
      "outputs": [],
      "source": [
        "weights, losses = fit(0.05, 200, X_std, y, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRHEWUYGxrrZ"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_decision_regions(X, y, weights, resolution=0.02):\n",
        "    \"\"\"\n",
        "    plot the decisoin regions of the logistic regression represented by weights.\n",
        "    Input: X: nxm input values\n",
        "           y: nx1 true labels\n",
        "           weights: learned logistic regression model\n",
        "           resolution: resolution on the grid\n",
        "    \"\"\"\n",
        "\n",
        "    # setup marker generator and color map\n",
        "    markers = ('s', 'x')\n",
        "    colors = ('lightgreen', 'cyan', 'purple', 'orange')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    # plot the decision surface\n",
        "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                           np.arange(x2_min, x2_max, resolution))\n",
        "\n",
        "    Z = predict(weights, np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "\n",
        "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
        "    #plt.scatter(xx1, xx2, c=Z, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=X[y == cl, 0],\n",
        "                    y=X[y == cl, 1],\n",
        "                    alpha=0.8,\n",
        "                    color=colors[idx+2],\n",
        "                    marker=markers[idx],\n",
        "                    label=cl,\n",
        "                    edgecolor='black')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wobrm_VL3uw4"
      },
      "outputs": [],
      "source": [
        "plot_decision_regions(X_std, y, weights)\n",
        "plt.xlabel(\"Exam1 Score [standardized]\")\n",
        "plt.ylabel(\"Exam2 Score [standardized]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQHV74uiS5xM"
      },
      "source": [
        "Step 10: Predict the admission probability for a student whose scores are: exam1_score = 45 and exam2_score = 85. (The probability is 0.776). Again, standardize the values before calling the `predict_proba()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ8Hbj-LviqW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GSRVumPTgzV"
      },
      "source": [
        "Step 11: Check the convergence of the training process by plotting the `losses`. You should see the losses decrease and flatten out after a certain number of epochs. **What does the plot tell you?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlnDMmBKvkH9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOdKnMZ4UCga"
      },
      "source": [
        "Step 12: Compute the classification accuracy of the final logistic regression model on the training data. (My accurary is 89%.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhZUFtk31Ofo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIAlJeWemOM1"
      },
      "source": [
        "# Question 2: Add Regularization to Logistic Regression\n",
        "In this question 2, you will build a regularized logistic regression model to predict whether microchips from a fabrication plant passes quality assurance (QA). Each training example has the results of two different tests. From these two tests, each example is labeled whether the microchip is accepted (1) or rejected (0). Your task is to build a regularized classication model that estimates the probability of accepting a microhip based the results of those two tests.\n",
        "\n",
        "Recall that the regularized logistic regression algorithm addes a regularization term to the loss function. As a result, the weight update rules have extra terms. The differences are summarized below:\n",
        "\n",
        "The algorithm for training a logistic regression model can be summarized as follows:\n",
        "- Assume we have $n$ training examples. Each example has $m$ features.\n",
        "- Given input $\\mathbf{x}=\\{x_j^{(i)}\\}$, $i=1...n, j=1...m$, and labels $\\mathbf{y}=\\{y^{(i)}\\}$, where $y^{(i)}\\in[0, 1]$, $i=1...n$.\n",
        "- Initialize weights $\\mathbf{w}=\\{w_j\\}$, $j=0...m$\n",
        "- Compute $z^{(i)}=\\mathbf{w}^{T}\\mathbf{x}^{(i)}=w_0x_0^{(i)}+w_1x_1^{(i)}+...+w_mx_m^{(i)}$\n",
        "- Compute $\\bar{y}^{(i)}=\\phi(z^{(i)})=\\frac{1}{1+e^{-z^{(i)}}}$\n",
        "- Calculate the loss (regularized): \\begin{equation}\n",
        "J(\\mathbf{w}) = -\\Sigma_i(y^{(i)} log(\\bar{y}^{(i)})+(1-y^{(i)}) log(1-\\bar{y}^{(i)}))+ \\frac{\\lambda}{2}\\Sigma_j w_j^2, j=1...m\n",
        "\\end{equation}\n",
        "- Update the weights by taking a step in the opposite direction of the gradient, $\\nabla J(\\mathbf{w})$ of our cost function $J(\\mathbf{w})$:\n",
        " - $\\mathbf{w} := \\mathbf{w} + \\Delta \\mathbf{w}$, where $\\Delta \\mathbf{w}= - \\eta \\nabla J(\\mathbf{w})$, specifically,\n",
        " \\begin{equation}\n",
        "\\nabla J_j=\\frac{\\partial J}{\\partial w_j} = - \\Sigma_i(y^{(i)}-\\phi(z^{(i)}))x_j^{(i)} + \\lambda w_j\n",
        "\\end{equation}\n",
        " - Update $w_0$ without regularization:\n",
        "\\begin{equation}\n",
        "\\Delta w_0 = - \\eta \\frac{\\partial J}{\\partial w_0} = \\eta \\Sigma_i(y^{(i)}-\\phi(z^{(i)}))x_0^{(i)}\n",
        "\\end{equation}\n",
        " - Update the rest of $w_j$ with regularization:\n",
        "\\begin{equation}\n",
        "\\Delta w_j = - \\eta \\frac{\\partial J}{\\partial w_j} = \\eta (\\Sigma_i(y^{(i)}-\\phi(z^{(i)}))x_j^{(i)} - \\lambda w_j)\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhssTO1wHubI"
      },
      "source": [
        "Step 1: The training data set is `tests.csv`. Load the training data as a Pandas DataFrame. Visualize the results of test1 and test2 in a 2-dimensional plot. Label the data with their acceptance decisions. You will see that the dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straightforward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKe_0DYQv3lZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMLaVPp2HubU"
      },
      "source": [
        "Step 2: Implement the `sigmoid` function: $sigmoid(z)=\\frac{1}{1+e^{-z}}$. The function should be the same as that in Question 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0aEhehOHubV"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    compute the sigmoid activation of the given value\n",
        "    input: z: given value\n",
        "    output: sigmoid of the input value\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeG1o4NsHubV"
      },
      "source": [
        "Step 3: Implement the function `computeZ(w, X)`: $\\mathbf{z}=\\mathbf{X}\\mathbf{w}[1:] + {w}_0$, where $\\mathbf{X}$ is the input $n\\times m$ matrix with $n$ examples and $m$ features, $\\mathbf{w}$ is $(m+1)\\times 1$ feature vector, and $w_0$ is the bias. This function sould be the same as in Question 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWyL1TQiHubV"
      },
      "outputs": [],
      "source": [
        "def computeZ(w, X):\n",
        "    \"\"\"\n",
        "    compute the output z given the input and the weights\n",
        "    input: X: the nxm input values\n",
        "           w: the (m+1)x1 weights\n",
        "    output: nx1 output values\n",
        "    \"\"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv-vKM3nHubV"
      },
      "source": [
        "Step 4: Implement the function `computeLoss_reg(w, X, y)` to compute the **regularized** total loss, given the weights $\\mathbf{w}$, input $\\mathbf{X}$, and true label $\\mathbf{y}$:\n",
        "\\begin{equation}\n",
        "J(\\mathbf{w}) = -\\Sigma_i(y^{(i)} log(\\bar{y}^{(i)})+(1-y^{(i)}) log(1-\\bar{y}^{(i)}))+ \\frac{\\lambda}{2}\\Sigma_j w_j^2, j=1...m\n",
        "\\end{equation}\n",
        "where $\\mathbf{X}$ is the input $n\\times m$ matrix with $n$ examples and $m$ features, $\\mathbf{w}$ is $(m+1)\\times 1$ feature vector, and $w_0$ is the bias, $\\mathbf{y}$ is a $n\\times 1$ vector of true labels. The function returns a single value. Test the function on the first two rows of the training data with the weights $\\mathbf{w}=[0.5, 1, -1]$ and true labels $y=[1, 1]$. Verify the result is correct for lmd=0.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE_2O35NHubV"
      },
      "outputs": [],
      "source": [
        "def computeLoss_reg(w, X, y, lmd):\n",
        "    \"\"\"\n",
        "    compute the loss of predicting X given the weights and true labels\n",
        "    input: X: the nxm input values\n",
        "           w: the (m+1)x1 weights\n",
        "           y: the nx1 true labels\n",
        "           lmd: regularization parameter\n",
        "    output: a single regularized loss value\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return loss + reg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-5X4gaJHubW"
      },
      "source": [
        "Step 5: Implement the function `computeErrors(w, X, y)` to compute the errors of predicting the input, given the weights  ð° , input  ð— , and true label  ð² : $\\mathbf{errors} = \\mathbf{y} - \\bar{\\mathbf{y}}$,\n",
        "where  ð—  is the input  ð‘›Ã—ð‘š  matrix with  ð‘›  examples and  ð‘š  features,  ð°  is  (ð‘š+1)Ã—1  feature vector, and  $w_0$  is the bias,  ð²  is a  ð‘›Ã—1  vector of true labels, $\\bar{\\mathbf{y}}$ is a $n\\times 1$ vector of predicted probabilities. This function should be the same as in Question 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3FqW1gMHubW"
      },
      "outputs": [],
      "source": [
        "def computeErrors(w, X, y):\n",
        "    \"\"\"\n",
        "    compute the errors of predicting X given the weights and true labels\n",
        "    input: X: the nxm input values\n",
        "           w: the (m+1)x1 weights\n",
        "           y: the nx1 true labels\n",
        "    output: a nx1 vector of errors\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KxU53dEHubW"
      },
      "source": [
        "Step 6: Implement the training function `fit_reg(eta, epochs, X, y, lmd, random_state=123, verbose=False)` to train a **regularized** logistic regression model, given a learning rate `eta`, the number of `epochs`, and the regularization parameter `lmd`. The `fit_reg()` function initializes a weight vector by small random values. The parameter ð—  is the input  ð‘›Ã—ð‘š  matrix with  ð‘›  examples and  ð‘š  features,  ð²  is a  ð‘›Ã—1  vector of true labels, `random_state` sets a random state value, and `verbose` is a boolean value indicating whether to print out intermediate weights and losses. The function returns a $(m+1)\\times 1$ vector of weights and a $epochs\\times 1$ vector of losses. Test the function on the first two rows of the training data with the true labels  ð‘¦=[1,1], eta=0.01, epochs=5, and lmd=0.1. Verify the results are correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ3CN5xiHubX"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def fit_reg(eta, epochs, X, y, lmd, random_state=1234, verbose=False):\n",
        "    \"\"\"\n",
        "    fit a logistic regression model by updating the given weights\n",
        "    input: eta: learning rate\n",
        "           epochs: the number of learning iterations\n",
        "           X: the nxm input values\n",
        "           y: the nx1 true labels\n",
        "           lmd: regularization parameter\n",
        "           random_state: set up a random state for reproducible results\n",
        "           verbose: a boolean value; if True, print out intermediate values\n",
        "    output: w: the (m+1)x1 weights after logistic regression learning\n",
        "            losses: the loss in each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    rgen = np.random.RandomState(random_state)\n",
        "    w = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
        "\n",
        "    loss = computeLoss_reg(w, X, y, lmd)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"The initial weights are: {}\\nThe initial loss is: {}\".format(w, loss))\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    ### YOUR CODE BELOW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return w, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRDZTYaZHubX"
      },
      "source": [
        "Step 7: Implement the function `predict_proba(w, X)` to predit the probability of classifying the input as label 1. The parameter $\\mathbf{w}$ is a $(m+1)\\times 1$ vector of weights, $w_0$ is the bias, ð— is the input ð‘›Ã—ð‘š matrix with ð‘› examples and ð‘š features. This function is the same as in Question 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "terSYvgCHubX"
      },
      "outputs": [],
      "source": [
        "def predict_proba(w, X):\n",
        "    \"\"\"\n",
        "    compute the probabilities of predicting X as class 1\n",
        "    input: X: the nxm input values\n",
        "           w: the (m+1)x1 weights\n",
        "    output: a nx1 vector of probabilities\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_YfVKcjHubX"
      },
      "source": [
        "Step 8: Implement the function `predict(w, X)` to predit the input as class 1 or 0. The parameter $\\mathbf{w}$ is a $(m+1)\\times 1$ vector of weights, $w_0$ is the bias, ð— is the input ð‘›Ã—ð‘š matrix with ð‘› examples and ð‘š features. This function is the same as in Question 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkPit4gRHubX"
      },
      "outputs": [],
      "source": [
        "def predict(w, X):\n",
        "    \"\"\"\n",
        "    compute the classes of the input X\n",
        "    input: X: the nxm input values\n",
        "           w: the (m+1)x1 weights\n",
        "    output: a nx1 vector of 0 or 1\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return np.where(y_hat >= 0.5, 1, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nza79rceYVly"
      },
      "source": [
        "Step 9: One way to create a non-linear decision boundary from linear model is to create polynomial features from each data point. We will map the features into all polynomial terms of $x_1$ and $x_2$ up to the sixth power. As a result of this mapping, the vector of two features (the results of the\n",
        "two tests) will be transformed into a 27-dimensional vector:\n",
        "\\begin{align} \\mathbf{x}=\\begin{bmatrix}\n",
        "x_1\\\\\n",
        "x_2 \\\\\n",
        "x_1^2 \\\\\n",
        "x_1x_2\\\\\n",
        "x_2^2 \\\\\n",
        "x_1^3 \\\\\n",
        "x_1x_2^2 \\\\\n",
        "x_1^2 x_2 \\\\\n",
        "x_2^3 \\\\\n",
        "... \\\\\n",
        "x_1^6 \\\\\n",
        "... \\\\\n",
        "x_2^6\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "The combinations of powers for $x_1$ and $x_2$ are listed below. You can verify that the total number of terms is 27: \\\\\n",
        "1 1 \\\\\n",
        "2 11 2 \\\\\n",
        "3 21 12 3 \\\\\n",
        "4 31 22 13 4 \\\\\n",
        "5 41 32 23 14 5 \\\\\n",
        "6 51 42 33 24 15 6 \\\\\n",
        "\n",
        "A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will appear non-linear when drawn in our 2-dimensional plot as shown below:\n",
        "<img src=\"https://i.imgur.com/hDa7VE1.png\" width=800>\n",
        "\n",
        "While the feature mapping allows us to build a more expressive classifier, it also more susceptible to overfitting. That is why we add a regularization term to the loss function.\n",
        "\n",
        "At this step, you are asked to implement the function `expand_polynomials(X)` to generate a new feature matrix of all the polynomial combinations of the input `m` features. Each polynomial has less or equal to degree 6. If $m=2$, the new matrix will have 27 new features. You can use the scikit-learn class `PolynomialFeatures` for the implementation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqxsnwUZYXNI"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "def expand_polynomials(X):\n",
        "    \"\"\"\n",
        "    Expand the input X with m features to a\n",
        "    feature matrix consisting of all polynomial combinations of the\n",
        "    m features with degree less than or equal to 6.\n",
        "    Input: X: a nxm input matrix\n",
        "    Output: a new feature matrix of all polynomial combinations of\n",
        "            the m features with degree less than or equal to 6\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return polynomials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jLSLy0KHubX"
      },
      "source": [
        "Step 10: Train the regularized logistic regression model on the training data using appropriate learning rate, number of epochs, and regularization parameter so that the algorithm coverges. My suggesetion is `eta=0.01`, `epochs=200`, and `lmd=1`. Assign the final weights to `weights_reg` and losses to `losses_reg`. Prior to training the model, it's essential to standardize the input X. You need first to generated polynomial features, and then standarized all the new features. Using the given function `plot_decision_regions_reg(X, y, weights)` to plot the decision regions of the regularized logistic regression. **Explain the results.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozbKNrmOwd20"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFr_aDH1wdy8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Pj3-6CRwdvn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O25jyoFXHubX"
      },
      "outputs": [],
      "source": [
        "weights_reg, losses_reg = fit_reg(0.01, 200, X1_poly_std, y1, lmd=1, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YebPUp6iHubY"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_decision_regions_reg(X, y, weights):\n",
        "    \"\"\"\n",
        "    plot the decisoin regions of the logistic regression represented by weights.\n",
        "    Input: X: nxm input values\n",
        "           y: nx1 true labels\n",
        "           weights: learned logistic regression model\n",
        "           resolution: resolution on the grid\n",
        "    \"\"\"\n",
        "\n",
        "    # setup marker generator and color map\n",
        "    markers = ('s', 'x')\n",
        "    colors = ('lightgreen', 'cyan', 'purple', 'orange')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    # plot the decision surface\n",
        "    x1_min, x1_max = X[:, 0].min(), X[:, 0].max()\n",
        "    x2_min, x2_max = X[:, 1].min(), X[:, 1].max()\n",
        "\n",
        "    # Creating a grid for plotting the polynomial line\n",
        "    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 200),\n",
        "                           np.linspace(x2_min, x2_max, 200))\n",
        "\n",
        "    xx_poly = expand_polynomials(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "\n",
        "    ss = StandardScaler()\n",
        "    ss.fit(xx_poly)\n",
        "    xx_poly_std = ss.transform(xx_poly)\n",
        "\n",
        "    Z = predict(weights, xx_poly_std)\n",
        "\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "\n",
        "    plt.contourf(xx_poly_std[:, 0].reshape(xx1.shape), xx_poly_std[:, 1].reshape(xx1.shape),\n",
        "                 Z, alpha=0.3, cmap=cmap)\n",
        "\n",
        "    plt.xlim(xx_poly_std[:, 0].min(), xx_poly_std[:, 0].max())\n",
        "    plt.ylim(xx_poly_std[:, 1].min(), xx_poly_std[:, 1].max())\n",
        "\n",
        "    sc = StandardScaler()\n",
        "    sc.fit(X)\n",
        "    X_std = sc.transform(X)\n",
        "\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=X_std[y == cl, 0],\n",
        "                    y=X_std[y == cl, 1],\n",
        "                    alpha=0.8,\n",
        "                    color=colors[idx+2],\n",
        "                    marker=markers[idx],\n",
        "                    label=cl,\n",
        "                    edgecolor='black')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KScBC1NHubY"
      },
      "source": [
        "Step 11: Predict the passing probability for a microchip whose test scores are: test1 = 0.045 and test2 = 0.085. (The probability is 0.96). Again, expand the input and standardize the values before calling the `predict_proba()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iui20aYnwxNd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEZ1oktBHubY"
      },
      "source": [
        "Step 12: Check the convergence of the training process by plotting the `losses`. You should see the losses decrease and flatten out after a certain number of epochs. **What does the plot tell you?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffu6XF-ywyYw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7R0hl2PHubY"
      },
      "source": [
        "Step 13: Compute the classification accuracy of the final logistic regression model on the training data. (My accurary is 82%.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tltGGj7Jwzp8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkBZkb7LrKsD"
      },
      "source": [
        "Step 14: Try out different regularization\n",
        "parameters for the dataset to understand how regularization prevents overfitting. Train models using following regularization parameters and draw the decision bounaries:\n",
        "- overfitting: `lmd=0`\n",
        "- underfitting: `lmd=100`\n",
        "\n",
        "**Explain the results.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3jhyybrwK03"
      },
      "outputs": [],
      "source": [
        "weights_reg, losses_reg = fit_reg(0.01, 200, X1_poly_std, y1, lmd=0, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fvwSvJqw1sU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YAAMir8sbNk"
      },
      "outputs": [],
      "source": [
        "weights_reg, losses_reg = fit_reg(0.01, 200, X1_poly_std, y1, lmd=100, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WchpnpVSsijP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}