{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anyuanay/INFO213/blob/main/INFO213_Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Km0XQRaaAvP"
      },
      "source": [
        "## Drexel University\n",
        "## College of Computing and Informatics\n",
        "## INFO 213: Data Science Programming II\n",
        "## Assignment 3\n",
        "## Due Date: Sunday, May 18, 2025\n",
        "## This assignment counts for 15% of the final grade\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOUR NAME:"
      ],
      "metadata": {
        "id": "Lygq4assosO5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRxdEw_NF1Db"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: Implement Principle Component Analysis (PCA) for Data Visualization and Classification\n",
        "\n",
        "- In this question, you will implement the Principle Component Analysis (PCA) algorithm for dimensionality reduction on a digits dataset. The dataset contains 1,797 images of size 8x8 pixels. Each image is one of 10 digits in [0-9]. You can load the data from Scikit Learn's datasets using `load_digits()`.\n",
        "- You will transform each image from 64 (8x8) dimensions to 2 dimensions and visualize the clusters of the 1797 images on a 2-D plane.\n",
        "- You will fit a logistic regression classifier and test the accuracy on the original and dimensionality reduced image data.\n",
        "\n",
        "**The steps for PCA are summarized as below**:\n",
        "\n",
        " 1. Standardize the $d$-dimensional dataset.\n",
        " 2. Construct the covariance matrix.\n",
        " 3. Decompose the covariance matrix into its eigenvectors and eigenvalues.\n",
        " 4. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.\n",
        " 5. Select $k$ eigenvectors, which correspond to the $k$ largest eigenvalues, where $k$ is the dimensionality of the new feature subspace ($k\\leq d$).\n",
        " 6. Construct a projection matrix, $\\mathbf{W}$, from the “top” $k$ eigenvectors.\n",
        " 7. Transform the $d$-dimensional input dataset, $\\mathbf{X}$, using the projection matrix, $\\mathbf{W}$, to obtain the new $k$-dimensional feature subspace.\n"
      ],
      "metadata": {
        "id": "trFuWxLw38Lk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Load the digits dataset from Scikit-Learn datasets as a DataFrame using `load_digits()`. The dataframe should have the shape of (1797, 65). List the columns of the DataFrame."
      ],
      "metadata": {
        "id": "M-e-17r6JZ6m"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lluQDTipw2Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Extract the columns 0-63 as feature matrix $X$, and the column 64 as target vector $y$. Split the data into separate training and test set. Show the shape of each set."
      ],
      "metadata": {
        "id": "d8uUArSNPITg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3TZFhYkTw3WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Standardize the features into the same scale."
      ],
      "metadata": {
        "id": "GNO_RZLTQ51_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oEc2H3Okw4ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Construct the covariance matrix by using numpy's `np.cov()` function on the standardized training data."
      ],
      "metadata": {
        "id": "TFYrxdS0UV66"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IiNEGq3Fw5hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Decompose the covariance matrix into its eigenvectors and eigenvalues by using numpy's `np.linalg.eig()` function. Print out the eigenvalues."
      ],
      "metadata": {
        "id": "7EIUKlREUvVc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EGX85UIhw7LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: The variance explained ratio of an eigenvalue $\\lambda_j$ is the fraction of the eigenvalue, $\\lambda_j$, and the total sum of the eigenvalues:\n",
        "$$\n",
        "explained\\_variance\\_ratio =\\frac{\\lambda_j}{\\sum_{j=1}^{d} \\lambda_j}\n",
        "$$\n",
        "\n",
        "Plot the variance explained ratios of all the eigenvalues."
      ],
      "metadata": {
        "id": "NPhn2K38VwjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8o2y1MR_w807"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Make a list of (eigenvalue, eigenvector) tuples and sort the (eigenvalue, eigenvector) tuples from high to low based on the values of eigenvalues."
      ],
      "metadata": {
        "id": "9WmlFK0kXe-R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Cq02J_5kw-Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Select the 2 eigenvectors, which correspond to the 2  largest eigenvalues. Construct a projection matrix,  $\\mathbf{W}$ , from the “top”  2 eigenvectors."
      ],
      "metadata": {
        "id": "Zd1eQVlnX9V_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-aKAioGPw_LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9: Transform the  64-dimensional input digit images using the projection matrix, $\\mathbf{W}$, to obtain the new  2-dimensional feature subspace of the images."
      ],
      "metadata": {
        "id": "v4bH6DOdYlb5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4gj_hSGtxAvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 10: Plot the image data points with only 2 features on a 2-D plane. Color the points with different colors based on its digit labels. **Do the 2 largest principles help you visualize the image clusters?**"
      ],
      "metadata": {
        "id": "LFvS_i-QY2M6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A8wegPWOxCPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 11: At this step, you will evaluate a default logistic regression classifier on 3 different data sets, the 64-dimensional original images data, the reduced 2-dimensional images data, and the $k$-dimensional images data, where $k$ is the number of principle components which account for 80% of the explained variance ratio.\n",
        "- Evaluate the accuracy of classifying the original 64-dimensional images data.\n",
        "- Evaluate the accuracy of classifying the reduced 2-dimensional images data.\n",
        "- Evaluate the accuracy of classifying the $k$-dimensional images data, where $k$ is the number of principle components which account for 80% of the explained variance ratio.\n",
        "\n",
        "**Discuss and explain the results and insights.**"
      ],
      "metadata": {
        "id": "hUSnR7ArdxWw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AAah92DmxEWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3fLgG7Rexd1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZsDjWSW3xdfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKGfeAlhPllr"
      },
      "source": [
        "# Question 2: Use scikit-learn Pipeline and Grid Search\n",
        "In this question, you will practice with the Pipeline and GridSearchCV classes in scikit-learn for model selection.\n",
        "\n",
        "You will compare the scikit-learn Logistic Regression and Random Forest models using various hyperparameters for classifying the wines in the scikit-learn wine dataset.  At the end, you will choose the best model for the classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_u-jlRgZrMB"
      },
      "source": [
        "Step 1: Load the wine data set from scikit-learn datasets using `load_wine()`. Load the data as a Pandas DataFrame `df`. Show the head of the DataFrame. List the feature names. How many classes are in the target?"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V3EQ-ZE8xIt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9GVeG_faQbp"
      },
      "source": [
        "Step 2: Import the necessary libraries from scikit-learn. Split the data into train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UIBgCQ3TxJzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRqgYt7catWR"
      },
      "source": [
        "Step 3: Make a pipeline with StandardScaler and LogisticRegression model. Also, make a pipeline with StandardScaler and RandomForestClassifier model."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N4Y5YdTuxLGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irlKxSrvbGeN"
      },
      "source": [
        "Step 4: Create a parameter grid for the logistic regression model. You should vary the `C` and `solver` parameters."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "__rMzJMsxMhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPD_Qe06bX_M"
      },
      "source": [
        "Step 5: Create a parameter grid for the random forest model. You shoudl vary the `n_estimators` and `max_depth` parameters."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t-Z8O76OxNlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Axqqq-S-bkB2"
      },
      "source": [
        "Step 6: Create a GridSearchCV object with the logistic regression pipeline, its parameter grid, and 5-fold cross validation. Fit the GridSearchCV object to find the best logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WrULW6aUxOzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5SpLRdvcHsq"
      },
      "source": [
        "Step 7: Create a GridSearchCV object with the random forest pipeline, its parameter grid, and 5-fold cross validation. Fit the GridSearchCV object to find the best random forest model."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UbZ6-QY1xP37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4Ov37WUcQ2C"
      },
      "source": [
        "Step 8: Print out the best logistic regression and random forest model."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ByEKHhMDxR50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RdBLcSQcZEj"
      },
      "source": [
        "Step 9: Evaluate the best logistic regression and random forest models on the test data. Print out the classification accuracy. Which model should be selected for the classification task?"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RH-zzn_nlY_M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}