{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anyuanay/INFO213/blob/main/INFO213_Week8_ensemble_lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "uEx04c6FAvEZ"
      },
      "source": [
        "# INFO 213: Data Science Programming 2\n",
        "___\n",
        "\n",
        "## Week 8: Combining Different Models for Ensemble Learning\n",
        "\n",
        "\n",
        "**Overview:**\n",
        "- [Learning with ensembles](#Learning-with-ensembles)\n",
        "- [Combining classifiers via majority vote](#Combining-classifiers-via-majority-vote)\n",
        "    - [Implementing a simple majority vote classifier](#Implementing-a-simple-majority-vote-classifier)\n",
        "    - [Using the majority voting principle to make predictions](#Using-the-majority-voting-principle-to-make-predictions)\n",
        "    - [Evaluating and tuning the ensemble classifier](#Evaluating-and-tuning-the-ensemble-classifier)\n",
        "- [Bagging â€“ building an ensemble of classifiers from bootstrap samples](#Bagging----Building-an-ensemble-of-classifiers-from-bootstrap-samples)\n",
        "    - [Bagging in a nutshell](#Bagging-in-a-nutshell)\n",
        "    - [Applying bagging to classify examples in the Wine dataset](#Applying-bagging-to-classify-examples-in-the-Wine-dataset)\n",
        "- [Leveraging weak learners via adaptive boosting](#Leveraging-weak-learners-via-adaptive-boosting)\n",
        "    - [How boosting works](#How-boosting-works)\n",
        "    - [Applying AdaBoost using scikit-learn](#Applying-AdaBoost-using-scikit-learn)\n",
        "- [Gradient boosting -- training an ensemble based on loss gradients](#Gradient-boosting----training-an-ensemble-based-on-loss-gradients)\n",
        "  - [Comparing AdaBoost with gradient boosting](#Comparing-AdaBoost-with-gradient-boosting)\n",
        "  - [GradientBoostingClassifier in Scikit Learn](#link)\n",
        "  - [Using XGBoost](#Using-XGBoost)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motivation:\n",
        "- We have focused on the best practices for tuning and evaluating different models.\n",
        "- We will build upon those techniques and explore different methods for constructing a set of classifiers that can often have a better predictive performance than any of its\n",
        "individual members.\n",
        "- We will do the following:\n",
        "    - Make predictions based on majority voting.\n",
        "    - Use bagging to reduce overfitting by drawing random combinations of the training dataset with repetition\n",
        "    - Apply boosting to build powerful models from weak learners that learn from their mistakes."
      ],
      "metadata": {
        "id": "5U9TylqkEdl9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkKG7p6bAvEe"
      },
      "source": [
        "# Learning with ensembles\n",
        "- Goal: To combine different classifiers into a meta-classifier that has better generalization performance than each individual classifier alone.\n",
        "- Majority vs. plurality voting:\n",
        "\n",
        "<img src=\"https://github.com/rasbt/machine-learning-book/blob/main/ch07/figures/07_01.png?raw=true\" width=\"600px\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Majority Voting:\n",
        "\n",
        "- Using the training dataset, we start by training m different classifiers (C1, ..., Cm).\n",
        "- different classification algorithms, for example, decision\n",
        "trees, support vector machines, logistic regression classifiers, and so on.\n",
        "- or same base classification algorithm, fitting different subsets of the training dataset, for example, random forest. algorithm combining different decision tree classifiers.\n",
        "\n",
        "<img src=\"https://github.com/rasbt/machine-learning-book/blob/main/ch07/figures/07_02.png?raw=true\" width=\"600px\" />"
      ],
      "metadata": {
        "id": "XmTbdsaOGGuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why ensemble with majority works?\n",
        "- Assume, all n-base classifiers for a binary classification task have an equal error rate, $\\epsilon$ and independent. - Simpley simply express the error probability of an ensemble of base classifiers as a probability mass function of a binomial distribution:\n",
        "\n",
        "$$\n",
        "P(y>k) = \\sum_{k}^{n}\\binom{n}{k}\\epsilon^k(1-\\epsilon)^{n-k}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "TEQOSdPaIxgU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let us take a look at a more concrete example of 11 base\n",
        "classifiers (n = 11), where each classifier has an error rate of 0.25 ($\\epsilon = 0.25$).\n",
        "- As we can see, the error rate of the ensemble (0.034) is much lower than the error rate of each individual\n",
        "classifier (0.25) if all the assumptions are met."
      ],
      "metadata": {
        "id": "aEUaMXfEKZus"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y9ZhfdVAvEf"
      },
      "source": [
        "```python\n",
        "from scipy.special import comb\n",
        "import math\n",
        "\n",
        "\n",
        "def ensemble_error(n_classifier, error):\n",
        "    k_start = int(math.ceil(n_classifier / 2.))\n",
        "    probs = [comb(n_classifier, k) * error**k * (1-error)**(n_classifier - k)\n",
        "             for k in range(k_start, n_classifier + 1)]\n",
        "    return sum(probs)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU9b6wToAvEf",
        "outputId": "39a03676-7422-46b0-e8d4-03c97991c53e"
      },
      "source": [
        "```python\n",
        "ensemble_error(n_classifier=11, error=0.25)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let us plot the ensemble errors vs. individual errors for n=11"
      ],
      "metadata": {
        "id": "elMpw4knLLy4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSt5OAa-AvEf"
      },
      "source": [
        "```python\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "error_range = np.arange(0.0, 1.01, 0.01)\n",
        "ens_errors = [ensemble_error(n_classifier=11, error=error)\n",
        "              for error in error_range]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejoXhNa6AvEf",
        "outputId": "0b47bd3c-1599-4349-f7d6-6a4a7da7f555"
      },
      "source": [
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(error_range,\n",
        "         ens_errors,\n",
        "         label='Ensemble error',\n",
        "         linewidth=2)\n",
        "\n",
        "plt.plot(error_range,\n",
        "         error_range,\n",
        "         linestyle='--',\n",
        "         label='Base error',\n",
        "         linewidth=2)\n",
        "\n",
        "plt.xlabel('Base error')\n",
        "plt.ylabel('Base/Ensemble error')\n",
        "plt.legend(loc='upper left')\n",
        "plt.grid(alpha=0.5)\n",
        "#plt.savefig('figures/07_03.png', dpi=300)\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**: The error probability of an ensemble is always better than the error\n",
        "of an individual base classifier, as long as the base classifiers perform better than random guessing\n",
        "($\\epsilon < 0.5$)."
      ],
      "metadata": {
        "id": "SuIBGSkgLf4t"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDaWAF2GAvEf"
      },
      "source": [
        "# Combining classifiers via majority vote\n",
        "\n",
        "- We will implement an algorithm to combine different classification algorithms associated with individual weights for confidence.\n",
        "- Our goal is to build a stronger meta-classifier that balances out the individual classifiers' weaknesses on a particular dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $C_1, C_2, C_m$ be $m$ classifiers with weights $\\mathbf{w}=\\{w_1, w_2, ... w_m\\}$ in an ensemble. Let $A$ be a set of class labels, for example, $[0, 1]$. Let $\\hat{y}$ be the prediction of the ensemble. Let $\\chi_A$ be an indicator function such as $\\chi_A(x=0)=1$ if $x=0$, otherwise, $\\chi_A(x=0)=0$.\n",
        "\n",
        "The weighted majority vote is:\n",
        "$$\n",
        "\\hat{y} = \\arg \\max_j \\sum_i^m w_i \\chi_A(C_i(\\mathbf{x})=j)\n",
        "$$\n",
        "\n",
        "For all equal weights,\n",
        "$$\n",
        "\\hat{y} = mode\\{C_1(\\mathbf{x}), C_2(\\mathbf{x}),..., C_m(\\mathbf{x})\\}\n",
        "$$\n",
        "\n",
        "### A manual example:\n",
        "\n",
        "Let $C_1(\\mathbf{x})=0$, $C_2(\\mathbf{x})=0$, and $C_3(\\mathbf{x})=1$ be $3$ classifiers with weights $\\mathbf{w}=\\{0.2, 0.2, 0.6\\}$ in an ensemble.\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\arg \\max_j \\sum_i^m w_i \\chi_A(C_i(\\mathbf{x})=j) =\n",
        "\\arg \\max_i [0.2\\times 1 + 0.2\\times 1, 0.6\\times 1] = 1 (the\\ index)\n",
        "$$"
      ],
      "metadata": {
        "id": "D-0Heedr0a_h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiLxUmLAAvEf"
      },
      "source": [
        "## Implementing a simple majority vote\n",
        "- We can use NumPy's convenient argmax and bincount functions, where bincount counts the number of occurrences of each\n",
        "class label. The argmax function then returns the index position of the maximum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4BessHnAvEf",
        "outputId": "3d7dc1f8-deeb-4ba2-9c25-d083cb7b6632"
      },
      "source": [
        "```python\n",
        "import numpy as np\n",
        "\n",
        "np.argmax(np.bincount([0, 0, 1],\n",
        "                      weights=[0.2, 0.2, 0.6]))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement and Evaluating a Majority Vote Ensemble Algorithm in Python\n",
        "\n",
        "1. Load the iris data from sklearn.datasets. Use only sepal width and petal length to make the classification task more challenging for illustration purposes. Only classify flower examples from the Iris-versicolor and Iris-virginica classes,\n",
        "2. Split the data into training and test sets\n",
        "3. Creating 3 classifiers: LogisticRegression, DecisionTree, and KNN\n",
        "4. Making pipelines for classifiers that require transformations.\n",
        "5. Fitting and evaluating the individual classifiers via 10-fold cross-validation.\n",
        "6. Collecting AUC scores of the ensembled classifiers through 10-fold cross-validation\n",
        "7. Evaluate the ensemble classifier by AUC"
      ],
      "metadata": {
        "id": "UBSYP1Y-5xb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading and pre-processing the data"
      ],
      "metadata": {
        "id": "AyLS5yYT98cm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data[50:, [1, 2]], iris.target[50:]\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test =\\\n",
        "       train_test_split(X, y,\n",
        "                        test_size=0.5,\n",
        "                        random_state=1,\n",
        "                        stratify=y)\n",
        "```"
      ],
      "metadata": {
        "id": "KGSSThBW69KQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
        "```"
      ],
      "metadata": {
        "id": "WdBSCRo091GH",
        "outputId": "0f1501ef-699f-42ca-a2f1-43a1f1d30e62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "y_train\n",
        "```"
      ],
      "metadata": {
        "id": "s0xqywUPA6JG",
        "outputId": "b23ed627-5eb2-4e68-e5c4-177fc4cb730f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating 3 classifiers: LogisticRegression, DecisionTree, and KNN"
      ],
      "metadata": {
        "id": "9twq_grj-Rw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "clf1 = LogisticRegression(penalty='l2',\n",
        "                          C=0.001,\n",
        "                          solver='lbfgs',\n",
        "                          random_state=1)\n",
        "\n",
        "clf2 = DecisionTreeClassifier(max_depth=1,\n",
        "                              criterion='entropy',\n",
        "                              random_state=0)\n",
        "\n",
        "clf3 = KNeighborsClassifier(n_neighbors=1,\n",
        "                            p=2,\n",
        "                            metric='minkowski')\n",
        "```"
      ],
      "metadata": {
        "id": "oL7YJNEG-cJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Making 2 pipelines for LogisticRegression and KNN\n",
        "- Both classifiers are sensitive to feature scales. We need to apply standardized scaler on the features. So, make pipelines."
      ],
      "metadata": {
        "id": "kMqREENK-puJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipe1 = Pipeline([['sc', StandardScaler()],\n",
        "                  ['clf', clf1]])\n",
        "pipe3 = Pipeline([['sc', StandardScaler()],\n",
        "                  ['clf', clf3]])\n",
        "```"
      ],
      "metadata": {
        "id": "hYCqA4nH_AX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Making a list of classifier labels"
      ],
      "metadata": {
        "id": "MJPRRz2i_Nig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "clf_labels = ['Logistic regression', 'Decision tree', 'KNN']\n",
        "```"
      ],
      "metadata": {
        "id": "2x6JekYc_Sq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fitting and evaluating the classifiers via 10-fold cross-validation\n",
        "- We will then evaluate the model performance of each classifier via 10-fold cross-validation on the training dataset before we combine them into an ensemble classifier."
      ],
      "metadata": {
        "id": "CIxioqQU_aat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "print('10-fold cross validation:\\n')\n",
        "for clf, label in zip([pipe1, clf2, pipe3], clf_labels):\n",
        "    scores = cross_val_score(estimator=clf,\n",
        "                             X=X_train,\n",
        "                             y=y_train,\n",
        "                             cv=10,\n",
        "                             scoring='roc_auc')\n",
        "    print(f'ROC AUC: {scores.mean():.2f} '\n",
        "          f'(+/- {scores.std():.2f}) [{label}]')\n",
        "```"
      ],
      "metadata": {
        "id": "wd26lHGL-Kwl",
        "outputId": "3be07a02-fcc8-4030-f64e-22735b8bb8b6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Collecting AUC scores of the ensembled classifiers through 10-fold cross-validation"
      ],
      "metadata": {
        "id": "WxicW1SmYgbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "```"
      ],
      "metadata": {
        "id": "EFFimrUxZ3_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "```"
      ],
      "metadata": {
        "id": "9Ts9eVGfbFQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from sklearn.metrics import roc_auc_score\n",
        "```"
      ],
      "metadata": {
        "id": "ckOO1KxgsOXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "auc_scores = []\n",
        "\n",
        "for train_idx, val_idx in kf.split(X_train, y_train):\n",
        "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
        "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "    # Collect predicted probabilities from each classifier\n",
        "    probas = []\n",
        "    for clf in [pipe1, clf2, pipe3]:\n",
        "        clf.fit(X_tr, y_tr)\n",
        "        probas.append(clf.predict_proba(X_val)[:, 1])\n",
        "\n",
        "\n",
        "    # Average probabilities across classifiers\n",
        "    avg_proba = np.mean(probas, axis=0)\n",
        "\n",
        "    # Compute AUC score for this fold\n",
        "    auc = roc_auc_score(y_val, avg_proba)\n",
        "    auc_scores.append(auc)\n",
        "```"
      ],
      "metadata": {
        "id": "eZ3oZMH_bMe2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "auc_scores = np.array(auc_scores)\n",
        "auc_scores\n",
        "```"
      ],
      "metadata": {
        "id": "NrdGBRhicjt7",
        "outputId": "8407f85d-4e19-4f00-d8b2-4501e33e00e4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluating the ensemble classifier by AUC"
      ],
      "metadata": {
        "id": "H-nn__0NfyRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "print(f'ROC AUC: {auc_scores.mean():.2f} '\n",
        "          f'(+/- {auc_scores.std():.2f}) [\"Ensemble\"]')\n",
        "```"
      ],
      "metadata": {
        "id": "Imf3oVYGcCfN",
        "outputId": "b362cfeb-5cc7-4353-f294-02339d7f6b8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement a MajorityVoteClassifier"
      ],
      "metadata": {
        "id": "6hFt2HZKgmXS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XAKyyFNAvEg"
      },
      "source": [
        "```python\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.base import ClassifierMixin\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.base import clone\n",
        "from sklearn.pipeline import _name_estimators\n",
        "import numpy as np\n",
        "import operator\n",
        "\n",
        "\n",
        "class MajorityVoteClassifier(ClassifierMixin, BaseEstimator):\n",
        "\n",
        "    \"\"\" A majority vote ensemble classifier\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    classifiers : array-like, shape = [n_classifiers]\n",
        "      Different classifiers for the ensemble\n",
        "\n",
        "    vote : str, {'classlabel', 'probability'} (default='classlabel')\n",
        "      If 'classlabel' the prediction is based on the argmax of\n",
        "        class labels. Else if 'probability', the argmax of\n",
        "        the sum of probabilities is used to predict the class label\n",
        "        (recommended for calibrated classifiers).\n",
        "\n",
        "    weights : array-like, shape = [n_classifiers], optional (default=None)\n",
        "      If a list of `int` or `float` values are provided, the classifiers\n",
        "      are weighted by importance; Uses uniform weights if `weights=None`.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, classifiers, vote='classlabel', weights=None):\n",
        "\n",
        "        self.classifiers = classifiers\n",
        "        self.named_classifiers = {key: value for key, value\n",
        "                                  in _name_estimators(classifiers)}\n",
        "        self.vote = vote\n",
        "        self.weights = weights\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Fit classifiers.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix}, shape = [n_examples, n_features]\n",
        "            Matrix of training examples.\n",
        "\n",
        "        y : array-like, shape = [n_examples]\n",
        "            Vector of target class labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "        \"\"\"\n",
        "        if self.vote not in ('probability', 'classlabel'):\n",
        "            raise ValueError(f\"vote must be 'probability' or 'classlabel'\"\n",
        "                             f\"; got (vote={self.vote})\")\n",
        "\n",
        "        if self.weights and len(self.weights) != len(self.classifiers):\n",
        "            raise ValueError(f'Number of classifiers and weights must be equal'\n",
        "                             f'; got {len(self.weights)} weights,'\n",
        "                             f' {len(self.classifiers)} classifiers')\n",
        "\n",
        "        # Use LabelEncoder to ensure class labels start with 0, which\n",
        "        # is important for np.argmax call in self.predict\n",
        "        self.lablenc_ = LabelEncoder()\n",
        "        self.lablenc_.fit(y)\n",
        "        self.classes_ = self.lablenc_.classes_\n",
        "        self.classifiers_ = []\n",
        "        for clf in self.classifiers:\n",
        "            fitted_clf = clone(clf).fit(X, self.lablenc_.transform(y))\n",
        "            self.classifiers_.append(fitted_clf)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\" Predict class labels for X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix}, shape = [n_examples, n_features]\n",
        "            Matrix of training examples.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        maj_vote : array-like, shape = [n_examples]\n",
        "            Predicted class labels.\n",
        "\n",
        "        \"\"\"\n",
        "        if self.vote == 'probability':\n",
        "            maj_vote = np.argmax(self.predict_proba(X), axis=1)\n",
        "        else:  # 'classlabel' vote\n",
        "\n",
        "            #  Collect results from clf.predict calls\n",
        "            predictions = np.asarray([clf.predict(X)\n",
        "                                      for clf in self.classifiers_]).T\n",
        "\n",
        "            maj_vote = np.apply_along_axis(\n",
        "                                      lambda x:\n",
        "                                      np.argmax(np.bincount(x,\n",
        "                                                weights=self.weights)),\n",
        "                                      axis=1,\n",
        "                                      arr=predictions)\n",
        "        maj_vote = self.lablenc_.inverse_transform(maj_vote)\n",
        "        return maj_vote\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\" Predict class probabilities for X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix}, shape = [n_examples, n_features]\n",
        "            Training vectors, where n_examples is the number of examples and\n",
        "            n_features is the number of features.\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        avg_proba : array-like, shape = [n_examples, n_classes]\n",
        "            Weighted average probability for each class per example.\n",
        "\n",
        "        \"\"\"\n",
        "        probas = np.asarray([clf.predict_proba(X)\n",
        "                             for clf in self.classifiers_])\n",
        "        avg_proba = np.average(probas, axis=0, weights=self.weights)\n",
        "        return avg_proba\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        \"\"\" Get classifier parameter names for GridSearch\"\"\"\n",
        "        if not deep:\n",
        "            return super().get_params(deep=False)\n",
        "        else:\n",
        "            out = self.named_classifiers.copy()\n",
        "            for name, step in self.named_classifiers.items():\n",
        "                for key, value in step.get_params(deep=True).items():\n",
        "                    out[f'{name}__{key}'] = value\n",
        "            return out\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subKa6gaAvEg"
      },
      "source": [
        "## Using the majority voting principle to make predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfbXM2b9AvEg"
      },
      "source": [
        "```python\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data[50:, [1, 2]], iris.target[50:]\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test =\\\n",
        "       train_test_split(X, y,\n",
        "                        test_size=0.5,\n",
        "                        random_state=1,\n",
        "                        stratify=y)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uooIy2qaAvEg",
        "outputId": "c845b42e-049e-46a0-c46d-fb3b39a414fd"
      },
      "source": [
        "```python\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "clf1 = LogisticRegression(penalty='l2',\n",
        "                          C=0.001,\n",
        "                          solver='lbfgs',\n",
        "                          random_state=1)\n",
        "\n",
        "clf2 = DecisionTreeClassifier(max_depth=1,\n",
        "                              criterion='entropy',\n",
        "                              random_state=0)\n",
        "\n",
        "clf3 = KNeighborsClassifier(n_neighbors=1,\n",
        "                            p=2,\n",
        "                            metric='minkowski')\n",
        "\n",
        "pipe1 = Pipeline([['sc', StandardScaler()],\n",
        "                  ['clf', clf1]])\n",
        "pipe3 = Pipeline([['sc', StandardScaler()],\n",
        "                  ['clf', clf3]])\n",
        "\n",
        "clf_labels = ['Logistic regression', 'Decision tree', 'KNN']\n",
        "\n",
        "print('10-fold cross validation:\\n')\n",
        "for clf, label in zip([pipe1, clf2, pipe3], clf_labels):\n",
        "    scores = cross_val_score(estimator=clf,\n",
        "                             X=X_train,\n",
        "                             y=y_train,\n",
        "                             cv=10,\n",
        "                             scoring='roc_auc')\n",
        "    print(f'ROC AUC: {scores.mean():.2f} '\n",
        "          f'(+/- {scores.std():.2f}) [{label}]')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYYFod_pAvEg",
        "outputId": "8d6946c6-5b3a-4680-915a-5b157f05d556"
      },
      "source": [
        "```python\n",
        "# Majority Rule (hard) Voting\n",
        "\n",
        "mv_clf = MajorityVoteClassifier(classifiers=[pipe1, clf2, pipe3])\n",
        "\n",
        "clf_labels += ['Majority voting']\n",
        "all_clf = [pipe1, clf2, pipe3, mv_clf]\n",
        "\n",
        "for clf, label in zip(all_clf, clf_labels):\n",
        "    scores = cross_val_score(estimator=clf,\n",
        "                             X=X_train,\n",
        "                             y=y_train,\n",
        "                             cv=10,\n",
        "                             scoring='roc_auc')\n",
        "    print(f'ROC AUC: {scores.mean():.2f} '\n",
        "          f'(+/- {scores.std():.2f}) [{label}]')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi9Tp2O9AvEg"
      },
      "source": [
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2rIvLBhAvEh"
      },
      "source": [
        "# Evaluating and tuning the ensemble classifier\n",
        "- We compute the ROC curves from the test dataset to check that\n",
        "MajorityVoteClassifier generalizes well with unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "all_clf\n",
        "```"
      ],
      "metadata": {
        "id": "D2RZE1yFtx3g",
        "outputId": "615b71cc-87ea-4f26-f768-551651f7e4d7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2rCHiWZAvEh",
        "outputId": "a922bc99-8f4d-46af-c4d2-3ba76da1098f"
      },
      "source": [
        "```python\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "colors = ['black', 'orange', 'blue', 'green']\n",
        "linestyles = [':', '--', '-.', '-']\n",
        "for clf, label, clr, ls \\\n",
        "        in zip(all_clf,\n",
        "               clf_labels, colors, linestyles):\n",
        "\n",
        "    # assuming the label of the positive class is 1\n",
        "    y_pred = clf.fit(X_train,\n",
        "                     y_train).predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, thresholds = roc_curve(y_true=y_test,\n",
        "                                     y_score=y_pred)\n",
        "    roc_auc = auc(x=fpr, y=tpr)\n",
        "    plt.plot(fpr, tpr,\n",
        "             color=clr,\n",
        "             linestyle=ls,\n",
        "             label=f'{label} (auc = {roc_auc:.2f})')\n",
        "\n",
        "plt.legend(loc='lower right')\n",
        "plt.plot([0, 1], [0, 1],\n",
        "         linestyle='--',\n",
        "         color='gray',\n",
        "         linewidth=2)\n",
        "\n",
        "plt.xlim([-0.1, 1.1])\n",
        "plt.ylim([-0.1, 1.1])\n",
        "plt.grid(alpha=0.5)\n",
        "plt.xlabel('False positive rate (FPR)')\n",
        "plt.ylabel('True positive rate (TPR)')\n",
        "\n",
        "\n",
        "#plt.savefig('figures/07_04', dpi=300)\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the decision boundaries of individual and ensemble classifiers"
      ],
      "metadata": {
        "id": "Gka0AbxgufbO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNlsYpRkAvEh"
      },
      "source": [
        "```python\n",
        "sc = StandardScaler()\n",
        "X_train_std = sc.fit_transform(X_train)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0jlQKXuAvEh",
        "outputId": "de5328ab-11a1-43e9-db91-3b7d69cca06e"
      },
      "source": [
        "```python\n",
        "from itertools import product\n",
        "\n",
        "\n",
        "all_clf = [pipe1, clf2, pipe3, mv_clf]\n",
        "\n",
        "x_min = X_train_std[:, 0].min() - 1\n",
        "x_max = X_train_std[:, 0].max() + 1\n",
        "y_min = X_train_std[:, 1].min() - 1\n",
        "y_max = X_train_std[:, 1].max() + 1\n",
        "\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                     np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "f, axarr = plt.subplots(nrows=2, ncols=2,\n",
        "                        sharex='col',\n",
        "                        sharey='row',\n",
        "                        figsize=(7, 5))\n",
        "\n",
        "for idx, clf, tt in zip(product([0, 1], [0, 1]),\n",
        "                        all_clf, clf_labels):\n",
        "    clf.fit(X_train_std, y_train)\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.3)\n",
        "\n",
        "    axarr[idx[0], idx[1]].scatter(X_train_std[y_train==0, 0],\n",
        "                                  X_train_std[y_train==0, 1],\n",
        "                                  c='blue',\n",
        "                                  marker='^',\n",
        "                                  s=50)\n",
        "\n",
        "    axarr[idx[0], idx[1]].scatter(X_train_std[y_train==1, 0],\n",
        "                                  X_train_std[y_train==1, 1],\n",
        "                                  c='green',\n",
        "                                  marker='o',\n",
        "                                  s=50)\n",
        "\n",
        "    axarr[idx[0], idx[1]].set_title(tt)\n",
        "\n",
        "plt.text(-3.5, -5.,\n",
        "         s='Sepal width [standardized]',\n",
        "         ha='center', va='center', fontsize=12)\n",
        "plt.text(-12.5, 4.5,\n",
        "         s='Petal length [standardized]',\n",
        "         ha='center', va='center',\n",
        "         fontsize=12, rotation=90)\n",
        "\n",
        "#plt.savefig('figures/07_05', dpi=300)\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Access the individual parameters for GridSearch"
      ],
      "metadata": {
        "id": "yLa1Qy-5u0Dt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCFkv70iAvEh",
        "outputId": "e17580c9-c5ca-4cfb-f925-d1601cd499ce"
      },
      "source": [
        "```python\n",
        "mv_clf.get_params()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning for MajorityVoteClassifier\n",
        "- Based on the values returned by the get_params method, we now know how to access the individual classifierâ€™s attributes.\n",
        "- Letâ€™s now tune the inverse regularization parameter, C, of the logistic regression classifier and the decision tree depth via a grid search for demonstration purposes:"
      ],
      "metadata": {
        "id": "0IdTEZPVvbWH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nDao9FdAvEh",
        "outputId": "141f48cb-ebb0-4654-fe49-a4a8f9d2e97f"
      },
      "source": [
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "params = {'decisiontreeclassifier__max_depth': [1, 2],\n",
        "          'pipeline-1__clf__C': [0.001, 0.1, 100.0]}\n",
        "\n",
        "grid = GridSearchCV(estimator=mv_clf,\n",
        "                    param_grid=params,\n",
        "                    cv=10,\n",
        "                    scoring='roc_auc')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "for r, _ in enumerate(grid.cv_results_['mean_test_score']):\n",
        "    mean_score = grid.cv_results_['mean_test_score'][r]\n",
        "    std_dev = grid.cv_results_['std_test_score'][r]\n",
        "    params = grid.cv_results_['params'][r]\n",
        "    print(f'{mean_score:.3f} +/- {std_dev:.2f} {params}')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoZawDAAAvEm",
        "outputId": "67305219-d9d3-4dab-8fa6-b61d3f1596ec"
      },
      "source": [
        "```python\n",
        "print(f'Best parameters: {grid.best_params_}')\n",
        "print(f'ROC AUC: {grid.best_score_:.2f}')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goh0r-KtAvEm"
      },
      "source": [
        "**Note**  \n",
        "By default, the default setting for `refit` in `GridSearchCV` is `True` (i.e., `GridSeachCV(..., refit=True)`), which means that we can use the fitted `GridSearchCV` estimator to make predictions via the `predict` method, for example:\n",
        "\n",
        "    grid = GridSearchCV(estimator=mv_clf,\n",
        "                        param_grid=params,\n",
        "                        cv=10,\n",
        "                        scoring='roc_auc')\n",
        "    grid.fit(X_train, y_train)\n",
        "    y_pred = grid.predict(X_test)\n",
        "\n",
        "In addition, the \"best\" estimator can directly be accessed via the `best_estimator_` attribute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhRj0GBuAvEm",
        "outputId": "cf8cc4ca-52c9-4204-a607-e26c4705e158"
      },
      "source": [
        "```python\n",
        "grid.best_estimator_.classifiers\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZPBI2TkAvEm"
      },
      "source": [
        "```python\n",
        "mv_clf = grid.best_estimator_\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km1XnxvoAvEm",
        "outputId": "1e06c3c6-e5f8-4c18-c9cd-667cb5581d3f"
      },
      "source": [
        "```python\n",
        "mv_clf.set_params(**grid.best_estimator_.get_params())\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChMeePhdAvEm",
        "outputId": "9a08ceee-4b95-441e-8f4b-78373df65392"
      },
      "source": [
        "```python\n",
        "mv_clf\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vnq63sDAvEm"
      },
      "source": [
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8dPj1zEAvEm"
      },
      "source": [
        "# Bagging -- Building an ensemble of classifiers from bootstrap samples\n",
        "\n",
        "- Bagging is an ensemble learning technique that is closely related to the MajorityVoteClassifier that\n",
        "we implemented in the previous section.\n",
        "- However, instead of using the same training dataset to fit the\n",
        "individual classifiers in the ensemble, we draw bootstrap samples (random samples with replacement)\n",
        "from the initial training dataset, which is why bagging is also known as bootstrap aggregating."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The concept of bagging is summarized below:\n",
        "\n",
        "<img src = \"https://github.com/rasbt/machine-learning-book/blob/main/ch07/figures/07_06.png?raw=true\" width=600 />"
      ],
      "metadata": {
        "id": "q-8BNJyiwUsP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NylJJFACAvEm"
      },
      "source": [
        "## Bagging in a nutshell\n",
        "\n",
        "- Each classifier receives a random subset of examples from the training dataset. We denote these random samples obtained via bagging as Bagging round 1, Bagging round 2,\n",
        "and so on.\n",
        "- Each subset contains a certain portion of duplicates and some of the original examples donâ€™t\n",
        "appear in a resampled dataset at all due to sampling with replacement.\n",
        "- Once the individual classifiers\n",
        "are fit to the bootstrap samples, the predictions are combined using majority voting.\n",
        "\n",
        "<img src=\"https://github.com/rasbt/machine-learning-book/blob/main/ch07/figures/07_07.png?raw=true\" width=800 />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHDcqtkSAvEn"
      },
      "source": [
        "## Applying bagging to classify examples in the Wine dataset\n",
        "- To see bagging in action, letâ€™s create a more complex classification problem using the Wine dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "wine = load_wine(as_frame=True)\n",
        "\n",
        "df_wine = wine.data\n",
        "df_wine\n",
        "```"
      ],
      "metadata": {
        "id": "BiBY4loAxRNv",
        "outputId": "f46c0edf-2d4c-40ae-f0c0-50c621cb0f57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "y = wine.target.values\n",
        "y\n",
        "```"
      ],
      "metadata": {
        "id": "cSuFhnkRyAFZ",
        "outputId": "982a7c28-1bf0-40d0-a406-b4c9f7cec34d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "df_wine.columns\n",
        "```"
      ],
      "metadata": {
        "id": "oSdGh_S-yIx3",
        "outputId": "0b2f9e99-82b7-41f9-d95f-0fa7f29ea064"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VUUPq_rAvEn",
        "outputId": "e9a0bfa6-1fa1-41ef-e3f3-ad1b15775844"
      },
      "source": [
        "```python\n",
        "X = df_wine[['alcohol', 'od280/od315_of_diluted_wines']].values\n",
        "X\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY1O2KnBAvEn"
      },
      "source": [
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test =\\\n",
        "            train_test_split(X, y,\n",
        "                             test_size=0.2,\n",
        "                             random_state=1,\n",
        "                             stratify=y)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
        "```"
      ],
      "metadata": {
        "id": "v0svMN2PyXDT",
        "outputId": "bd7462cf-8b6f-474d-930b-1249a41bd7a2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Scikit-Learn BaggingClassifier\n",
        "- A BaggingClassifier algorithm is already implemented in scikit-learn, which we can import from the\n",
        "ensemble submodule.\n",
        "- Here, we will use an unpruned decision tree as the base classifier and create an ensemble of 500 decision trees fit on different bootstrap samples of the training dataset."
      ],
      "metadata": {
        "id": "mCIjlXiFynaH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtmoPdeaAvEn"
      },
      "source": [
        "```python\n",
        "# A base classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier(criterion='entropy',\n",
        "                              max_depth=1,\n",
        "                              random_state=1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "# create a bagging\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "bag = BaggingClassifier(estimator=tree,\n",
        "                        n_estimators=500,\n",
        "                        max_samples=1.0,\n",
        "                        max_features=1.0,\n",
        "                        bootstrap=True,\n",
        "                        bootstrap_features=False,\n",
        "                        n_jobs=1,\n",
        "                        random_state=1)\n",
        "```"
      ],
      "metadata": {
        "id": "mjvXv3_xyy1D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olmdiS2WAvEn",
        "outputId": "2e530d67-c1d6-4b2c-d31e-7ee641858e96"
      },
      "source": [
        "```python\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "tree = tree.fit(X_train, y_train)\n",
        "y_train_pred = tree.predict(X_train)\n",
        "y_test_pred = tree.predict(X_test)\n",
        "\n",
        "tree_train = accuracy_score(y_train, y_train_pred)\n",
        "tree_test = accuracy_score(y_test, y_test_pred)\n",
        "print(f'Decision tree train/test accuracies '\n",
        "      f'{tree_train:.3f}/{tree_test:.3f}')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "bag = bag.fit(X_train, y_train)\n",
        "y_train_pred = bag.predict(X_train)\n",
        "y_test_pred = bag.predict(X_test)\n",
        "\n",
        "bag_train = accuracy_score(y_train, y_train_pred)\n",
        "bag_test = accuracy_score(y_test, y_test_pred)\n",
        "print(f'Bagging train/test accuracies '\n",
        "      f'{bag_train:.3f}/{bag_test:.3f}')\n",
        "```"
      ],
      "metadata": {
        "id": "B_6DvImAzJUN",
        "outputId": "f9f83866-dca8-46da-8ecb-4f05768bc0e8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYRQmfHdAvEn",
        "outputId": "467f8cc8-1984-4f99-848c-254db370253c"
      },
      "source": [
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "x_min = X_train[:, 0].min() - 1\n",
        "x_max = X_train[:, 0].max() + 1\n",
        "y_min = X_train[:, 1].min() - 1\n",
        "y_max = X_train[:, 1].max() + 1\n",
        "\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                     np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "f, axarr = plt.subplots(nrows=1, ncols=2,\n",
        "                        sharex='col',\n",
        "                        sharey='row',\n",
        "                        figsize=(8, 3))\n",
        "\n",
        "\n",
        "for idx, clf, tt in zip([0, 1],\n",
        "                        [tree, bag],\n",
        "                        ['Decision tree', 'Bagging']):\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    axarr[idx].contourf(xx, yy, Z, alpha=0.3)\n",
        "    axarr[idx].scatter(X_train[y_train == 0, 0],\n",
        "                       X_train[y_train == 0, 1],\n",
        "                       c='blue', marker='^')\n",
        "\n",
        "    axarr[idx].scatter(X_train[y_train == 1, 0],\n",
        "                       X_train[y_train == 1, 1],\n",
        "                       c='green', marker='o')\n",
        "\n",
        "    axarr[idx].set_title(tt)\n",
        "\n",
        "axarr[0].set_ylabel('OD280/OD315 of diluted wines', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.text(0, -0.2,\n",
        "         s='Alcohol',\n",
        "         ha='center',\n",
        "         va='center',\n",
        "         fontsize=12,\n",
        "         transform=axarr[1].transAxes)\n",
        "\n",
        "#plt.savefig('figures/07_08.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljshPeEGAvEn"
      },
      "source": [
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuQsz3ePAvEn"
      },
      "source": [
        "# Leveraging weak learners via adaptive boosting\n",
        "\n",
        "- In boosting, the ensemble consists of very simple base classifiers, also often referred to as weak\n",
        "learners, which often only have a slight performance advantage over random guessing - a typical example of a weak learner is a decision tree stump.\n",
        "- The key concept behind boosting is to focus on\n",
        "training examples that are hard to classify, that is, to let the weak learners subsequently learn from\n",
        "misclassified training examples to improve the performance of the ensemble."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksn_hplIAvEn"
      },
      "source": [
        "## How boosting works\n",
        "\n",
        "In contrast to bagging, the initial formulation of the boosting algorithm uses random subsets of training examples drawn from the training dataset without replacement; the original boosting procedure can be summarized in the following four key steps:\n",
        "1. Draw a random subset (sample) of training examples, d1, without replacement from the training dataset, D, to train a weak learner, C1.\n",
        "2. Draw a second random training subset, d2, without replacement from the training dataset and add 50 percent of the examples that were previously misclassified to train a weak learner, C2.\n",
        "3. Find the training examples, d3, in the training dataset, D, which C1 and C2 disagree upon, to train a third weak learner, C3.\n",
        "4. Combine the weak learners C1, C2, and C3 via majority voting."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concepts behind Adaboost\n",
        "- Subfigure 1 represents a training dataset for binary classification where all training examples are assigned equal weights.\n",
        "- In subfigure 2, we assign a larger weight to the two previously misclassified examples.\n",
        "- Similarly, in subfigure 3, previously misclassified examples get larger weights.\n",
        "- In subfigure 4, we combine\n",
        "the three weak learners trained on different re-weighted training subsets by a weighted majority vote.\n",
        "\n",
        "<img src=\"https://github.com/rasbt/machine-learning-book/blob/main/ch07/figures/07_09.png?raw=true\" width=600 />"
      ],
      "metadata": {
        "id": "ev-J3-cU0kc3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGt-pKWGAvEo"
      },
      "source": [
        "## Applying AdaBoost using scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a weaker learner which is a decision tree stump:"
      ],
      "metadata": {
        "id": "bEAsq0fc1skI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_T-XzEgAvEo"
      },
      "source": [
        "```python\n",
        "tree = DecisionTreeClassifier(criterion='entropy',\n",
        "                              max_depth=1,\n",
        "                              random_state=1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create an AdaBoostClassifier using sklearn:"
      ],
      "metadata": {
        "id": "ckCufutO12E-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada = AdaBoostClassifier(estimator=tree,\n",
        "                         n_estimators=500,\n",
        "                         learning_rate=0.1,\n",
        "                         random_state=1)\n",
        "```"
      ],
      "metadata": {
        "id": "PK5ou0r81zLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the weaker learner:"
      ],
      "metadata": {
        "id": "TE_sMmaR1-QT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM_1Q6kJAvEo",
        "outputId": "5ce37d8e-9952-46b6-f083-51c2c435d0a1"
      },
      "source": [
        "```python\n",
        "tree = tree.fit(X_train, y_train)\n",
        "y_train_pred = tree.predict(X_train)\n",
        "y_test_pred = tree.predict(X_test)\n",
        "\n",
        "tree_train = accuracy_score(y_train, y_train_pred)\n",
        "tree_test = accuracy_score(y_test, y_test_pred)\n",
        "print(f'Decision tree train/test accuracies '\n",
        "      f'{tree_train:.3f}/{tree_test:.3f}')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit and evaluate the AdaBoostClassifier:"
      ],
      "metadata": {
        "id": "DgiZW8lC2F1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "ada = ada.fit(X_train, y_train)\n",
        "y_train_pred = ada.predict(X_train)\n",
        "y_test_pred = ada.predict(X_test)\n",
        "\n",
        "ada_train = accuracy_score(y_train, y_train_pred)\n",
        "ada_test = accuracy_score(y_test, y_test_pred)\n",
        "print(f'AdaBoost train/test accuracies '\n",
        "      f'{ada_train:.3f}/{ada_test:.3f}')\n",
        "```"
      ],
      "metadata": {
        "id": "qGWZZn8k2LEJ",
        "outputId": "402c4228-03e2-43e2-9bf0-60f7dc99e67c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq53_HeUAvEo",
        "outputId": "becccdc7-ba4c-4df1-9f9a-e865d4e5a5fa"
      },
      "source": [
        "```python\n",
        "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
        "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                     np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "f, axarr = plt.subplots(1, 2, sharex='col', sharey='row', figsize=(8, 3))\n",
        "\n",
        "\n",
        "for idx, clf, tt in zip([0, 1],\n",
        "                        [tree, ada],\n",
        "                        ['Decision tree', 'AdaBoost']):\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    axarr[idx].contourf(xx, yy, Z, alpha=0.3)\n",
        "    axarr[idx].scatter(X_train[y_train == 0, 0],\n",
        "                       X_train[y_train == 0, 1],\n",
        "                       c='blue', marker='^')\n",
        "    axarr[idx].scatter(X_train[y_train == 1, 0],\n",
        "                       X_train[y_train == 1, 1],\n",
        "                       c='green', marker='o')\n",
        "    axarr[idx].set_title(tt)\n",
        "\n",
        "axarr[0].set_ylabel('OD280/OD315 of diluted wines', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.text(0, -0.2,\n",
        "         s='Alcohol',\n",
        "         ha='center',\n",
        "         va='center',\n",
        "         fontsize=12,\n",
        "         transform=axarr[1].transAxes)\n",
        "\n",
        "# plt.savefig('figures/07_11.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BumjKvh5AvEo"
      },
      "source": [
        "# Gradient boosting -- training an ensemble based on loss gradients\n",
        "\n",
        "- Gradient boosting is another variant of the boosting concept introduced in the previous section, that is, successively training weak learners to create a strong ensemble.\n",
        "- Gradient boosting is an extremely important topic because it forms the basis of popular machine learning algorithms such as XGBoost, which is well-known for winning Kaggle competitions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaXX9iBlAvEo"
      },
      "source": [
        "## Comparing AdaBoost with gradient boosting\n",
        "\n",
        "- AdaBoost trains decision tree stumps based on errors of the previous decision tree stump.\n",
        "- Gradient boosting fits decision trees in an iterative fashion using prediction errors. However, gradient boosting trees are usually deeper than decision tree stumps.\n",
        "- Also, in contrast to AdaBoost, gradient boosting\n",
        "does not use the prediction errors for assigning sample weights; they are used directly to form the\n",
        "target variable for fitting the next tree.\n",
        "- Moreover, instead of having an individual weighting term for\n",
        "each tree, like in AdaBoost, gradient boosting uses a global learning rate that is the same for each tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBCFfMM8AvEp"
      },
      "source": [
        "## GradientBoost in Scikit Learn\n",
        "\n",
        "- In scikit-learn, gradient boosting is implemented as sklearn.ensemble.GradientBoostingClassifier.\n",
        "- It is important to note that gradient boosting\n",
        "is a sequential process that can be slow to train.\n",
        "- However, in recent years a more popular implementation\n",
        "of gradient boosting has emerged, namely, XGBoost."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "```"
      ],
      "metadata": {
        "id": "yeoRi3XSFNAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "clf = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.01,\n",
        "    max_depth=4, random_state=1).fit(X_train, y_train)\n",
        "```"
      ],
      "metadata": {
        "id": "UeF_ETymFRkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "clf.score(X_test, y_test)\n",
        "```"
      ],
      "metadata": {
        "id": "U9l54TwkFVpa",
        "outputId": "c7947f84-c96f-41eb-9556-c5cd8718f812"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use XGBoost"
      ],
      "metadata": {
        "id": "MpkESdYZFZAa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ7AcOoLAvEp"
      },
      "source": [
        "```python\n",
        "import xgboost as xgb\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_UgkTuaAvEp",
        "outputId": "82838eff-97f9-4da6-b638-7c83e8cac6da"
      },
      "source": [
        "```python\n",
        "xgb.__version__\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "model = xgb.XGBClassifier(n_estimators=1000, learning_rate=0.01, max_depth=4, random_state=1, use_label_encoder=False)\n",
        "\n",
        "\n",
        "gbm = model.fit(X_train, y_train)\n",
        "```"
      ],
      "metadata": {
        "id": "-vQ5x-3xFqss",
        "outputId": "d264bcff-f7d5-406f-9a76-f588f943f175"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "gbm.score(X_test, y_test)\n",
        "```"
      ],
      "metadata": {
        "id": "0bIR5QvTFujT",
        "outputId": "1b01a5bd-d895-4222-bfb9-a539ed5c506e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMBseVRQAvEp"
      },
      "source": [
        "<br>\n",
        "<br>"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}