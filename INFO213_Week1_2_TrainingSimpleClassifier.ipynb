{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anyuanay/INFO213/blob/main/INFO213_Week1_2_TrainingSimpleClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dYQKc2PHACn"
      },
      "source": [
        "# INFO 213: Data Science Programming 2\n",
        "___\n",
        "\n",
        "### Week 1-2: Training Simple Artificial Neurons for Classification\n",
        "___\n",
        "\n",
        "\n",
        "**Aganda:**\n",
        "- Implement a perceptron step by step in Python\n",
        "- Develop the concept of machine learning algorithms for classification\n",
        "- Develop the basics of optimization using adaptive linear neurons\n",
        "- Investigate the impact of learning rate to gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries\n",
        "```\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "```"
      ],
      "metadata": {
        "id": "SrXTTB788Fm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training data contains two parts: feature matrix $\\mathbf{X}$ and target vector $y$.\n"
      ],
      "metadata": {
        "id": "mZmau9NK8KNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Neurons\n",
        "- Multiple signals arrive at the dendrites\n",
        "- Signals are integrated into the cell body\n",
        "- If the accumulated signal exceedsa certain threshold, an output signal is generated that will be passed onby the axon.\n",
        "<img src=\"https://i.imgur.com/JR3ZHFS.png\" width=800>"
      ],
      "metadata": {
        "id": "zwi8W2V9StdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The formal definition of an artificial neuron\n",
        "\n",
        "## Classify by a linear combination and a threshold\n",
        "- In context of binary classification problem.\n",
        "- Two classes: 1 and -1\n",
        "- Input\n",
        "\\begin{align} \\mathbf{x}=\\begin{bmatrix}\n",
        "x_1\\\\\n",
        "x_2 \\\\\n",
        "...\\\\\n",
        "x_m\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "- Weights\n",
        "\\begin{align} \\mathbf{w}=\\begin{bmatrix}\n",
        "w_1\\\\\n",
        "w_2 \\\\\n",
        "...\\\\\n",
        "w_m\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "- The output is a linear combination of the input by the weights: $z=w_1x_1 + w_2x_2 + ... + w_m x_m = \\mathbf{w}^T \\mathbf{x}$ (**if necessary, review linear algebra on product.**)\n",
        "- Given a threshold $\\theta$, define a decision function $\\phi(z)$ as:\n",
        "\\begin{equation}\n",
        "    \\phi(z)=\n",
        "\\begin{cases}\n",
        "    1,& \\text{if } z\\geq \\theta\\\\\n",
        "    -1,              & \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "-WflccseTmid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear combination with a bias\n",
        "- For simplicity, we can bring the threshold, $\\theta$, to the left side of the equation and define a weight-zero as $w_0 = -\\theta$ and $x_0=1$, then we have the equation:\n",
        "$z=w_0 x_0 + w_1x_1 + w_2x_2 + ... + w_m x_m = \\mathbf{w}^T \\mathbf{x}$\n",
        "- The decision function $\\phi(z)$ is:\n",
        "\\begin{equation}\n",
        "    \\phi(z)=\n",
        "\\begin{cases}\n",
        "    1,& \\text{if } z\\geq 0\\\\\n",
        "    -1,              & \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "- In machine learning literature, $w_0$ is called **bias**."
      ],
      "metadata": {
        "id": "ZeNcJThKWXnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization\n",
        "The following figure illustrates how the net input\n",
        "is squashed into a binary output (â€“1 or 1) by the decision\n",
        "function of the perceptron (left subfigure) and how it can be used to discriminate between two linearly separable classes\n",
        "(right subfigure):\n",
        "<img src=\"https://i.imgur.com/z7EJx4v.png\" width=800>"
      ],
      "metadata": {
        "id": "I4VEkOBleRnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The perceptron learning rule\n",
        "The perceptron algorithm can be summarized by the following steps:\n",
        "1. Initialize the weights to 0 or small random numbers.\n",
        "2. For each training example $\\mathbf{x}^{(i)}$,\n",
        " 1. Compute the output value $\\bar{y}^{(i)}$,\n",
        " 2. Update the weights $\\mathbf{w} = \\{w_j, j=0..m\\}$ as follows: $w_j := w_j + \\Delta w_j$\n",
        " 3. Where, $\\Delta w_j=\\eta(y^{(i)}-\\bar{y}^{(i)})x_j^{(i)}$ and $\\eta$ is a learning rate."
      ],
      "metadata": {
        "id": "7WE00xwNfLcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization\n",
        "The perceptron learning rule can be summarized in a simple diagram:\n",
        "<img src=\"https://i.imgur.com/vZOdr5s.png\" width=800>"
      ],
      "metadata": {
        "id": "8vs2MoyqkTXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual exercise on single variable input\n",
        "- Input: $\\mathbf{x}^1 = 0.5$ and $\\mathbf{x}^2 = 2$\n",
        "- True labels: $y^1 = -1$ and $y^2 = 1$.\n",
        "- Initial weights: $\\mathbf{w}= [w_0, w_1] = [1, -1]$\n",
        "- $ z = 1 + (-1 \\times 0.5) = 0.5 > 0$, so $\\bar{y}^1 = \\phi(z) = 1$\n",
        "- Set the learning rate $\\eta=1$\n",
        "- update the weight $w_0$ : $\\Delta w_0 = \\eta(y^1 - \\bar{y}^1)= 1(-1-1) = -2$, $w_0 := w_0 + \\Delta w_0 = 1 + (-2) = -1$\n",
        "- update the weight $w_1$: $\\Delta w_1 = \\eta(y^1 - \\bar{y}^1) x_1 =1(-1-1)0.5=-1$, $w_1 := w_1 + \\Delta w_1 = -1 + (-1) = -2$\n",
        "- The new weights are: $\\mathbf{w}= [w_0, w_1] = [-1, -2]$\n",
        "- Continue to update on $\\mathbf{x}^2$\n"
      ],
      "metadata": {
        "id": "6FXYhznzkri1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Practice"
      ],
      "metadata": {
        "id": "Xse9u-gJbiJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot the Data and Models"
      ],
      "metadata": {
        "id": "AHsJ1_i3pJqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot line y = w0 + w1*x\n",
        "def plot_line(w0, w1, color='r', legend='y = w0 + w1*x'):\n",
        "  \"\"\"\n",
        "  Plots a line defined by y = w0 + w1*x.\n",
        "\n",
        "  Args:\n",
        "    w0: The y-intercept of the line.\n",
        "    w1: The slope of the line.\n",
        "    color: The color of the line (default: 'r' for red).\n",
        "    legend: The legend string for the line (default: 'y = w0 + w1*x').\n",
        "  \"\"\"\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "3rDqV2gkwHGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot the point (x, y)\n",
        "def plot_point(x, y):\n",
        "\n"
      ],
      "metadata": {
        "id": "Y9R20YCpwMI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the x and y axis in black color\n",
        "def plot_axis():\n",
        "\n"
      ],
      "metadata": {
        "id": "yq2e43mxwR5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_point(0.5, -1)\n",
        "plot_point(2, 1)\n",
        "\n",
        "# 1-x\n",
        "plot_line(1, -1, 'blue', 'Initial Model')\n",
        "# -1-2x\n",
        "plot_line(-1, -2, 'red', 'Epoch 1: Updated Model by the First Instance')\n",
        "# 1+2x\n",
        "plot_line(1, 2, 'green', 'Epoch 1: Updated Model by the Second Instance')\n",
        "# -1+x\n",
        "plot_line(-1, 1, 'purple', 'Epoch 2: Updated Model by the First Instance')\n",
        "\n",
        "plot_axis()\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel(\"Input variable x\")\n",
        "plt.ylabel(\"Labels of the inputs\")\n",
        "\n",
        "plt.title(\"Classificatoin Model Updates\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fS3Z6LN4_IFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the Perceptron Learning Algorithm\n",
        "We take an object-oriented approach to defining the perceptron in-terface as a Python class, which will allow us to initialize new Perceptron objects that can learn from data via a `fit` method, andmake predictions via a separate `predict` method."
      ],
      "metadata": {
        "id": "jqGH-xJImwAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron(object):\n",
        "    \"\"\"Perceptron classifier.\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    eta : float\n",
        "      Learning rate (between 0.0 and 1.0)\n",
        "    epochs : int\n",
        "      Passes over the training dataset.\n",
        "    random_state : int\n",
        "      Random number generator seed for random weight\n",
        "      initialization.\n",
        "\n",
        "    Attributes\n",
        "    -----------\n",
        "    w_ : 1d-array\n",
        "      Weights after fitting.\n",
        "    b_: 1d-array\n",
        "      bias after fitting\n",
        "    errors_ : list\n",
        "      Number of misclassifications (updates) in each epoch.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, eta=0.01, epochs=50, random_state=1):\n",
        "\n",
        "        # initialize eta, epochs, and random state\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return class label after unit step\"\"\"\n",
        "\n",
        "        # compute the z values\n",
        "\n",
        "\n",
        "\n",
        "        # return class labels based on the z value\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like}, shape = [n_examples, n_features]\n",
        "          Training vectors, where n_examples is the number of examples and\n",
        "          n_features is the number of features.\n",
        "        y : array-like, shape = [n_examples]\n",
        "          Target values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "        \"\"\"\n",
        "        # initialize w_ with random values and b_ with all zeros.\n",
        "\n",
        "\n",
        "        # initialize empty errors_\n",
        "\n",
        "\n",
        "        # update weights and bias by epochs; record errors\n",
        "\n",
        "\n",
        "        return self\n"
      ],
      "metadata": {
        "id": "MAzv4lR_TPqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a perceptron model on the Iris dataset\n",
        "- We will restrict to two features, sepal length and petal length.\n",
        "- We only consider two flower classes, Setosa and Versicolor, from the Iris dataset for practical reasons. That is, we only consider the first 100 examples."
      ],
      "metadata": {
        "id": "oz3OzRtWuxCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris data into a dataframe\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "-SlX7msrt0HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load iris data to a DataFrame\n"
      ],
      "metadata": {
        "id": "5AKJUYltvYDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the iris data"
      ],
      "metadata": {
        "id": "37HSvGYywAjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select setosa and versicolor\n",
        "y = df.iloc[0:100, 4].values\n",
        "y = np.where(y == 'setosa', -1, 1)\n",
        "\n",
        "# extract sepal length and petal length\n",
        "X = df.iloc[0:100, [0, 2]].values\n",
        "\n",
        "# plot data\n",
        "plt.scatter(X[:50, 0], X[:50, 1],\n",
        "            color='red', marker='o', label='setosa')\n",
        "plt.scatter(X[50:100, 0], X[50:100, 1],\n",
        "            color='blue', marker='x', label='versicolor')\n",
        "\n",
        "plt.xlabel('sepal length [cm]')\n",
        "plt.ylabel('petal length [cm]')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# plt.savefig('images/02_06.png', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H-QO2PPOvthc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the perceptron"
      ],
      "metadata": {
        "id": "uN_0s-vnwep1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract sepal length and petal length as the training data\n",
        "X =\n",
        "X.shape"
      ],
      "metadata": {
        "id": "TwphraXbcePG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the target label\n",
        "y =\n",
        "y.shape"
      ],
      "metadata": {
        "id": "PfV0_NNzcmfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppn = Perceptron(eta=0.1, epochs=10)\n",
        "\n",
        "ppn.fit(X, y)"
      ],
      "metadata": {
        "id": "G-gtDO-wwg89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the errors\n",
        "plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Number of Misclassifications')\n",
        "\n",
        "# plt.savefig('images/02_07.png', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7QOaxqOrwkuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the decision bounary and regions"
      ],
      "metadata": {
        "id": "yrIAtVeOw1ZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
        "\n",
        "    # setup marker generator and color map\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    # plot the decision surface\n",
        "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                           np.arange(x2_min, x2_max, resolution))\n",
        "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    # plot class examples\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=X[y == cl, 0],\n",
        "                    y=X[y == cl, 1],\n",
        "                    alpha=0.8,\n",
        "                    c=colors[idx],\n",
        "                    marker=markers[idx],\n",
        "                    label=cl,\n",
        "                    edgecolor='black')"
      ],
      "metadata": {
        "id": "PHI6m31LwpXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_decision_regions(X, y, classifier=ppn)\n",
        "plt.xlabel('sepal length [cm]')\n",
        "plt.ylabel('petal length [cm]')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "\n",
        "# plt.savefig('images/02_08.png', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nm17QGLcxLic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Practice"
      ],
      "metadata": {
        "id": "4LcIYNp-bsfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adaptive linear neurons and the convergence of learning\n",
        "- To better understand logistic regression, support vector machine, and regression, we will take a look at another type of single-layer neural network (NN): ADAptive LInear NEuron ( Adaline).\n",
        "- The key difference the Adaline and perceptron rule\n",
        "is that the weights are updated based on a linear activation function rather than a unit step function.\n",
        "- In Adaline, $\\phi(z)=\\phi(\\mathbf{w}^T \\mathbf(x)) = \\mathbf{w}^T \\mathbf(x)$.\n",
        "- The Adaline algorithm compares the true class labels with the linear activation function's continuous valued output to compute the model error and update the weights. In contrast, the perceptron compares the true class labels to the predicted class labels.\n",
        "\n"
      ],
      "metadata": {
        "id": "O0iDhacTRcVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adaline Cost Function\n",
        "- The cost function $J$ of Adaline learning is defined as the\n",
        "sum of squared errors (SSE) between the calculated\n",
        "outcome and the true class label:\n",
        "\\begin{equation}\n",
        "J(\\mathbf{w}) = \\frac{1}{2}\\Sigma_i(y^{(i)}-\\phi(z^{(i)}))^2\n",
        "\\end{equation}\n",
        "- Question: why there is a factor $\\frac{1}{2}$ in the cost function?\n",
        "- The cost function is differentiable.\n",
        "- The cost function is convex.\n",
        "- A very simple yet powerful optimization algorithm called\n",
        "gradient descent can be used to find the weights that minimize our cost function.\n",
        "<img src=\"https://i.imgur.com/v6IaYKR.png\" width=800>"
      ],
      "metadata": {
        "id": "qbNhcSF5TXkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient descent\n",
        "- As climbing down a hill until a local or global cost minimum is reached.\n",
        "- In each iteration, we take a step in the opposite direction of the gradient\n",
        "- The step size is determined by the value ofthe learning rate, as well as the slope of the gradient.\n",
        "<img src=\"https://i.imgur.com/2nqogyT.png\" width=800>"
      ],
      "metadata": {
        "id": "c0IPmDamWjOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update the weights by gradient descent\n",
        "- Update the weights by taking a step in the opposite direction of the gradient, $\\nabla J(\\mathbf{w})$\n",
        ", of our cost function $J(\\mathbf{w})$:\n",
        " - $\\mathbf{w} := \\mathbf{w} + \\Delta \\mathbf{w}$, where $\\Delta \\mathbf{w}= - \\eta \\nabla J(\\mathbf{w})$.\n",
        "- To compute the gradient of the cost function, we need to compute the partial derivative of the cost function with respect to each weight, $w_j$:\n",
        "\\begin{equation}\n",
        "\\frac{\\partial J}{\\partial w_j} = - \\Sigma_i(y^{(i)}-\\phi(z^{(i)}))x_j^{(i)}\n",
        "\\end{equation}\n",
        "- Let us derive the above partial derivative (exercise)\n",
        "- Each $w_j$ is updated:\n",
        "\\begin{equation}\n",
        "\\Delta w_j = - \\eta \\frac{\\partial J}{\\partial w_j} = \\eta \\Sigma_i(y^{(i)}-\\phi(z^{(i)}))x_j^{(i)}\n",
        "\\end{equation}\n"
      ],
      "metadata": {
        "id": "C_B4mkfdXZXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual exercise on single variable input\n",
        "- Input: $\\mathbf{x}^1 = 0.5$ and $\\mathbf{x}^2 = 2$\n",
        "- Set the learning rate $\\eta=1$\n",
        "- True labels: $y^1 = -1$ and $y^2 = 1$.\n",
        "- Initial weights: $\\mathbf{w}= [w_0, w_1] = [1, -1]$\n",
        "- In the first epoch:\n",
        " - $ z^1 = 1 + (-1 \\times 0.5) = 0.5$, so $\\bar{y}^1 = \\phi(z) = 0.5$\n",
        " - $ z^2 = 1 + (-1 \\times 2) = -1$, so $\\bar{y}^2 = \\phi(z) = -1$\n",
        " - errors = $y^i - \\bar{y}^i = [-1-0.5, 1-(-1)] = [-1.5, 2]$\n",
        " - update the weight $w_0$ : $\\Delta w_0 = \\eta \\Sigma(errors)=\\eta \\Sigma_i(y^i - \\bar{y}^i)= 1(-1.5 + 2) = 0.5$, $w_0 := w_0 + \\Delta w_0 = 1 + 0.5 = 1.5$\n",
        " - update the weight $w_1$: $\\Delta w_1 = \\eta \\Sigma_i (y^i - \\bar{y}^i) x_i =1[(-1.5\\times0.5)+(2\\times 2)]=3.25$, $w_1 := w_1 + \\Delta w_1 = -1 + 3.25 = 2.25$\n",
        " - The new weights are: $\\mathbf{w}= [w_0, w_1] = [1.5, 2.25]$\n",
        " - The cost is $\\frac{1}{2}\\Sigma_i errors^2=\\frac{1}{2}((-1.5)^2 + 2^2)=3.125$\n",
        " - Continue to update the weights in next epoch.\n",
        "\n",
        "- In the second epoch:\n",
        " - $ z^1 = 1.5 + (2.25 \\times 0.5) = 2.625$, so $\\bar{y}^1 = \\phi(z) = 2.625$\n",
        " - $ z^2 = 1.5 + (2.25 \\times 2) = 6$, so $\\bar{y}^2 = \\phi(z) = 6$\n",
        " - errors = $y^i - \\bar{y}^i = [-1-2.625, 1-6] = [-3.625, -5]$\n",
        " - update the weight $w_0$ : $\\Delta w_0 = \\eta \\Sigma(errors)=\\eta \\Sigma_i(y^i - \\bar{y}^i)= 1(-3.625 - 5) = -8.625$, $w_0 := w_0 + \\Delta w_0 = 1.5 + (-8.625) = -7.125$\n",
        " - update the weight $w_1$: $\\Delta w_1 = \\eta \\Sigma_i (y^i - \\bar{y}^i) x_i =1[(-3.625\\times0.5)+(-5\\times 2)]=-11.8125$, $w_1 := w_1 + \\Delta w_1 = 2.25 + (-11.8125) = -9.5625$\n",
        " - The new weights are: $\\mathbf{w}= [w_0, w_1] = [-7.125, -9.5625]$\n",
        " - The cost is $\\frac{1}{2}\\Sigma_i errors^2=\\frac{1}{2}((-3.625)^2 + (-5)^2)=19.07$\n",
        " - Continue to update the weights in next epochs.$\n"
      ],
      "metadata": {
        "id": "cl-k8bIXtXAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Practice"
      ],
      "metadata": {
        "id": "NHXoIl-fbxIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot the Data and Models"
      ],
      "metadata": {
        "id": "7el87Q7spcCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_point(0.5, -1)\n",
        "plot_point(2, 1)\n",
        "\n",
        "# 1-x\n",
        "plot_line(1, -1, 'blue', 'Initial Model')\n",
        "# 1.5+2.25x\n",
        "plot_line(1.5, 2.25, 'red', 'Epoch 1: Updated Model')\n",
        "# -7.125-9.5625x\n",
        "plot_line(-7.125, -9.5625, 'green', 'Epoch 2: Updated Model')\n",
        "# -1.4+1.16x\n",
        "plot_line(-1.4, 1.16, 'purple', 'Epoch 3: Updated Model')\n",
        "\n",
        "\n",
        "plot_axis()\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel(\"Input variable x\")\n",
        "plt.ylabel(\"Labels of the inputs\")\n",
        "\n",
        "plt.title(\"Model Updates\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "COANx1Tg0Uqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the Adaline learning algorithm"
      ],
      "metadata": {
        "id": "8_FpX_a_e40v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdalineGD(object):\n",
        "    \"\"\"ADAptive LInear NEuron classifier.\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    eta : float\n",
        "      Learning rate (between 0.0 and 1.0)\n",
        "    epochs : int\n",
        "      Passes over the training dataset.\n",
        "    random_state : int\n",
        "      Random number generator seed for random weight\n",
        "      initialization.\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    -----------\n",
        "    w_ : 1d-array\n",
        "      Weights after fitting.\n",
        "    b_ : 1d-array\n",
        "      bias after fitting.\n",
        "    cost_ : list\n",
        "      Sum-of-squares cost function value in each epoch.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, eta=0.01, epochs=50, random_state=1):\n",
        "\n",
        "        # initialize parameters\n",
        "\n",
        "\n",
        "    def activation(self, X):\n",
        "        \"\"\"Compute linear activation\"\"\"\n",
        "\n",
        "        # return the input itself\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return class label after unit step\"\"\"\n",
        "\n",
        "        # compute the z values;\n",
        "\n",
        "\n",
        "        # return labels based on activation of z\n",
        "\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Fit training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like}, shape = [n_examples, n_features]\n",
        "          Training vectors, where n_examples is the number of examples and\n",
        "          n_features is the number of features.\n",
        "        y : array-like, shape = [n_examples]\n",
        "          Target values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "        \"\"\"\n",
        "        # initialize w_ and b_\n",
        "\n",
        "\n",
        "        # initialize an empty cost_ list\n",
        "\n",
        "\n",
        "        # update weights and bias through epochs\n",
        "        for i in range(self.epochs):\n",
        "            # compute the z values\n",
        "\n",
        "\n",
        "            # Please note that the \"activation\" method has no effect\n",
        "            # in the code since it is simply an identity function. We\n",
        "            # could write `output = z` directly instead.\n",
        "            # The purpose of the activation is more conceptual, i.e.,\n",
        "            # in the case of logistic regression (as we will see later),\n",
        "            # we could change it to\n",
        "            # a sigmoid function to implement a logistic regression classifier.\n",
        "\n",
        "\n",
        "        return self"
      ],
      "metadata": {
        "id": "5N68md_DxQaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classify the Iris data by Adaline"
      ],
      "metadata": {
        "id": "DRfwobo6dCkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris data into a dataframe\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "V3HqOKgGdMYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the iris data as a DataFrame\n"
      ],
      "metadata": {
        "id": "Idjr44IudMYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the iris data"
      ],
      "metadata": {
        "id": "FOboSJ8DdMYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select setosa and versicolor\n",
        "y = df.iloc[0:100, 4].values\n",
        "y = np.where(y == 'setosa', -1, 1)\n",
        "\n",
        "# extract sepal length and petal length\n",
        "X = df.iloc[0:100, [0, 2]].values\n",
        "\n",
        "# plot data\n",
        "plt.scatter(X[:50, 0], X[:50, 1],\n",
        "            color='red', marker='o', label='setosa')\n",
        "plt.scatter(X[50:100, 0], X[50:100, 1],\n",
        "            color='blue', marker='x', label='versicolor')\n",
        "\n",
        "plt.xlabel('sepal length [cm]')\n",
        "plt.ylabel('petal length [cm]')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# plt.savefig('images/02_06.png', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YrGbpVy1dMYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the Adaline Model on Iris setosa and versicolor"
      ],
      "metadata": {
        "id": "kR6PZ4CpdMYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract sepal length and petal length as the training data\n",
        "X ="
      ],
      "metadata": {
        "id": "aMQVl26_dMYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the target label\n",
        "y =\n",
        "y.shape"
      ],
      "metadata": {
        "id": "TtLwNNB1dMYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an Adaline model\n",
        "adaline = AdalineGD(epochs=10, eta=1)"
      ],
      "metadata": {
        "id": "8GQqZ4YQdgbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Adaline model\n",
        "adaline.fit(X,y)"
      ],
      "metadata": {
        "id": "FDtgmN4vd0qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the decision bounary of the Adaline model"
      ],
      "metadata": {
        "id": "CGXzMqx4eFEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
        "\n",
        "    # setup marker generator and color map\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    # plot the decision surface\n",
        "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                           np.arange(x2_min, x2_max, resolution))\n",
        "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    # plot class examples\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=X[y == cl, 0],\n",
        "                    y=X[y == cl, 1],\n",
        "                    alpha=0.8,\n",
        "                    c=colors[idx],\n",
        "                    marker=markers[idx],\n",
        "                    label=cl,\n",
        "                    edgecolor='black')"
      ],
      "metadata": {
        "id": "5gesfnS_eJxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_decision_regions(X, y, classifier=adaline)\n",
        "plt.xlabel('sepal length [cm]')\n",
        "plt.ylabel('petal length [cm]')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x_KWXdKseMbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the losses of the Adaline Model"
      ],
      "metadata": {
        "id": "7kexTElqeqd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1, len(adaline.cost_) + 1), np.log10(adaline.cost_), marker='o')"
      ],
      "metadata": {
        "id": "Qv20eOMkevHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval practice"
      ],
      "metadata": {
        "id": "1T2MhDsGb204"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning\n",
        "- In practice, it often requires some experimentation to find a good learn-ing rate, $\\eta$, for optimal convergence.\n",
        "- Let's choose two different learn-ing rates, 0.01, and, 0.001, to start with and plot the cost functions versus the number of epochs."
      ],
      "metadata": {
        "id": "b6lgQKQHg172"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
        "\n",
        "ada1 = AdalineGD(epochs=10, eta=0.01).fit(X, y)\n",
        "ax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker='o')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('log(Sum-squared-error)')\n",
        "ax[0].set_title('Adaline - Learning rate 0.01')\n",
        "\n",
        "ada2 = AdalineGD(epochs=10, eta=0.0001).fit(X, y)\n",
        "ax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker='o')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Sum-squared-error')\n",
        "ax[1].set_title('Adaline - Learning rate 0.0001')\n",
        "\n",
        "# plt.savefig('images/02_11.png', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "12StPEYwf4_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization\n",
        "- The left subfigure illustrates the case of a well-chosen learning rate\n",
        "- The subfigure on the right, however, illustrates what happens if we choose a learning rate that is too large â€” we overshoot the global minimum:\n",
        "<img src=\"https://i.imgur.com/nqZnv7s.png\" width=800>"
      ],
      "metadata": {
        "id": "iSh7hBEqi4Ov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improving gradient descent through feature scaling\n",
        "- Gradient descent is one of the many algorithms that benefit from feature scaling.\n",
        "- Normalization procedure helps gradient descent learning to converge more quickly.\n",
        "- Normalization does not make the original dataset normally distributed.\n",
        "- It shifts the mean of each feature so that it is centered at zero and each feature has a standard deviation of 1 (unit variance).\n",
        "- To standardize the $j$th feature, we can simply subtract the sample mean, $\\mu_j$,from every training example and divide it by its standard deviation, $\\sigma_j$:\n",
        "\\begin{equation}\n",
        "x_j' = \\frac{x_j - \\mu_j}{\\sigma_j}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "QcGo46x0jWuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization\n",
        "- One of the reasons why standardization helps with gradient descent learning is that the optimizer has to go through fewer steps to find a good or optimal solution (the global cost minimum):\n",
        "<img src=\"https://i.imgur.com/UkrW657.png\" width=800>"
      ],
      "metadata": {
        "id": "aVE6Dsxik8KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# standardize features by subtracting the mean and dividing the std\n",
        "X_std = np.copy(X)\n",
        "X_std[:, 0] =\n",
        "X_std[:, 1] ="
      ],
      "metadata": {
        "id": "FUGtlLMyhQIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ada_gd = AdalineGD(epochs=15, eta=0.01)\n",
        "ada_gd.fit(X_std, y)\n",
        "\n",
        "plot_decision_regions(X_std, y, classifier=ada_gd)\n",
        "plt.title('Adaline - Gradient Descent')\n",
        "plt.xlabel('sepal length [standardized]')\n",
        "plt.ylabel('petal length [standardized]')\n",
        "plt.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "# plt.savefig('images/02_14_1.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(1, len(ada_gd.cost_) + 1), ada_gd.cost_, marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Sum-squared-error')\n",
        "\n",
        "plt.tight_layout()\n",
        "# plt.savefig('images/02_14_2.png', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FheWLuZhlm6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval practice"
      ],
      "metadata": {
        "id": "zlUHuqoAb6lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Large-scale machine learning and stochastic gradient descent\n",
        "- Batch gradient descent: a cost gradient that is calculated\n",
        "from the whole training dataset.\n",
        "- Batch gradient descent can be computationally quite costly for a very large dataset with millions of data points.\n",
        "- Stochastic gradient descent: update\n",
        "the weights incrementally for each training example:\n",
        "\\begin{equation}\n",
        "    \\Delta w_j = - \\eta \\frac{\\partial J}{\\partial w_j} = \\eta (y^{(i)}-\\phi(z^{(i)}))x_j^{(i)}\n",
        "\\end{equation}\n",
        "- In SGD implementations, the fixed learning rate, $\\eta$, is oftenreplaced by an adaptive learning rate that decreases overtime, for example:\n",
        "\\begin{equation}\n",
        "\\eta = \\frac{c_1}{number\\_of\\_epochs + c_2}\n",
        "\\end{equation}\n",
        "- A compromise between batch gradient descent and SGD is\n",
        "so-called mini-batch learning.\n",
        "- Mini-batch learning can be understood as applying batch gradient descent to smaller subsets of the training data."
      ],
      "metadata": {
        "id": "H6WMxma9nNVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdalineSGD(object):\n",
        "    \"\"\"ADAptive LInear NEuron classifier.\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    eta : float\n",
        "      Learning rate (between 0.0 and 1.0)\n",
        "    epochs : int\n",
        "      Passes over the training dataset.\n",
        "    shuffle : bool (default: True)\n",
        "      Shuffles training data every epoch if True to prevent cycles.\n",
        "    random_state : int\n",
        "      Random number generator seed for random weight\n",
        "      initialization.\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    -----------\n",
        "    w_ : 1d-array\n",
        "      Weights after fitting.\n",
        "    b_ : 1d-array\n",
        "      Bias after fitting\n",
        "    cost_ : list\n",
        "      Sum-of-squares cost function value averaged over all\n",
        "      training examples in each epoch.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, eta=0.01, epochs=10, shuffle=True, random_state=None):\n",
        "        self.eta = eta\n",
        "        self.epochs = epochs\n",
        "        self.w_initialized = False\n",
        "        self.shuffle = shuffle\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def _initialize_weights(self, m):\n",
        "        \"\"\"Initialize weights to small random numbers\"\"\"\n",
        "        self.rgen = np.random.RandomState(self.random_state)\n",
        "\n",
        "        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size = m)\n",
        "        self.b_ = np.float_(0.)\n",
        "\n",
        "        self.w_initialized = True\n",
        "\n",
        "    def _shuffle(self, X, y):\n",
        "        \"\"\"Shuffle training data\"\"\"\n",
        "        r = self.rgen.permutation(len(y))\n",
        "\n",
        "        return X[r], y[r]\n",
        "\n",
        "    def activation(self, X):\n",
        "        \"\"\"Compute linear activation\"\"\"\n",
        "        return X\n",
        "\n",
        "    def _update_weights(self, xi, target):\n",
        "        \"\"\"Apply Adaline learning rule to update the weights\"\"\"\n",
        "\n",
        "        # Calculate net input\n",
        "        z = np.dot(xi, self.w_) + self.b_\n",
        "\n",
        "        output = self.activation(z)\n",
        "\n",
        "        error = (target - output)\n",
        "\n",
        "        self.w_ += self.eta * xi.dot(error)\n",
        "        self.b_ += self.eta * error\n",
        "\n",
        "        cost = 0.5 * error**2\n",
        "\n",
        "        return cost\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\" Fit training data.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like}, shape = [n_examples, n_features]\n",
        "          Training vectors, where n_examples is the number of examples and\n",
        "          n_features is the number of features.\n",
        "        y : array-like, shape = [n_examples]\n",
        "          Target values.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "\n",
        "        \"\"\"\n",
        "        self._initialize_weights(X.shape[1])\n",
        "        self.cost_ = []\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "\n",
        "            if self.shuffle:\n",
        "                X, y = self._shuffle(X, y)\n",
        "\n",
        "            cost = []\n",
        "\n",
        "            for xi, target in zip(X, y):\n",
        "                cost.append(self._update_weights(xi, target))\n",
        "\n",
        "            avg_cost = sum(cost) / len(y)\n",
        "            self.cost_.append(avg_cost)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return class label after unit step\"\"\"\n",
        "\n",
        "        # Calculate net input\n",
        "        z = np.dot(X, self.w_) + self.b_\n",
        "\n",
        "        return np.where(self.activation(z) >= 0.0, 1, -1)"
      ],
      "metadata": {
        "id": "RV6-mtr6ln1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ada_sgd = AdalineSGD(epochs=15, eta=0.01, random_state=1)\n",
        "ada_sgd.fit(X_std, y)\n",
        "\n",
        "plot_decision_regions(X_std, y, classifier=ada_sgd)\n",
        "plt.title('Adaline - Stochastic Gradient Descent')\n",
        "plt.xlabel('sepal length [standardized]')\n",
        "plt.ylabel('petal length [standardized]')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "# plt.savefig('images/02_15_1.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(1, len(ada_sgd.cost_) + 1), ada_sgd.cost_, marker='o')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Average Cost')\n",
        "\n",
        "plt.tight_layout()\n",
        "# plt.savefig('images/02_15_2.png', dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SqkaK7OhqGZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Scikit-Learn Perceptron Class"
      ],
      "metadata": {
        "id": "JBjmr6O6ojT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def generateXy():\n",
        "    \"\"\"\n",
        "    Generate 25 random values that are in [-1, 0.5] with label -1 and\n",
        "    25 random values that are in [1.5, 3] with label 1.\n",
        "    The values are used for training a single variable perceptron.\n",
        "    Input: None\n",
        "    Output: (X, y): 25 values in [-1, 0.5] with label -1\n",
        "                    and 25 values in [1.5, 3] with label 1\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate 25 random points that x < 0.5 and y = -1\n",
        "    np.random.RandomState(1234)\n",
        "\n",
        "    x1 = np.random.uniform(-1, 0.5, size=25)\n",
        "    y1 = np.ones(len(x1)) * -1\n",
        "\n",
        "    # Generate 25 random points that x > 1.5 and y = 1\n",
        "    x2 = np.random.uniform(1.5, 3, size=25)\n",
        "    y2 = np.ones(len(x2))\n",
        "\n",
        "    x12 = np.concatenate((x1, x2))\n",
        "    y12 = np.concatenate((y1, y2))\n",
        "\n",
        "    Xy = list(zip(x12, y12))\n",
        "    random.shuffle(Xy)\n",
        "\n",
        "    return Xy"
      ],
      "metadata": {
        "id": "LeWXVULulp_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron"
      ],
      "metadata": {
        "id": "4jlpij6qoo97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xy = generateXy()\n",
        "X, y = zip(*Xy)"
      ],
      "metadata": {
        "id": "ObiSOaKWo0-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_arr = np.array(X).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "FCBTOnm0o9CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_arr = np.array(y)"
      ],
      "metadata": {
        "id": "rWiyGQ-4pEfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sk_pnn = Perceptron(eta0=0.1, random_state=123)"
      ],
      "metadata": {
        "id": "MVaol4BGpKcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sk_pnn.fit(X_arr, y_arr)"
      ],
      "metadata": {
        "id": "80cVA5qIpOzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sk_pnn.intercept_, sk_pnn.coef_"
      ],
      "metadata": {
        "id": "H5_0BQM0peTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute weights for different learning rates between 0.1 and 1 by step 0.1\n"
      ],
      "metadata": {
        "id": "mncvDNUGq4k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## plot the points and the models based on different learning rates: eta"
      ],
      "metadata": {
        "id": "VvrQaELeAmqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the points\n",
        "plt.scatter(X, y, c=y, cmap=\"coolwarm\")\n",
        "\n",
        "# Plot the line y = weights[0] + weights[1] * x\n",
        "etas = np.arange(0.1, 1.1, 0.1)\n",
        "for idx, weights in enumerate(weights_list):\n",
        "    label = \"Eta = {:0.1f}\".format(etas[idx])\n",
        "    plot_line(weights[0], weights[1], label)\n",
        "\n",
        "plt.axhline(y=0, color='black')\n",
        "plt.axvline(x=0, color='black')\n",
        "\n",
        "plt.xlabel(\"The input variable x\")\n",
        "plt.ylabel(\"The labels of points\")\n",
        "plt.title(\"Simple Perceptron Models Given Different Learning Rates\")\n",
        "\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MK4mDsYrrosT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval practice"
      ],
      "metadata": {
        "id": "cP2wVbyUb--c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nXyZ4pGGcFfJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}