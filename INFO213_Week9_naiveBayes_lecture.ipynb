{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anyuanay/INFO213/blob/main/INFO213_Week9_naiveBayes_lecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tygLzVj-CIrY"
      },
      "source": [
        "# INFO 213: Data Science Programming 2\n",
        "___\n",
        "\n",
        "### Week 9: Probability, Naive Bayes, and Text Classification\n",
        "\n",
        "**Question:**\n",
        "- How to do fast and straigthforward probabilistic predictions on text classification?\n",
        "\n",
        "\n",
        "**Objectives:**\n",
        "- Define the problem of using conditional probabilities for classification\n",
        "- Describe the assumptions in Naive Bayes classification\n",
        "- Estimate the prior probabilities for Naive Bayes classification\n",
        "- Explain smoothing techiques in estimation\n",
        "- Implement Naive Bayes classification from scratch\n",
        "- Apply Naive Bayes methods in the Scikit Learn package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "089K-jUNCIrb"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "- Naive Bayes models are a group of extremely fast and simple classification algorithms that are often suitable for very high-dimensional datasets.\n",
        "- Because they are so fast and have so few tunable parameters, they end up being very useful as a quick-and-dirty baseline for a classification problem."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()"
      ],
      "metadata": {
        "id": "3jzBgoFHPLAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Practice on Probability"
      ],
      "metadata": {
        "id": "6luExJUIw_sU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "BQkkjzvYCIrb"
      },
      "source": [
        "## Bayesian Classification\n",
        "\n",
        "- Naive Bayes classifiers are built on Bayesian classification methods.\n",
        "- These rely on Bayes's theorem, which is an equation describing the relationship of conditional probabilities of statistical quantities.\n",
        "- In Bayesian classification, we're interested in finding the probability of a label given some observed features, which we can write as $P(L~|~{\\rm features})$.\n",
        "- Bayes's theorem tells us how to express this in terms of quantities we can compute more directly:\n",
        "\n",
        "$$\n",
        "P(L~|~{\\rm features}) = \\frac{P({\\rm features}~|~L)P(L)}{P({\\rm features})}\n",
        "$$\n",
        "\n",
        "- If we are trying to decide between two labels—let's call them $L_1$ and $L_2$—then one way to make this decision is to compute the ratio of the posterior probabilities for each label:\n",
        "\n",
        "$$\n",
        "\\frac{P(L_1~|~{\\rm features})}{P(L_2~|~{\\rm features})} = \\frac{P({\\rm features}~|~L_1)}{P({\\rm features}~|~L_2)}\\frac{P(L_1)}{P(L_2)}\n",
        "$$\n",
        "\n",
        "- All we need now is some model by which we can compute $P({\\rm features}~|~L_i)$ for each label.\n",
        "- Such a model is called a *generative model* because it specifies the hypothetical random process that generates the data.\n",
        "- Specifying this generative model for each label is the main piece of the training of such a Bayesian classifier.\n",
        "- The general version of such a training step is a very difficult task, but we can make it simpler through the use of some simplifying assumptions about the form of this model.\n",
        "\n",
        "- This is where the \"naive\" in \"naive Bayes\" comes in: if we make very naive assumptions about the generative model for each label, we can find a rough approximation of the generative model for each class, and then proceed with the Bayesian classification.\n",
        "- Different types of naive Bayes classifiers rest on different naive assumptions about the data.\n",
        "\n",
        "**Naive Assumption**\n",
        "- Given a lable L and a set of data containing $n$ features. The event \"the data contains a feature $f_i$ is independent of the events of the data containing any other features\".\n",
        "- In other words, the features of the data are mutually independent. With the properties of conditional probabilities, we can write\n",
        "$$\n",
        "P(f_1, f_2, ..., f_n|L) = P(f_1|L)\\times P(f_2|L)\\times ...\\times P(f_n|L).\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPgQyZ4MCIrc"
      },
      "source": [
        "### A Really Dumb Spam Filter\n",
        "\n",
        "- Imagine a “universe” that consists of receiving a message chosen randomly from all possible messages.\n",
        "- Let $S$ be the event “the message is spam” and $V$ be the event “the message contains the word Gold” and $R$ be the event \"the message contains the word Rolex.\".\n",
        "- Then Bayes’s Theorem tells us that the probability\n",
        "that the message is spam conditional on containing the word Gold and Rolex is:\n",
        "\n",
        "$$P(S|V, R) = \\frac{P(V, R|S)\\times P(S)}{P(V, R)}$$\n",
        "\n",
        "- By the Naive assumption, we can write it as:\n",
        "\n",
        "$$P(S|V, R) = \\frac{P(V, R|S)\\times P(S)}{P(V, R)} = \\frac{P(V|S)\\times P(R|S)\\times P(S)}{P(V, R)}$$\n",
        "\n",
        "- Given a set of spaming and non-spamming emails, we can estimate the quantities $P(V|S)$, $P(R|S)$, $P(S)$, and $P(V, R)$ by counting the number of occurences of each event."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsqC7tAGCIrc"
      },
      "source": [
        "## Probability\n",
        "- It is hard to do data science without some sort of understanding of probability and its\n",
        "mathematics.\n",
        "- For our purposes you should think of probability as a way of quantifying the uncertainty\n",
        "associated with events chosen from a some universe of events. - The universe consists of all possible outcomes. And any subset of these outcomes is an event; for\n",
        "example, “the die rolls a one” or “the die rolls an even number.”\n",
        "\n",
        "- Notationally, we write $P(E)$ to mean “the probability of the event $E$.”\n",
        "- We’ll use probability theory to build models. We’ll use probability theory to evaluate\n",
        "models. We’ll use probability theory all over the place."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-2C4AGzCIrd"
      },
      "source": [
        "### Dependence and Independence\n",
        "- Roughly speaking, we say that two events E and F are dependent if knowing something\n",
        "about whether E happens gives us information about whether F happens (and\n",
        "vice versa). Otherwise they are independent.\n",
        "\n",
        "- Mathematically, we say that two events E and F are independent if the probability that\n",
        "they both happen is the product of the probabilities that each one happens:\n",
        "\n",
        "$$P(E, F) = P(E)\\times P(F)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q6M3unFCIre"
      },
      "source": [
        "### Properties of Probability\n",
        "- Given a set of possible events $S$. The probabilities of events in $S$ must satisfy the following properties:\n",
        "    1. $P(a) \\geq 0$ for $a\\in S$\n",
        "    2. $\\sum_{a\\in S} P(a) = 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvKYt_PYCIre"
      },
      "source": [
        "### Conditional Probability\n",
        "- When two events E and F are independent, then by definition we have:\n",
        "\n",
        "$$P(E, F) = P(E)\\times P(F)$$\n",
        "\n",
        "- If they are not necessarily independent (and if the probability of F is not zero), then\n",
        "we define the probability of E “conditional on F” as:\n",
        "\n",
        "$$P(E|F) = P(E, F) /P(F)$$\n",
        "\n",
        "- You should think of this as the probability that E happens, given that we know that F happens. We often rewrite this as:\n",
        "\n",
        "$$P(E, F) = P(E|F)\\times P(F)$$\n",
        "\n",
        "- When E and F are independent, you can check that this gives:\n",
        "\n",
        "$$P(E|F) = P(E)$$\n",
        "\n",
        "- which is the mathematical way of expressing that knowing F occurred gives us no additional information about whether E occurred."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGYXoTrNCIrf"
      },
      "source": [
        "### Example\n",
        "- One common tricky example involves a family with two (unknown) children.\n",
        "- If we assume that:\n",
        "    1. Each child is equally likely to be a boy or a girl\n",
        "    2. The gender of the second child is independent of the gender of the first child\n",
        "\n",
        "- Then the event “two boys” has probability 1/4, the event “one girl, one boy” has probability 1/2, and the event “two girls” has probability 1/4.\n",
        "\n",
        "- OK. Let us consider the following two events:\n",
        "    1. what is the probability of the event “both children are girls” (B) conditional on the event “the older child is a girl” (G)?\n",
        "    2. what is the probability of the event “both children are girls” conditional on the event “at least one of the children is a girl” (L)?\n",
        "\n",
        "- If you have gotten the answers, let us simulate the events and check your answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk4P99PeCIrf"
      },
      "source": [
        "```\n",
        "import random\n",
        "def random_kid():\n",
        "    return random.choice([\"boy\", \"girl\"])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR8oditPCIrg"
      },
      "source": [
        "```\n",
        "both_girls = 0\n",
        "older_girl = 0\n",
        "either_girl = 0\n",
        "girl_boy = 0\n",
        "\n",
        "random.seed(0)\n",
        "for _ in range(10000):\n",
        "    younger = random_kid()\n",
        "    older = random_kid()\n",
        "    if older == \"girl\":\n",
        "        older_girl += 1\n",
        "    if older == \"girl\" and younger == \"girl\":\n",
        "        both_girls += 1\n",
        "    if older == \"girl\" or younger == \"girl\":\n",
        "        either_girl += 1\n",
        "    if younger != older:\n",
        "        girl_boy += 1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eudcFe0qCIrg"
      },
      "source": [
        "```\n",
        "print('The probability of \"both children are girls (B) conditional on the event the older child is a girl (G)\" is: ' + \\\n",
        "     str(both_girls / older_girl))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq1AoVVZCIrh"
      },
      "source": [
        "```\n",
        "print('The probability of \"both children are girls (B) conditional on the event at least one of the children is a girl (L)\" is: ' + \\\n",
        "     str(both_girls / either_girl))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxbOld4gCIri"
      },
      "source": [
        "```\n",
        "print('The probability of \"one boy one girl (D) conditional on the event one child is a girl (L)\" is: ' + \\\n",
        "     str(girl_boy / either_girl))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs8o7J2WCIri"
      },
      "source": [
        "#### Why is it that if you know one child is girl then having another child as boy is twice as likely as having another girl?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fo1vGV-CIri"
      },
      "source": [
        "### Bayes’s Theorem\n",
        "- One of the data scientist’s best friends is Bayes’s Theorem, which is a way of “reversing” conditional probabilities.\n",
        "- Let’s say we need to know the probability of some event\n",
        "$E$ conditional on some other event F occurring.\n",
        "- But we only have information about the probability of $F$ conditional on E occurring. Using the definition of conditional\n",
        "probability twice tells us that:\n",
        "\n",
        "$$P(E|F) = \\frac{P(E, F)}{P(F)} = \\frac{P(F|E) P(E)} {P(F)}$$\n",
        "\n",
        "- The event $F$ can be split into the two mutually exclusive events “$F$ and $E$” and “$F$ and\n",
        "not $E$.” If we write $¬E$ for “not $E$” (i.e., “$E$ doesn’t happen”), then:\n",
        "\n",
        "$$P(F) = P(F, E) + P(F, ¬E)$$\n",
        "\n",
        "so that:\n",
        "\n",
        "$$P(E|F) = \\frac{P(F|E) P(E)}  {P(F|E) P(E) + P(F|¬E) P(¬E)}$$\n",
        "\n",
        "which is how Bayes’s Theorem is often stated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJGBy4FeCIri"
      },
      "source": [
        "#### Exercise\n",
        "- Imagine a certain disease that affects 1 in every 10,000 people.\n",
        "- And imagine that there is a test for this disease that gives the correct result (“diseased” if you have\n",
        "the disease, “nondiseased” if you don’t) 99% of the time.\n",
        "- What is the probability of a person having the disease if the person has a positive test?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoBXz30VCIri",
        "outputId": "13b99968-238f-46a3-9e5f-69ce6bb3defa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.009900990099009901"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "(0.99*0.0001) / (0.99*0.0001 + 0.01*0.99)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAmfIiQTCIri"
      },
      "source": [
        "### Random Variables\n",
        "- A random variable is a variable whose possible values have an associated probability distribution.\n",
        "- A very simple random variable equals 1 if a coin flip turns up heads and 0 if the flip turns up tails.\n",
        "\n",
        "### Expected Value\n",
        "- The expected value $E[X]$ of a random varaible $X$ is the average value of $X$ weighted by its probabilities:\n",
        "\n",
        "$$E[X] = P(X = x_{1}) x_{1} + P(X=x_{2}) x_{2}+...+P(X=x_{n}) x_{n}$$\n",
        "\n",
        "### Variance\n",
        "- The variance of a random variable $X$ is the expected value of the squared deviation from the mean of $X$, $\\mu= E[X]$:\n",
        "\n",
        "$$Var(X) = E[(X-\\mu)^2]$$\n",
        "\n",
        "### Standard Deviation\n",
        "- Standard deviation is the squred root of variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEPjukzgCIri"
      },
      "source": [
        "### Continuous Distributions\n",
        "- A coin flip corresponds to a discrete distribution—one that associates positive probability with discrete outcomes.\n",
        "- Often we’ll want to model distributions across a continuum\n",
        "of outcomes. (For our purposes, these outcomes will always be real numbers, although that’s not always the case in real life.)\n",
        "- For example, the uniform distribution\n",
        "puts equal weight on all the numbers between 0 and 1.\n",
        "\n",
        "- Because there are infinitely many numbers between 0 and 1, this means that the weight it assigns to individual points must necessarily be zero.\n",
        "- For this reason, we represent a continuous distribution with a probability density function (pdf) such that\n",
        "the probability of seeing a value in a certain interval equals the integral of the density function over the interval.\n",
        "\n",
        "- We will often be more interested in the cumulative distribution function (cdf), which\n",
        "gives the probability that a random variable is less than or equal to a certain value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eXjHUvoCIri"
      },
      "source": [
        "#### Exercise\n",
        "- Write functions for the probability density function and the cumulative distribtuion function of the uniform distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2CFdR07CIri"
      },
      "source": [
        "### The Normal Distribution\n",
        "- The normal distribution is the king of distributions. It is the classic bell curve–shaped distribution and is completely determined by two parameters: its mean $\\mu$ and its\n",
        "standard deviation $\\sigma$. The mean indicates where the bell is centered, and the\n",
        "standard deviation how “wide” it is.\n",
        "\n",
        "- It has the distribution function:\n",
        "$$\n",
        "f(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp (-\\frac{(x − \\mu)^2}{2\\sigma^2})\n",
        "$$\n",
        "\n",
        "- which we can implement as:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP5C3DdMCIrj"
      },
      "source": [
        "```\n",
        "def normal_pdf(x, mu=0, sigma=1):\n",
        "    sqrt_two_pi = math.sqrt(2 * math.pi)\n",
        "    return (math.exp(-(x-mu) ** 2 / 2 / sigma ** 2) / (sqrt_two_pi * sigma))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1exAxAtoCIrj"
      },
      "source": [
        "```\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "xs = [x / 10.0 for x in range(-50, 50)]\n",
        "plt.figure(figsize = (15, 9))\n",
        "plt.plot(xs,[normal_pdf(x,sigma=1) for x in xs],'-',label='mu=0,sigma=1')\n",
        "plt.plot(xs,[normal_pdf(x,sigma=2) for x in xs],'--',label='mu=0,sigma=2')\n",
        "plt.plot(xs,[normal_pdf(x,sigma=0.5) for x in xs],':',label='mu=0,sigma=0.5')\n",
        "plt.plot(xs,[normal_pdf(x,mu=-1) for x in xs],'-.',label='mu=-1,sigma=1')\n",
        "plt.legend()\n",
        "plt.title(\"Various Normal pdfs\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRseNUhKCIrj"
      },
      "source": [
        "- When $\\mu = 0$ and $\\sigma = 1$, it’s called the standard normal distribution.\n",
        "- If Z is a standard\n",
        "normal random variable, then it turns out that:\n",
        "$$X = \\sigma Z + \\mu$$\n",
        "is also normal but with mean $\\mu$ and standard deviation $\\sigma$.\n",
        "- Conversely, if X is a normal random variable with mean $\\mu$ and standard deviation $\\sigma$,\n",
        "$$Z = (X − \\mu) /\\sigma$$\n",
        "is a standard normal variable -- **Rescalling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPQobAycCIrj"
      },
      "source": [
        "### The Central Limit Theorem\n",
        "\n",
        "- One reason the normal distribution is so useful is the central limit theorem, which says (in essence) that a random variable defined as the average of a large number of independent and identically distributed random variables is itself approximately normally\n",
        "distributed.\n",
        "\n",
        "- In particular, if $x_{1}, ..., x_{n}$ are random variables with mean $\\mu$ and standard deviation $\\sigma$,\n",
        "and if n is large, then:\n",
        "$$\n",
        "(x_{1} + ... + x_{n})/n\n",
        "$$\n",
        "is approximately normally distributed with mean $\\mu$ and standard deviation $\\sigma / \\sqrt{(n)}$.\n",
        "- Equivalently (but often more usefully),\n",
        "$$\n",
        "\\frac{(x_{1} + ... + x_{n}) − \\mu n}{\\sigma \\sqrt{(n)}}\n",
        "$$\n",
        "is approximately normally distributed with mean 0 and standard deviation 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyMZ2bQSCIrj"
      },
      "source": [
        "### Example\n",
        "- An easy way to illustrate this is by looking at binomial random variables, which have\n",
        "two parameters n and p.\n",
        "- A Binomial(n,p) random variable is simply the sum of n\n",
        "independent Bernoulli(p) random variables, each of which equals 1 with probability p and 0 with probability 1 − p:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuhmz46kCIrj"
      },
      "source": [
        "```\n",
        "def bernoulli_trial(p):\n",
        "    return 1 if random.random() < p else 0\n",
        "def binomial(n, p):\n",
        "    return sum(bernoulli_trial(p) for _ in range(n))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sugpACEBCIrj"
      },
      "source": [
        "- The mean of a Bernoulli(p) variable is $p$, and its standard deviation is $p(1 − p)$.\n",
        "- The central limit theorem says that as n gets large, a Binomial(n,p) variable is approximately\n",
        "a normal random variable with mean $\\mu = np$ and standard deviation $\\sigma = \\sqrt{np(1 − p)}$ . If we plot both, you can easily see the resemblance:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNPTgrLUCIrj"
      },
      "source": [
        "```\n",
        "from collections import Counter\n",
        "def normal_cdf(x, mu=0,sigma=1):\n",
        "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n",
        "def make_hist(p, n, num_points):\n",
        "    data = [binomial(n, p) for _ in range(num_points)]\n",
        "    # use a bar chart to show the actual binomial samples\n",
        "    histogram = Counter(data)\n",
        "    plt.figure(figsize=(15, 9))\n",
        "    plt.bar([x - 0.4 for x in histogram.keys()], \\\n",
        "    [v / num_points for v in histogram.values()], \\\n",
        "    0.8, \\\n",
        "    color='0.75')\n",
        "    mu = p * n\n",
        "    sigma = math.sqrt(n * p * (1 - p))\n",
        "    # use a line chart to show the normal approximation\n",
        "    xs = range(min(data), max(data) + 1)\n",
        "    ys = [normal_cdf(i + 0.5, mu, sigma) - normal_cdf(i - 0.5, mu, sigma) for i in xs]\n",
        "    #plt.figure(figsize=(15,9))\n",
        "    plt.plot(xs,ys)\n",
        "    plt.title(\"Binomial Distribution vs. Normal Approximation\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU8jciJTCIrp"
      },
      "source": [
        "```\n",
        "make_hist(0.75, 100, 10000)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Practice"
      ],
      "metadata": {
        "id": "Fbq-sb71wAao"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7aE0tXAEX_f"
      },
      "source": [
        "## Naive Bayes Spam Filter\n",
        "\n",
        "- Imagine now that we have a vocabulary of many words $w_{1}, ...,w_{n}$.\n",
        "- To move this into the realm of probability theory, we’ll write $X_{i}$ for the event “a message contains the\n",
        "word $w_{i}$.”\n",
        "- Also imagine that (through some unspecified-at-this-point process) we’ve come up with an estimate $P(X_{i}|S)$ for the probability that a spam message contains\n",
        "the ith word, and a similar estimate $P(X_{i}|\\neg S)$ for the probability that a nonspam\n",
        "message contains the ith word.\n",
        "\n",
        "- Naive Bayes method:\n",
        "$$\n",
        "P(X_{1} = x_{1}, . . . , X_{n} = x_{n}|S) = P(X_{1} = x_{1}|S) \\times ⋯ \\times P(X_{n} = x_{n}|S)\n",
        "$$\n",
        "\n",
        "- The Naive Bayes assumption allows us to compute each of the probabilities on the right simply by multiplying together the individual probability estimates for each\n",
        "vocabulary word.\n",
        "\n",
        "- In practice, you usually want to avoid multiplying lots of probabilities together, to avoid a problem called underflow, in which computers don’t deal well with floating point\n",
        "numbers that are too close to zero.\n",
        "\n",
        "- Recalling from algebra that\n",
        "$\\log (ab) = \\log (a) + \\log (b)$ and that $\\exp \\log (x) = x$, we usually compute $p_{1} \\times ...\\times p_{n}$ as\n",
        "the equivalent (but floating-point-friendlier):\n",
        "\n",
        "$$\n",
        "\\exp (\\log (p_{1}) + ⋯ + \\log (p_{n}))\n",
        "$$\n",
        "\n",
        "- If we have a fair number of “training” messages labeled as spam and not-spam, an obvious first try is to estimate $P(X_{i}|S)$ simply as the fraction of spam messages containing\n",
        "word $w_{i}$.\n",
        "\n",
        "- To avoid zero-probability problem, we usually use some kind of smoothing. In particular, we’ll choose a pseudocount—k—and estimate the probability of seeing\n",
        "the ith word in a spam as:\n",
        "$$\n",
        "P(X_{i}|S) = (k + number\\ of\\ spams\\ containing\\ w_{i}) / (2k + number\\ of\\ spams)\n",
        "$$\n",
        "\n",
        "- Similarly for $P(X_{i}|\\neg S)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "e_CPiUZSEX_j"
      },
      "source": [
        "## Use Scikit-Learn Multinomial Naive Bayes\n",
        "\n",
        "- We assume that features were generated from a simple multinomial distribution.\n",
        "- The multinomial distribution describes the probability of observing counts among a number of categories, and thus multinomial naive Bayes is most appropriate for features that represent counts or count rates.\n",
        "\n",
        "- The idea is precisely the same as before, except that instead of modeling the data distribution with the best-fit Gaussian, we model the data distribuiton with a best-fit multinomial distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "2A6hUxdwEX_j"
      },
      "source": [
        "### Example: Classifying Text\n",
        "\n",
        "- One place where multinomial naive Bayes is often used is in text classification, where the features are related to word counts or frequencies within the documents to be classified.\n",
        "\n",
        "- Here we will use the sparse word count features from the 20 Newsgroups corpus to show how we might classify these short documents into categories.\n",
        "\n",
        "- Let's download the data and take a look at the target names:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "S7QMgiG_EX_j"
      },
      "source": [
        "```\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "data = fetch_20newsgroups()\n",
        "data.target_names\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "gY_MPojhEX_k"
      },
      "source": [
        "For simplicity here, we will select just a few of these categories, and download the training and testing set:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "QcKEz8LvEX_k"
      },
      "source": [
        "```\n",
        "categories = ['comp.graphics', 'rec.autos', 'sci.space', 'talk.politics.misc']\n",
        "train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "test = fetch_20newsgroups(subset='test', categories=categories)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Map target numbers to target names\n",
        "target_num_name = {i: name for i, name in enumerate(train.target_names)}\n",
        "target_num_name\n",
        "```"
      ],
      "metadata": {
        "id": "aoIKf1wJbs2C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlz_GRgQEX_k"
      },
      "source": [
        "```\n",
        "train.keys()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "_YLq7GPTEX_k"
      },
      "source": [
        "Here is a representative entry from the data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ahazfGfkEX_q"
      },
      "source": [
        "```\n",
        "print(train.data[5])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jYRhWZrEX_q"
      },
      "source": [
        "```\n",
        "train.target[5]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nRiDc0bEX_q"
      },
      "source": [
        "```\n",
        "pd.unique(train.target)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Taf0yjpyEX_r"
      },
      "source": [
        "```\n",
        "test.data[2]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDSLOS-6EX_r"
      },
      "source": [
        "```\n",
        "test.target[2]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "bJfFZlFjEX_r"
      },
      "source": [
        "- In order to use this data for machine learning, we need to be able to convert the content of each string into a vector of numbers.\n",
        "- For this we will use the count vectorizer, and create a pipeline that attaches it to a multinomial naive Bayes classifier:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ZfyT5An2EX_r"
      },
      "source": [
        "```\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "model_vectorizer = CountVectorizer()\n",
        "clf = MultinomialNB()\n",
        "pipe = make_pipeline(model_vectorizer, clf)\n",
        "```"
      ],
      "metadata": {
        "id": "Ngx_CXrCb4M7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ECPHYiopEX_r"
      },
      "source": [
        "- With this pipeline, we can apply the model to the training data, and predict labels for the test data:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "pipe.fit(train.data, train.target)\n",
        "```"
      ],
      "metadata": {
        "id": "C7zZnW6Rb9Et"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "preds = pipe.predict(test.data)\n",
        "```"
      ],
      "metadata": {
        "id": "EdGW7p3ocBF0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "_a0ZAhjpEX_s"
      },
      "source": [
        "- Now that we have predicted the labels for the test data, we can evaluate them to learn about the performance of the estimator.\n",
        "- For example, here is the confusion matrix between the true and predicted labels for the test data:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "from sklearn.metrics import confusion_matrix\n",
        "mat = confusion_matrix(test.target, preds)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
        "            xticklabels=train.target_names, yticklabels=train.target_names)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');\n",
        "```"
      ],
      "metadata": {
        "id": "nfVR4WmNcGNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "precision, recall, f1, support = precision_recall_fscore_support(test.target, preds)\n",
        "```"
      ],
      "metadata": {
        "id": "RZdsnzLicKze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Print nicely with class names\n",
        "for i, name in enumerate(test.target_names):\n",
        "    print(f\"{name:25s} | Precision: {precision[i]:.2f}  Recall: {recall[i]:.2f}  F1: {f1[i]:.2f}  Support: {support[i]}\")\n",
        "```"
      ],
      "metadata": {
        "id": "zkPec3nRcOnb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "jN6H-aDZEX_s"
      },
      "source": [
        "- Here's a quick utility function that will return the prediction for a single string:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def predict_category(s, train=train, pipe=pipe):\n",
        "    pred = pipe.predict([s])\n",
        "    return train.target_names[pred[0]]\n",
        "```"
      ],
      "metadata": {
        "id": "0h9feHc-cSwC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Cs2lovcYEX_s"
      },
      "source": [
        "Let's try it out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzeDp3-SEX_s",
        "outputId": "fb191844-6854-4eab-c817-b3e7d48f9381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rec.autos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "predict_category('Honda Accord is a Sedan')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IeQf5PvEX_t",
        "outputId": "ea400d53-7928-4198-9eca-d0ffd250af0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sci.space'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "predict_category('discussion solar system')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcXrw6MBEX_t",
        "outputId": "8ced6257-f8b0-476c-a4c6-6dfd3d298574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'comp.graphics'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "predict_category('determining the screen size')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_category('George Washington was the first president of U.S.A.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9RUM8NsuNPPk",
        "outputId": "738cbdb5-dfca-4019-8f9a-1df039620a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'talk.politics.misc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "J2Ejhhw1EX_t"
      },
      "source": [
        "- Remember that this is nothing more sophisticated than a simple probability model for the (weighted) frequency of each word in the string; nevertheless, the result is striking.\n",
        "- Even a very naive algorithm, when used carefully and trained on a large set of high-dimensional data, can be surprisingly effective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "fK_DksChEX_t"
      },
      "source": [
        "## When to Use Naive Bayes\n",
        "\n",
        "Because naive Bayesian classifiers make such stringent assumptions about data, they will generally not perform as well as a more complicated model.\n",
        "That said, they have several advantages:\n",
        "\n",
        "- They are extremely fast for both training and prediction\n",
        "- They provide straightforward probabilistic prediction\n",
        "- They are often very easily interpretable\n",
        "- They have very few (if any) tunable parameters\n",
        "\n",
        "These advantages mean a naive Bayesian classifier is often a good choice as an initial baseline classification.\n",
        "If it performs suitably, then congratulations: you have a very fast, very interpretable classifier for your problem.\n",
        "If it does not perform well, then you can begin exploring more sophisticated models, with some baseline knowledge of how well they should perform.\n",
        "\n",
        "Naive Bayes classifiers tend to perform especially well in one of the following situations:\n",
        "\n",
        "- When the naive assumptions actually match the data (very rare in practice)\n",
        "- For very well-separated categories, when model complexity is less important\n",
        "- For very high-dimensional data, when model complexity is less important\n",
        "\n",
        "The last two points seem distinct, but they actually are related: as the dimension of a dataset grows, it is much less likely for any two points to be found close together (after all, they must be close in *every single dimension* to be close overall).\n",
        "This means that clusters in high dimensions tend to be more separated, on average, than clusters in low dimensions, assuming the new dimensions actually add information.\n",
        "For this reason, simplistic classifiers like naive Bayes tend to work as well or better than more complicated classifiers as the dimensionality grows: once you have enough data, even a simple model can be very powerful."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Practice"
      ],
      "metadata": {
        "id": "6eFUcHv3wIRh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQfRuLTTEvF3"
      },
      "source": [
        "# Applying Machine Learning To Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV3q89wiEvF5"
      },
      "source": [
        "## Obtaining the IMDb movie review dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISEkROG0EvF5"
      },
      "source": [
        "The IMDB movie review set can be downloaded from [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
        "After downloading the dataset, decompress the files.\n",
        "\n",
        "A) If you are working with Linux or MacOS X, open a new terminal window, `cd` into the download directory and execute\n",
        "\n",
        "`tar -zxf aclImdb_v1.tar.gz`\n",
        "\n",
        "B) If you are working with Windows, download an archiver such as [7Zip](http://www.7-zip.org) to extract the files from the download archive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0qzWNmJEvF5"
      },
      "source": [
        "**Optional code to download and unzip the dataset via Python:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0lDdEyQEvF5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66cb9987-3565-479c-d836-533d0c7b10db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% | 80.23 MB | 9.33 MB/s | 8.60 sec elapsed"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "source = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "target = 'aclImdb_v1.tar.gz'\n",
        "\n",
        "if os.path.exists(target):\n",
        "    os.remove(target)\n",
        "\n",
        "def reporthook(count, block_size, total_size):\n",
        "    global start_time\n",
        "    if count == 0:\n",
        "        start_time = time.time()\n",
        "        return\n",
        "    duration = time.time() - start_time\n",
        "    progress_size = int(count * block_size)\n",
        "    speed = progress_size / (1024.**2 * duration)\n",
        "    percent = count * block_size * 100. / total_size\n",
        "\n",
        "    sys.stdout.write(f'\\r{int(percent)}% | {progress_size / (1024.**2):.2f} MB '\n",
        "                     f'| {speed:.2f} MB/s | {duration:.2f} sec elapsed')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "if not os.path.isdir('aclImdb') and not os.path.isfile('aclImdb_v1.tar.gz'):\n",
        "    urllib.request.urlretrieve(source, target, reporthook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cn0W6sAwEvF5"
      },
      "outputs": [],
      "source": [
        "if not os.path.isdir('aclImdb'):\n",
        "\n",
        "    with tarfile.open(target, 'r:gz') as tar:\n",
        "        tar.extractall()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JxcJhG4EvF6"
      },
      "source": [
        "## Preprocessing the movie dataset into more convenient format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTQkOJfPEvF6"
      },
      "source": [
        "Install pyprind by uncommenting the next code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNg3W24CEvF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4326c7d9-4d3f-452f-fd7d-9bb0988b342d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pyprind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcc98jWQEvF6",
        "outputId": "822b6135-69cc-4123-b29c-bb00b18d219c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:01:31\n"
          ]
        }
      ],
      "source": [
        "import pyprind\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "from packaging import version\n",
        "\n",
        "\n",
        "# change the `basepath` to the directory of the\n",
        "# unzipped movie dataset\n",
        "\n",
        "basepath = 'aclImdb'\n",
        "\n",
        "labels = {'pos': 1, 'neg': 0}\n",
        "\n",
        "# if the progress bar does not show, change stream=sys.stdout to stream=2\n",
        "pbar = pyprind.ProgBar(50000, stream=2)\n",
        "\n",
        "df = pd.DataFrame()\n",
        "for s in ('test', 'train'):\n",
        "    for l in ('pos', 'neg'):\n",
        "        path = os.path.join(basepath, s, l)\n",
        "        for file in sorted(os.listdir(path)):\n",
        "            with open(os.path.join(path, file),\n",
        "                      'r', encoding='utf-8') as infile:\n",
        "                txt = infile.read()\n",
        "\n",
        "            if version.parse(pd.__version__) >= version.parse(\"1.3.2\"):\n",
        "                x = pd.DataFrame([[txt, labels[l]]], columns=['review', 'sentiment'])\n",
        "                df = pd.concat([df, x], ignore_index=False)\n",
        "\n",
        "            else:\n",
        "                df = df.append([[txt, labels[l]]],\n",
        "                               ignore_index=True)\n",
        "            pbar.update()\n",
        "df.columns = ['review', 'sentiment']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAyM5AsYEvF6"
      },
      "source": [
        "Shuffling the DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti45oPnjEvF6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "if version.parse(pd.__version__) >= version.parse(\"1.3.2\"):\n",
        "    df = df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "\n",
        "else:\n",
        "    np.random.seed(0)\n",
        "    df = df.reindex(np.random.permutation(df.index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MLyfguREvF6"
      },
      "source": [
        "Optional: Saving the assembled data as CSV file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt7OAWO5EvF6"
      },
      "outputs": [],
      "source": [
        "df.to_csv('movie_data.csv', index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf0yDh7fEvF6",
        "outputId": "cb5f5bc2-ed13-424e-ee08-2be955c7eb2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
              "1  OK... so... I really like Kris Kristofferson a...          0\n",
              "2  ***SPOILER*** Do not read this, if you think a...          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e82be08f-3699-4ebe-b10f-d3317df9d9b4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e82be08f-3699-4ebe-b10f-d3317df9d9b4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e82be08f-3699-4ebe-b10f-d3317df9d9b4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e82be08f-3699-4ebe-b10f-d3317df9d9b4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7711715b-aea8-4cca-9ae9-6892cf53474a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7711715b-aea8-4cca-9ae9-6892cf53474a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7711715b-aea8-4cca-9ae9-6892cf53474a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"Every time I think about this film I feel physically ill. To read such a great book and later discover there's a film of it was a great feeling. Years later and imagine my joy at switching on the sci-fi channel and finding it starts in just 5mins!!! Up go the titles and then uggg. If just a couple of things had changed OK. Everything is changed. Numerous characters are removed entirely new rubbish ones are added. The main hero is shrunk and de-aged by about 30 years, and hilariously his girlfriend/wife is now his mother! Even the dog is reduced to sub-lassie capabilities. This is truly appalling cinema at its absolute worst. I would quite happily remove my own toenails with pliers rather than sit through another horrific viewing, and I urge anyone thinking of watching this - please don't. If you own a copy burn it now, right now and think how much better your life would have been had this celluloid insult never occurred.\",\n          \"As with all environmentally aware films from the 1970s SOYLENT GREEN has a rather cheesy view of what ecological meltdown is . Overpopulation means there`s too many people to feed ? I was under the impression that famines were caused by either war or failed economic policies . Stalin`s policy in the Soviet Union in the 1930s left millions dead because of famine and to this day the greatest man made tragedy was Mao`s rural policy in China which led over 30 million starvation deaths in the 1950s . And let`s not forget the great famines in the horn of Africa in the 1980s and 90s which were to do with conflicts not overpopulation . You might like to also consider that two of the most heavily populated areas on Earth , Hong Kong and Macau , have never suffered a famine in modern times . Likewise the expansion of shanty towns around cities as seen here isn`t strictly down to overpopulation - it`s down to economic factors where people flock to cities to find better paid work than in the countryside ( It`s a symptom of industrial progress - not of too many births ) so the image of the streets of New York city being too congested to walk through and of having people sleep in stairwells is somewhat laughable<br /><br />But don`t be fooled into thinking SOYLENT GREEN is a pile of corny tree hugging crap because I consider this to be the best ecological film of the<br /><br />70s . It plays on the contempary audience`s knowledge of the world where Sol and Thorn are beside themselves with joy at finding fruit , brandy and fresh meat . Thorn gasps in amazement at having ice in his whisky , puffs on a cigarette and delivers the classic line \\\" If I could afford it I`d smoke two , maybe three of these a day \\\" . But it`s the visage of the euthanasia chamber that`s memorable as Thorn gazes at the images of wild animals , flowers , running water and snow covered mountains , a world Thorn`s generation has never known . This is a very haunting scene which makes SOYLENT GREEN a very memorable film , combined with the fact it features the final screen appearance of Edward G Robinson as the wise old Jew Sol Roth\",\n          \"Yah. I know. It has the name \\\"Sinatra\\\" in the title, so how bad can it be? Well, it's bad, trust me! I rented this thinking it was some movie I missed in the theaters. It's not. It's some garbage \\\"movie\\\" made by the folks at Showtime (cable station). Geez, these cable stations make a few bucks they think they can make whatever garbage movies they want! It's not good. I am as big a Sinatra fan as any sane man, but this movie was just dumb. Boring. Dull. Unfunny. Uninteresting. The only redeeming quality is that (assuming they did stick to the facts) you do learn about what happened to the captors of Frank Jr. Otherwise it's just a stupid film.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
        "\n",
        "# the following is necessary on some computers:\n",
        "df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})\n",
        "\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ttCqwzHEvF6",
        "outputId": "1e7cefe5-6eec-43ee-abfc-9413e28d4818",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omcOyOk4EvF7"
      },
      "source": [
        "## Transforming documents into feature vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C40EiTdOEvF7"
      },
      "source": [
        "By calling the fit_transform method on CountVectorizer, we just constructed the vocabulary of the bag-of-words model and transformed the following three sentences into sparse feature vectors:\n",
        "1. The sun is shining\n",
        "2. The weather is sweet\n",
        "3. The sun is shining, the weather is sweet, and one and one is two\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITNoGDduEvF7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count = CountVectorizer()\n",
        "docs = np.array([\n",
        "        'The sun is shining',\n",
        "        'The weather is sweet',\n",
        "        'The sun is shining, the weather is sweet, and one and one is two'])\n",
        "bag = count.fit_transform(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51LsUQ1dEvF7"
      },
      "source": [
        "Now let us print the contents of the vocabulary to get a better understanding of the underlying concepts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPNaQA3bEvF7",
        "outputId": "7383dbf9-78f2-4c5b-9a6a-2e7d40cb567d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
          ]
        }
      ],
      "source": [
        "print(count.vocabulary_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajpxe8-6EvF7"
      },
      "source": [
        "- As we can see from executing the preceding command, the vocabulary is stored in a Python dictionary, which maps the unique words to integer indices. Next let us print the feature vectors that we just created:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3fDT65NEvF7"
      },
      "source": [
        "- Each index position in the feature vectors shown here corresponds to the integer values that are stored as dictionary items in the CountVectorizer vocabulary.\n",
        "- For example, the first feature at index position 0 resembles the count of the word \"and\", which only occurs in the last document, and the word \"is\" at index position 1 (the 2nd feature in the document vectors) occurs in all three sentences. - Those values in the feature vectors are also called the raw term frequencies: *tf (t,d)*—the number of times a term t occurs in a document *d*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRyLM4xwEvF7",
        "outputId": "4cface88-7724-4b82-bf84-2d9e44cd9c8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 0 1 1 0 1 0 0]\n",
            " [0 1 0 0 0 1 1 0 1]\n",
            " [2 3 2 1 1 1 2 1 1]]\n"
          ]
        }
      ],
      "source": [
        "print(bag.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lStwAOCEvGB"
      },
      "source": [
        "## Assessing word relevancy via term frequency-inverse document frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oT3GTJ0TEvGB"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvTgj0XGEvGB"
      },
      "source": [
        "- When we are analyzing text data, we often encounter words that occur across multiple documents from both classes.\n",
        "- Those frequently occurring words typically don't contain useful or discriminatory information.\n",
        "- In this subsection, we will learn about a useful technique called term frequency-inverse document frequency (tf-idf) that can be used to downweigh those frequently occurring words in the feature vectors.\n",
        "- The tf-idf can be defined as the product of the term frequency and the inverse document frequency:\n",
        "\n",
        "$$\\text{tf-idf}(t,d)=\\text{tf (t,d)}\\times \\text{idf}(t,d)$$\n",
        "\n",
        "- Here the tf(t, d) is the term frequency that we introduced in the previous section,\n",
        "and the inverse document frequency *idf(t, d)* can be calculated as:\n",
        "$$\\text{idf}(t,d) = \\text{log}\\frac{n_d}{1+\\text{df}(d, t)},$$\n",
        "where $n_d$ is the total number of documents, and *df(d, t)* is the number of documents *d* that contain the term *t*.\n",
        "- Note that adding the constant 1 to the denominator is optional and serves the purpose of assigning a non-zero value to terms that occur in all training examples; the log is used to ensure that low document frequencies are not given too much weight.\n",
        "\n",
        "- Scikit-learn implements yet another transformer, the `TfidfTransformer`, that takes the raw term frequencies from `CountVectorizer` as input and transforms them into tf-idfs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmG-y0JyEvGB",
        "outputId": "80019e63-f65f-413a-e6cf-9884b1a4deaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
            " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
            " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf = TfidfTransformer(use_idf=True,\n",
        "                         norm='l2',\n",
        "                         smooth_idf=True)\n",
        "print(tfidf.fit_transform(count.fit_transform(docs))\n",
        "      .toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtdtPJunEvGB"
      },
      "source": [
        "- As we saw in the previous subsection, the word \"is\" had the largest term frequency in the 3rd document, being the most frequently occurring word. However, after transforming the same feature vector into tf-idfs, we see that the word \"is\" is now associated with a relatively small tf-idf (0.45) in document 3 since it is also contained in documents 1 and 2 and thus is unlikely to contain any useful, discriminatory information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx3kGD6WEvGB"
      },
      "source": [
        "- However, if we'd manually calculated the tf-idfs of the individual terms in our feature vectors, we'd have noticed that the `TfidfTransformer` calculates the tf-idfs slightly differently compared to the standard textbook equations that we defined earlier. The equations for the idf and tf-idf that were implemented in scikit-learn are:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNjf297KEvGC"
      },
      "source": [
        "$$\\text{idf} (t,d) = log\\frac{1 + n_d}{1 + \\text{df}(d, t)}$$\n",
        "\n",
        "- The tf-idf equation that was implemented in scikit-learn is as follows:\n",
        "\n",
        "$$\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times (\\text{idf}(t,d)+1)$$\n",
        "\n",
        "- While it is also more typical to normalize the raw term frequencies before calculating the tf-idfs, the `TfidfTransformer` normalizes the tf-idfs directly.\n",
        "\n",
        "- y default (`norm='l2'`), scikit-learn's TfidfTransformer applies the L2-normalization, which returns a vector of length 1 by dividing an un-normalized feature vector *v* by its L2-norm:\n",
        "\n",
        "$$v_{\\text{norm}} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}} = \\frac{v}{\\big (\\sum_{i=1}^{n} v_{i}^{2}\\big)^\\frac{1}{2}}$$\n",
        "\n",
        "- To make sure that we understand how `TfidfTransformer` works, let us walk through an example and calculate the tf-idf of the word \"is\" in the 3rd document.\n",
        "\n",
        "- The word \"is\" has a term frequency of 3 (tf = 3) in document 3 ($d_3$), and the document frequency of this term is 3 since the term \"is\" occurs in all three documents (df = 3). Thus, we can calculate the idf as follows:\n",
        "\n",
        "$$\\text{idf}(\"is\", d_3) = log \\frac{1+3}{1+3} = 0$$\n",
        "\n",
        "- Now in order to calculate the tf-idf, we simply need to add 1 to the inverse document frequency and multiply it by the term frequency:\n",
        "\n",
        "$$\\text{tf-idf}(\"is\", d_3)= 3 \\times (0+1) = 3$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KAD6HrxEvGC",
        "outputId": "f1f73239-298c-49c5-f9d7-d65a47818090",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf-idf of term \"is\" = 3.00\n"
          ]
        }
      ],
      "source": [
        "tf_is = 3\n",
        "n_docs = 3\n",
        "idf_is = np.log((n_docs+1) / (3+1))\n",
        "tfidf_is = tf_is * (idf_is + 1)\n",
        "print(f'tf-idf of term \"is\" = {tfidf_is:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnJ9kt51EvGC"
      },
      "source": [
        "- If we repeated these calculations for all terms in the 3rd document, we'd obtain the following tf-idf vectors: [3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29].\n",
        "- However, we notice that the values in this feature vector are different from the values that we obtained from the `TfidfTransformer` that we used previously.\n",
        "- The final step that we are missing in this tf-idf calculation is the L2-normalization, which can be applied as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph-rGYPoEvGC"
      },
      "source": [
        "$$\\text{tfi-df}_{norm} = \\frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]}{\\sqrt{[3.39^2, 3.0^2, 3.39^2, 1.29^2, 1.29^2, 1.29^2, 2.0^2 , 1.69^2, 1.29^2]}}$$\n",
        "\n",
        "$$=[0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]$$\n",
        "\n",
        "$$\\Rightarrow \\text{tfi-df}_{norm}(\"is\", d3) = 0.45$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtveEnBlEvGC"
      },
      "source": [
        "- As we can see, the results match the results returned by scikit-learn's `TfidfTransformer` (below). Since we now understand how tf-idfs are calculated, let us proceed to the next sections and apply those concepts to the movie review dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3MC6g7pEvGC",
        "outputId": "57af970e-b5bc-41cd-c4e7-db4c0bc8f80e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3.39, 3.  , 3.39, 1.29, 1.29, 1.29, 2.  , 1.69, 1.29])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True)\n",
        "raw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1]\n",
        "raw_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF5vZyNcEvGC",
        "outputId": "0261c99d-6fff-41d1-adfc-5e4a5fdf2200",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.5 , 0.45, 0.5 , 0.19, 0.19, 0.19, 0.3 , 0.25, 0.19])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2))\n",
        "l2_tfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ_WrnluEvGC"
      },
      "source": [
        "## Cleaning text data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "G-WUJI5fQZv6",
        "outputId": "9d05bb8e-73e8-44c0-e5ec-853e09036934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
              "1  OK... so... I really like Kris Kristofferson a...          0\n",
              "2  ***SPOILER*** Do not read this, if you think a...          0\n",
              "3  hi for all the people who have seen this wonde...          1\n",
              "4  I recently bought the DVD, forgetting just how...          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-43193523-3afa-4e29-a5c0-2867b9d18382\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hi for all the people who have seen this wonde...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I recently bought the DVD, forgetting just how...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43193523-3afa-4e29-a5c0-2867b9d18382')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-43193523-3afa-4e29-a5c0-2867b9d18382 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-43193523-3afa-4e29-a5c0-2867b9d18382');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-10fb350d-1a71-449c-a8ac-1faecd25e14b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-10fb350d-1a71-449c-a8ac-1faecd25e14b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-10fb350d-1a71-449c-a8ac-1faecd25e14b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"Every time I think about this film I feel physically ill. To read such a great book and later discover there's a film of it was a great feeling. Years later and imagine my joy at switching on the sci-fi channel and finding it starts in just 5mins!!! Up go the titles and then uggg. If just a couple of things had changed OK. Everything is changed. Numerous characters are removed entirely new rubbish ones are added. The main hero is shrunk and de-aged by about 30 years, and hilariously his girlfriend/wife is now his mother! Even the dog is reduced to sub-lassie capabilities. This is truly appalling cinema at its absolute worst. I would quite happily remove my own toenails with pliers rather than sit through another horrific viewing, and I urge anyone thinking of watching this - please don't. If you own a copy burn it now, right now and think how much better your life would have been had this celluloid insult never occurred.\",\n          \"As with all environmentally aware films from the 1970s SOYLENT GREEN has a rather cheesy view of what ecological meltdown is . Overpopulation means there`s too many people to feed ? I was under the impression that famines were caused by either war or failed economic policies . Stalin`s policy in the Soviet Union in the 1930s left millions dead because of famine and to this day the greatest man made tragedy was Mao`s rural policy in China which led over 30 million starvation deaths in the 1950s . And let`s not forget the great famines in the horn of Africa in the 1980s and 90s which were to do with conflicts not overpopulation . You might like to also consider that two of the most heavily populated areas on Earth , Hong Kong and Macau , have never suffered a famine in modern times . Likewise the expansion of shanty towns around cities as seen here isn`t strictly down to overpopulation - it`s down to economic factors where people flock to cities to find better paid work than in the countryside ( It`s a symptom of industrial progress - not of too many births ) so the image of the streets of New York city being too congested to walk through and of having people sleep in stairwells is somewhat laughable<br /><br />But don`t be fooled into thinking SOYLENT GREEN is a pile of corny tree hugging crap because I consider this to be the best ecological film of the<br /><br />70s . It plays on the contempary audience`s knowledge of the world where Sol and Thorn are beside themselves with joy at finding fruit , brandy and fresh meat . Thorn gasps in amazement at having ice in his whisky , puffs on a cigarette and delivers the classic line \\\" If I could afford it I`d smoke two , maybe three of these a day \\\" . But it`s the visage of the euthanasia chamber that`s memorable as Thorn gazes at the images of wild animals , flowers , running water and snow covered mountains , a world Thorn`s generation has never known . This is a very haunting scene which makes SOYLENT GREEN a very memorable film , combined with the fact it features the final screen appearance of Edward G Robinson as the wise old Jew Sol Roth\",\n          \"Yah. I know. It has the name \\\"Sinatra\\\" in the title, so how bad can it be? Well, it's bad, trust me! I rented this thinking it was some movie I missed in the theaters. It's not. It's some garbage \\\"movie\\\" made by the folks at Showtime (cable station). Geez, these cable stations make a few bucks they think they can make whatever garbage movies they want! It's not good. I am as big a Sinatra fan as any sane man, but this movie was just dumb. Boring. Dull. Unfunny. Uninteresting. The only redeeming quality is that (assuming they did stick to the facts) you do learn about what happened to the captors of Frank Jr. Otherwise it's just a stupid film.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oe-YnsRAEvGC",
        "outputId": "3eaea7bf-1496-4acc-e93c-9c8f2961498d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'is seven.<br /><br />Title (Brazil): Not Available'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "df.loc[0, 'review'][-50:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-78klK64EvGC"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def preprocessor(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
        "                           text)\n",
        "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
        "            ' '.join(emoticons).replace('-', ''))\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "284UGpV2EvGC",
        "outputId": "579eb9af-d6a7-4470-88e7-903dcecd1fd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'is seven title brazil not available'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "preprocessor(df.loc[0, 'review'][-50:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etk5QvrQEvGD",
        "outputId": "515cc9c8-f43e-4af4-9153-db74925c85d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this is a test :) :( :)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "preprocessor(\"</a>This :) is :( a test :-)!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OVVAzp2EvGD"
      },
      "outputs": [],
      "source": [
        "df['review'] = df['review'].apply(preprocessor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHmZkioMEvGD"
      },
      "source": [
        "## Processing documents into tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hrtcN5SEvGD"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def tokenizer(text):\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "    return [porter.stem(word) for word in text.split()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tdrp-T9-EvGD",
        "outputId": "17e12d69-241b-4d5c-a1a3-094c6bb20acc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "tokenizer('runners like running and thus they run')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOSLQkKREvGD",
        "outputId": "60e98730-c6e1-43a1-9ee3-4538c4535a81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "tokenizer_porter('runners like running and thus they run')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv0S5b5rEvGD",
        "outputId": "f3a6bd8b-ae5f-4d0b-fcd6-91a19ba29d5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGlLkmnnEvGD",
        "outputId": "61a08618-1c8f-41a5-9f6d-db8f61e2186f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['runner', 'like', 'run', 'run', 'lot']"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "[w for w in tokenizer_porter('a runner likes running and runs a lot')\n",
        " if w not in stop]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA9mm16REvGD"
      },
      "source": [
        "# Training a logistic regression model for document classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-dRXR9gEvGD"
      },
      "source": [
        "Strip HTML and punctuation to speed up the GridSearch later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seyjP10oEvGD"
      },
      "outputs": [],
      "source": [
        "X_train = df.loc[:25000, 'review'].values\n",
        "y_train = df.loc[:25000, 'sentiment'].values\n",
        "X_test = df.loc[25000:, 'review'].values\n",
        "y_test = df.loc[25000:, 'sentiment'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07OS8rAZEvGD"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "tfidf = TfidfVectorizer(strip_accents=None,\n",
        "                        lowercase=False,\n",
        "                        preprocessor=None)\n",
        "\n",
        "small_param_grid = [{\n",
        "    'vect__ngram_range': [(1, 1)],\n",
        "    'vect__stop_words': [None],\n",
        "    'vect__tokenizer': [tokenizer],\n",
        "    'clf__penalty': ['l2'],\n",
        "    'clf__C': [1.0]\n",
        "}]\n",
        "\n",
        "lr_tfidf = Pipeline([\n",
        "    ('vect', tfidf),\n",
        "    ('clf', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "gs_lr_tfidf = GridSearchCV(\n",
        "    lr_tfidf,\n",
        "    small_param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEnljet_EvGE"
      },
      "source": [
        "**Important Note about `n_jobs`**\n",
        "\n",
        "- Please note that it is highly recommended to use `n_jobs=-1` (instead of `n_jobs=1`) in the previous code example to utilize all available cores on your machine and speed up the grid search.\n",
        "- However, some Windows users reported issues when running the previous code with the `n_jobs=-1` setting related to pickling the tokenizer and tokenizer_porter functions for multiprocessing on Windows.\n",
        "- Another workaround would be to replace those two functions, `[tokenizer, tokenizer_porter]`, with `[str.split]`. However, note that the replacement by the simple `str.split` would not support stemming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfXboOKHEvGE",
        "outputId": "a8fbe969-b6bc-4ce0-e63d-f505dd8a5412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[('vect',\n",
              "                                        TfidfVectorizer(lowercase=False)),\n",
              "                                       ('clf',\n",
              "                                        LogisticRegression(solver='liblinear'))]),\n",
              "             n_jobs=-1,\n",
              "             param_grid=[{'clf__C': [1.0], 'clf__penalty': ['l2'],\n",
              "                          'vect__ngram_range': [(1, 1)],\n",
              "                          'vect__stop_words': [None],\n",
              "                          'vect__tokenizer': [<function tokenizer at 0x7ae126ff22a0>]}],\n",
              "             scoring='accuracy', verbose=1)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-2 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-2 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-2 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-2 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-2 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-2 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-2 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
              "                                        TfidfVectorizer(lowercase=False)),\n",
              "                                       (&#x27;clf&#x27;,\n",
              "                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n",
              "             n_jobs=-1,\n",
              "             param_grid=[{&#x27;clf__C&#x27;: [1.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;],\n",
              "                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n",
              "                          &#x27;vect__stop_words&#x27;: [None],\n",
              "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x7ae126ff22a0&gt;]}],\n",
              "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>GridSearchCV</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
              "                                        TfidfVectorizer(lowercase=False)),\n",
              "                                       (&#x27;clf&#x27;,\n",
              "                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n",
              "             n_jobs=-1,\n",
              "             param_grid=[{&#x27;clf__C&#x27;: [1.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;],\n",
              "                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n",
              "                          &#x27;vect__stop_words&#x27;: [None],\n",
              "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x7ae126ff22a0&gt;]}],\n",
              "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>best_estimator_: Pipeline</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
              "                 TfidfVectorizer(lowercase=False,\n",
              "                                 tokenizer=&lt;function tokenizer at 0x7ae126ff22a0&gt;)),\n",
              "                (&#x27;clf&#x27;, LogisticRegression(solver=&#x27;liblinear&#x27;))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>TfidfVectorizer</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer(lowercase=False,\n",
              "                tokenizer=&lt;function tokenizer at 0x7ae126ff22a0&gt;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "gs_lr_tfidf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRDkfj1XEvGE",
        "outputId": "13cc2fce-cf94-44ee-c0ab-05b9c6d059ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameter set: {'clf__C': 1.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x7ae126ff22a0>}\n",
            "CV Accuracy: 0.889\n"
          ]
        }
      ],
      "source": [
        "print(f'Best parameter set: {gs_lr_tfidf.best_params_}')\n",
        "print(f'CV Accuracy: {gs_lr_tfidf.best_score_:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0p5Bz9JEvGE",
        "outputId": "e6bb0fa4-965f-4f3d-d044-bd911f45b40e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.894\n"
          ]
        }
      ],
      "source": [
        "clf = gs_lr_tfidf.best_estimator_\n",
        "print(f'Test Accuracy: {clf.score(X_test, y_test):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Practice"
      ],
      "metadata": {
        "id": "WfKLOfv6yod2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEDZRfXhEvGE"
      },
      "source": [
        "## comment:\n",
        "    \n",
        "- Please note that `gs_lr_tfidf.best_score_` is the average k-fold cross-validation score. I.e., if we have a `GridSearchCV` object with 5-fold cross-validation (like the one above), the `best_score_` attribute returns the average score over the 5-folds of the best model. To illustrate this with an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4AKLqbsEvGE",
        "outputId": "e4b09e67-c78d-4dfb-93ca-7929c474ae05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.6, 0.4, 0.6, 0.2, 0.6])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "np.random.seed(0)\n",
        "np.set_printoptions(precision=6)\n",
        "y = [np.random.randint(3) for i in range(25)]\n",
        "X = (y + np.random.randn(25)).reshape(-1, 1)\n",
        "\n",
        "cv5_idx = list(StratifiedKFold(n_splits=5, shuffle=False).split(X, y))\n",
        "\n",
        "lr = LogisticRegression()\n",
        "cross_val_score(lr, X, y, cv=cv5_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKFRPbEZEvGE"
      },
      "source": [
        "- By executing the code above, we created a simple data set of random integers that shall represent our class labels. Next, we fed the indices of 5 cross-validation folds (`cv3_idx`) to the `cross_val_score` scorer, which returned 5 accuracy scores -- these are the 5 accuracy values for the 5 test folds.  \n",
        "\n",
        "- Next, let us use the `GridSearchCV` object and feed it the same 5 cross-validation sets (via the pre-generated `cv3_idx` indices):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iABBg5k7EvGE",
        "outputId": "a88bb5b5-dd2e-42c5-bcb6-62585870b6b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
            "[CV 1/5] END ..................................., score=0.600 total time=   0.0s\n",
            "[CV 2/5] END ..................................., score=0.400 total time=   0.0s\n",
            "[CV 3/5] END ..................................., score=0.600 total time=   0.0s\n",
            "[CV 4/5] END ..................................., score=0.200 total time=   0.0s\n",
            "[CV 5/5] END ..................................., score=0.600 total time=   0.0s\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "lr = LogisticRegression()\n",
        "gs = GridSearchCV(lr, {}, cv=cv5_idx, verbose=3).fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7hcyibAEvGE"
      },
      "source": [
        "- As we can see, the scores for the 5 folds are exactly the same as the ones from `cross_val_score` earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTxLgzw6EvGE"
      },
      "source": [
        "- Now, the best_score_ attribute of the `GridSearchCV` object, which becomes available after `fit`ting, returns the average accuracy score of the best model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fietRf8dEvGE",
        "outputId": "47c2217e-b120-4eea-ca24-ed403bd11e94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.48)"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ],
      "source": [
        "gs.best_score_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdHk9FUEvGE"
      },
      "source": [
        "- As we can see, the result above is consistent with the average score computed with `cross_val_score`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s08iDofwEvGF",
        "outputId": "31788308-ee65-45af-9c78-4d69e25c7c71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.48)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "lr = LogisticRegression()\n",
        "cross_val_score(lr, X, y, cv=cv5_idx).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cznFy3UbEvGF"
      },
      "source": [
        "# Working with bigger data - online algorithms and out-of-core learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15ZK15JZEvGF"
      },
      "outputs": [],
      "source": [
        "# This cell is not contained in the book but\n",
        "# added for convenience so that the notebook\n",
        "# can be executed starting here, without\n",
        "# executing prior code in this notebook\n",
        "\n",
        "import os\n",
        "import gzip\n",
        "\n",
        "\n",
        "if not os.path.isfile('movie_data.csv'):\n",
        "    if not os.path.isfile('movie_data.csv.gz'):\n",
        "        print('Please place a copy of the movie_data.csv.gz'\n",
        "              'in this directory. You can obtain it by'\n",
        "              'a) executing the code in the beginning of this'\n",
        "              'notebook or b) by downloading it from GitHub:'\n",
        "              'https://github.com/rasbt/machine-learning-book/'\n",
        "              'blob/main/ch08/movie_data.csv.gz')\n",
        "    else:\n",
        "        with gzip.open('movie_data.csv.gz', 'rb') as in_f, \\\n",
        "                open('movie_data.csv', 'wb') as out_f:\n",
        "            out_f.write(in_f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2MdVJO_EvGF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "# The `stop` is defined as earlier in this chapter\n",
        "# Added it here for convenience, so that this section\n",
        "# can be run as standalone without executing prior code\n",
        "# in the directory\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
        "        ' '.join(emoticons).replace('-', '')\n",
        "    tokenized = [w for w in text.split() if w not in stop]\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "def stream_docs(path):\n",
        "    with open(path, 'r', encoding='utf-8') as csv:\n",
        "        next(csv)  # skip header\n",
        "        for line in csv:\n",
        "            text, label = line[:-3], int(line[-2])\n",
        "            yield text, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlgdY8vIEvGF",
        "outputId": "8382b83d-b7a3-4e14-8db6-40b74b189829",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
              " 1)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "next(stream_docs(path='movie_data.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZ9scBhXEvGF"
      },
      "outputs": [],
      "source": [
        "def get_minibatch(doc_stream, size):\n",
        "    docs, y = [], []\n",
        "    try:\n",
        "        for _ in range(size):\n",
        "            text, label = next(doc_stream)\n",
        "            docs.append(text)\n",
        "            y.append(label)\n",
        "    except StopIteration:\n",
        "        return None, None\n",
        "    return docs, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Eo9TxODEvGF"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "\n",
        "vect = HashingVectorizer(decode_error='ignore',\n",
        "                         n_features=2**21,\n",
        "                         preprocessor=None,\n",
        "                         tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sILdP8v8EvGF"
      },
      "outputs": [],
      "source": [
        "from distutils.version import LooseVersion as Version\n",
        "from sklearn import __version__ as sklearn_version\n",
        "\n",
        "clf = SGDClassifier(loss='log_loss', random_state=1)\n",
        "\n",
        "\n",
        "doc_stream = stream_docs(path='movie_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xysK3cXMEvGF",
        "outputId": "a4844614-1eda-4462-fe87-305c4d7dfeb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:36\n"
          ]
        }
      ],
      "source": [
        "import pyprind\n",
        "pbar = pyprind.ProgBar(45)\n",
        "\n",
        "classes = np.array([0, 1])\n",
        "for _ in range(45):\n",
        "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
        "    if not X_train:\n",
        "        break\n",
        "    X_train = vect.transform(X_train)\n",
        "    clf.partial_fit(X_train, y_train, classes=classes)\n",
        "    pbar.update()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqResPADEvGF",
        "outputId": "61779a38-3265-4434-8a4a-d878f0db32ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.868\n"
          ]
        }
      ],
      "source": [
        "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
        "X_test = vect.transform(X_test)\n",
        "print(f'Accuracy: {clf.score(X_test, y_test):.3f}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}